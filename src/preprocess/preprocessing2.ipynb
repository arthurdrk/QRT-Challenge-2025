{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf39754",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Preprocessing notebook converted to a Python script (cell-based).\n",
    "All comments and markdown have been translated to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.util import Surv\n",
    "\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "import optuna\n",
    "from tqdm.notebook import tqdm\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set plotly as pandas plotting backend\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338eee9d",
   "metadata": {},
   "source": [
    "## 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/clinical_train.csv\")\n",
    "df_eval = pd.read_csv(\"../../data/clinical_val.csv\")\n",
    "maf_df = pd.read_csv(\"../../data/molecular_train.csv\")\n",
    "maf_eval = pd.read_csv(\"../../data/molecular_val.csv\")\n",
    "target_df = pd.read_csv(\"../../data/target_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db47541",
   "metadata": {},
   "source": [
    "## 1. Missing values — Clinical / Molecular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e805327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove IDs and center info for missingness analysis\n",
    "df_noid = df.drop(columns=[\"ID\", \"CENTER\"], errors=\"ignore\")\n",
    "maf_noid = maf_df.drop(columns=[\"ID\"], errors=\"ignore\")\n",
    "\n",
    "# -----------------------\n",
    "# 1.1 Missing values (clinical)\n",
    "# -----------------------\n",
    "missing_clinical = df_noid.isnull().sum()\n",
    "missing_clinical_pct = (missing_clinical / len(df_noid) * 100).sort_values()\n",
    "\n",
    "fig1 = go.Figure(\n",
    "    go.Bar(\n",
    "        x=missing_clinical_pct.values,\n",
    "        y=missing_clinical_pct.index,\n",
    "        orientation=\"h\",\n",
    "        text=[f\"{v:.1f}%\" for v in missing_clinical_pct.values],\n",
    "        textposition=\"outside\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    xaxis_title=\"Percentage of Missing Values (%)\",\n",
    "    yaxis_title=\"Features\",\n",
    "    margin=dict(l=120, r=40, t=10, b=40),\n",
    "    showlegend=False,\n",
    ")\n",
    "fig1.update_xaxes(showgrid=False)\n",
    "fig1.update_yaxes(showgrid=False)\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "# -----------------------\n",
    "# 1.2 Missing-value heatmap (clinical)\n",
    "# -----------------------\n",
    "missing_matrix = df_noid.isnull().astype(int)\n",
    "\n",
    "sample_size = min(500, len(df_noid))\n",
    "missing_sample = missing_matrix.sample(n=sample_size, random_state=42)\n",
    "\n",
    "z = missing_sample.T.values\n",
    "x = missing_sample.index.astype(str)\n",
    "y = missing_sample.columns.astype(str)\n",
    "\n",
    "fig2 = go.Figure(\n",
    "    go.Heatmap(\n",
    "        z=z,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        colorscale=[[0, \"lightblue\"], [1, \"darkred\"]],\n",
    "        colorbar=dict(title=\"Missing\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    xaxis_title=\"Patients\",\n",
    "    yaxis_title=\"Features\",\n",
    "    margin=dict(l=120, r=40, t=10, b=40),\n",
    ")\n",
    "fig2.update_xaxes(showgrid=False)\n",
    "fig2.update_yaxes(showgrid=False)\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "# -----------------------\n",
    "# 1.3 Missing values (molecular)\n",
    "# -----------------------\n",
    "missing_molecular = maf_noid.isnull().sum()\n",
    "missing_molecular_pct = (missing_molecular / len(maf_noid) * 100).sort_values()\n",
    "\n",
    "fig3 = go.Figure(\n",
    "    go.Bar(\n",
    "        x=missing_molecular_pct.values,\n",
    "        y=missing_molecular_pct.index,\n",
    "        orientation=\"h\",\n",
    "        text=[f\"{v:.1f}%\" for v in missing_molecular_pct.values],\n",
    "        textposition=\"outside\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig3.update_layout(\n",
    "    xaxis_title=\"Percentage of Missing Values (%)\",\n",
    "    yaxis_title=\"Features\",\n",
    "    margin=dict(l=120, r=40, t=10, b=40),\n",
    "    showlegend=False,\n",
    ")\n",
    "fig3.update_xaxes(showgrid=False)\n",
    "fig3.update_yaxes(showgrid=False)\n",
    "\n",
    "fig3.show()\n",
    "\n",
    "# -----------------------\n",
    "# 1.4 Mutations per patient\n",
    "# -----------------------\n",
    "mutations_per_patient = maf_df.groupby(\"ID\").size()\n",
    "\n",
    "fig4 = go.Figure(\n",
    "    go.Histogram(\n",
    "        x=mutations_per_patient.values,\n",
    "        nbinsx=50,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig4.update_traces(marker_line_width=1, marker_line_color=\"black\")\n",
    "fig4.update_layout(\n",
    "    xaxis_title=\"Number of Mutations\",\n",
    "    yaxis_title=\"Number of Patients\",\n",
    "    margin=dict(l=60, r=40, t=10, b=40),\n",
    "    showlegend=False,\n",
    ")\n",
    "fig4.update_xaxes(showgrid=False)\n",
    "fig4.update_yaxes(showgrid=False)\n",
    "\n",
    "fig4.show()\n",
    "\n",
    "# -----------------------\n",
    "# 1.5 Clinical vs Molecular comparison (side-by-side)\n",
    "# -----------------------\n",
    "fig5 = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig5.add_trace(\n",
    "    go.Bar(\n",
    "        x=missing_clinical_pct.values,\n",
    "        y=missing_clinical_pct.index,\n",
    "        orientation=\"h\",\n",
    "        text=[f\"{v:.1f}%\" for v in missing_clinical_pct.values],\n",
    "        textposition=\"outside\",\n",
    "        marker_color=\"salmon\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig5.add_trace(\n",
    "    go.Bar(\n",
    "        x=missing_molecular_pct.values,\n",
    "        y=missing_molecular_pct.index,\n",
    "        orientation=\"h\",\n",
    "        text=[f\"{v:.1f}%\" for v in missing_molecular_pct.values],\n",
    "        textposition=\"outside\",\n",
    "        marker_color=\"skyblue\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig5.update_xaxes(title_text=\"Missing %\", showgrid=False, row=1, col=1)\n",
    "fig5.update_yaxes(title_text=\"Features\", showgrid=False, row=1, col=1)\n",
    "\n",
    "fig5.update_xaxes(title_text=\"Missing %\", showgrid=False, row=1, col=2)\n",
    "fig5.update_yaxes(showgrid=False, row=1, col=2)\n",
    "\n",
    "fig5.update_layout(\n",
    "    margin=dict(l=120, r=40, t=10, b=40),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "fig5.show()\n",
    "\n",
    "# -----------------------\n",
    "# 1.6 Summary statistics\n",
    "# -----------------------\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(\"\\nClinical data (df, excluding ID & CENTER):\")\n",
    "print(f\"  Total samples: {len(df_noid)}\")\n",
    "print(\n",
    "    f\"  Features with missing values: \"\n",
    "    f\"{(missing_clinical > 0).sum()}/{len(missing_clinical)}\"\n",
    ")\n",
    "print(f\"  Average missing rate: {missing_clinical_pct.mean():.2f}%\")\n",
    "\n",
    "print(\"\\nMolecular data (maf_df, excluding ID):\")\n",
    "print(f\"  Total mutations (rows): {len(maf_df)}\")\n",
    "print(\n",
    "    f\"  Features with missing values: \"\n",
    "    f\"{(missing_molecular > 0).sum()}/{len(missing_molecular)}\"\n",
    ")\n",
    "print(f\"  Average missing rate: {missing_molecular_pct.mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4dd722",
   "metadata": {},
   "source": [
    "## 2. Survival target and base clinical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload molecular train/val (if needed later).\n",
    "maf_df2 = pd.read_csv(\"../../data/molecular_train.csv\")\n",
    "maf_eval2 = pd.read_csv(\"../../data/molecular_val.csv\")\n",
    "\n",
    "# Survival target columns\n",
    "target = [\"OS_YEARS\", \"OS_STATUS\"]\n",
    "\n",
    "# Clean target dataframe\n",
    "target_df[\"OS_YEARS\"] = pd.to_numeric(target_df[\"OS_YEARS\"], errors=\"coerce\")\n",
    "target_df[\"OS_STATUS\"] = target_df[\"OS_STATUS\"].astype(bool)\n",
    "target_df = target_df.dropna(subset=target)\n",
    "\n",
    "# Base clinical features\n",
    "features = [\"ID\", \"BM_BLAST\", \"WBC\", \"ANC\", \"MONOCYTES\", \"HB\", \"PLT\", \"CYTOGENETICS\"]\n",
    "\n",
    "X = df.loc[df[\"ID\"].isin(target_df[\"ID\"]), features].copy()\n",
    "X_eval = df_eval.loc[:, features].copy()\n",
    "\n",
    "# sksurv-style survival object (not used later but kept for compatibility)\n",
    "y = Surv.from_dataframe(\"OS_STATUS\", \"OS_YEARS\", target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a79eb7",
   "metadata": {},
   "source": [
    "# 3. Impute Missing Values (XGBoost + Optuna per feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d51454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns\n",
    "num_cols = X.select_dtypes(\"number\").columns\n",
    "\n",
    "# Add missingness indicators for numerical columns before imputation\n",
    "for df_ in (X, X_eval):\n",
    "    for col in num_cols:\n",
    "        df_[f\"{col}_missing\"] = df_[col].isna().astype(\"int8\")\n",
    "\n",
    "X_num = X[num_cols].to_numpy(dtype=float)\n",
    "n_rows, n_features = X_num.shape\n",
    "\n",
    "base_xgb_params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "n_trials = 1000\n",
    "mask_frac = 0.1\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "best_params_per_feature: Dict[str, Dict] = {}\n",
    "\n",
    "\n",
    "def make_objective_for_feature(j: int):\n",
    "    \"\"\"Optuna objective for a single numerical feature j.\"\"\"\n",
    "    def objective(trial):\n",
    "        xgb_params_trial = {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\n",
    "                \"learning_rate\", 0.01, 0.3, log=True\n",
    "            ),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        }\n",
    "\n",
    "        y_col = X_num[:, j]\n",
    "        X_features = np.delete(X_num, j, axis=1)\n",
    "\n",
    "        not_nan_y = ~np.isnan(y_col)\n",
    "        if not_nan_y.sum() < 20:\n",
    "            # Not enough data to train a stable model\n",
    "            return 1e6\n",
    "\n",
    "        y_obs = y_col[not_nan_y]\n",
    "        X_obs = X_features[not_nan_y]\n",
    "\n",
    "        mask_eval = rng.rand(len(y_obs)) < mask_frac\n",
    "        if mask_eval.sum() == 0 or (~mask_eval).sum() < 10:\n",
    "            return 1e6\n",
    "\n",
    "        X_train = X_obs[~mask_eval]\n",
    "        y_train = y_obs[~mask_eval]\n",
    "        X_eval_local = X_obs[mask_eval]\n",
    "        y_eval_local = y_obs[mask_eval]\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            **base_xgb_params,\n",
    "            **xgb_params_trial,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_eval_local)\n",
    "        mse = np.mean((y_pred - y_eval_local) ** 2)\n",
    "        return mse\n",
    "\n",
    "    return objective\n",
    "\n",
    "\n",
    "# Optuna tuning for each numerical feature\n",
    "for j, col in enumerate(num_cols):\n",
    "    y_col = X_num[:, j]\n",
    "    if (~np.isnan(y_col)).sum() < 20:\n",
    "        # Too few non-missing values, skip this feature\n",
    "        continue\n",
    "\n",
    "    print(f\"Tuning feature {j + 1}/{len(num_cols)}: {col}\")\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    with tqdm(total=n_trials, desc=col) as pbar:\n",
    "        def callback(study_, trial_):\n",
    "            pbar.set_description(f\"{col} | best MSE={study_.best_value:.5f}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "        study.optimize(\n",
    "            make_objective_for_feature(j),\n",
    "            n_trials=n_trials,\n",
    "            callbacks=[callback],\n",
    "        )\n",
    "\n",
    "    best_params_per_feature[col] = {**base_xgb_params, **study.best_params}\n",
    "\n",
    "print(\"Number of tuned numerical features:\", len(best_params_per_feature))\n",
    "\n",
    "\n",
    "def fit_xgb_imputers_per_feature(\n",
    "    X_df: pd.DataFrame,\n",
    "    numeric_cols: pd.Index,\n",
    "    best_params: Dict[str, Dict],\n",
    ") -> Dict[str, xgb.XGBRegressor]:\n",
    "    \"\"\"Train one XGBoost regressor per feature for imputation.\"\"\"\n",
    "    models: Dict[str, xgb.XGBRegressor] = {}\n",
    "    X_values = X_df[numeric_cols].to_numpy(dtype=float)\n",
    "\n",
    "    for j, col in enumerate(numeric_cols):\n",
    "        y_col = X_values[:, j]\n",
    "        missing_mask = np.isnan(y_col)\n",
    "        not_missing = ~missing_mask\n",
    "\n",
    "        if not_missing.sum() < 20:\n",
    "            continue\n",
    "        if col not in best_params:\n",
    "            continue\n",
    "\n",
    "        X_features = np.delete(X_values, j, axis=1)\n",
    "        X_train = X_features[not_missing]\n",
    "        y_train = y_col[not_missing]\n",
    "\n",
    "        params = best_params[col]\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        models[col] = model\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def transform_with_xgb_imputers(\n",
    "    X_df: pd.DataFrame,\n",
    "    numeric_cols: pd.Index,\n",
    "    models: Dict[str, xgb.XGBRegressor],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Apply trained imputers to fill missing values.\"\"\"\n",
    "    X_values = X_df[numeric_cols].to_numpy(dtype=float)\n",
    "\n",
    "    for j, col in enumerate(numeric_cols):\n",
    "        missing_mask = np.isnan(X_values[:, j])\n",
    "\n",
    "        if missing_mask.any():\n",
    "            if col in models:\n",
    "                X_features = np.delete(X_values, j, axis=1)\n",
    "                X_missing = X_features[missing_mask]\n",
    "                preds = models[col].predict(X_missing)\n",
    "                X_values[missing_mask, j] = preds\n",
    "            else:\n",
    "                X_values[missing_mask, j] = np.nanmedian(X_values[:, j])\n",
    "\n",
    "    return pd.DataFrame(X_values, columns=numeric_cols, index=X_df.index)\n",
    "\n",
    "\n",
    "# Final imputation\n",
    "xgb_models = fit_xgb_imputers_per_feature(X, num_cols, best_params_per_feature)\n",
    "\n",
    "X[num_cols] = transform_with_xgb_imputers(X, num_cols, xgb_models)\n",
    "X_eval[num_cols] = transform_with_xgb_imputers(X_eval, num_cols, xgb_models)\n",
    "\n",
    "# Save imputed clinical data\n",
    "X.to_csv(\"../../data/clinical_train_imputed.csv\", index=False)\n",
    "X_eval.to_csv(\"../../data/clinical_val_imputed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a08a9",
   "metadata": {},
   "source": [
    "## 4. Enhanced Mutation Features (global mutation burden / VAF / depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../../data/clinical_train_imputed.csv\")\n",
    "X_eval = pd.read_csv(\"../../data/clinical_val_imputed.csv\")\n",
    "\n",
    "# Ensure EFFECT dummies exist for molecular train/val\n",
    "pd.get_dummies(maf_df, columns=[\"EFFECT\"])\n",
    "pd.get_dummies(maf_eval, columns=[\"EFFECT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_mutation_features(\n",
    "    maf_df: pd.DataFrame,\n",
    "    X_df: pd.DataFrame,\n",
    "    top_k_chr: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute aggregated mutation-level features per patient.\"\"\"\n",
    "    maf_df = maf_df.copy()\n",
    "\n",
    "    if \"CHR\" not in maf_df.columns:\n",
    "        raise ValueError(\n",
    "            \"Column 'CHR' is missing from maf_df. \"\n",
    "            \"Do not one-hot encode CHR before calling compute_mutation_features.\"\n",
    "        )\n",
    "\n",
    "    # Mutation length and deletion length\n",
    "    maf_df[\"LEN\"] = maf_df[\"END\"] - maf_df[\"START\"] + 1\n",
    "    maf_df[\"DELLEN\"] = maf_df[\"LEN\"] - maf_df[\"REF\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "    # Keep only the top-k chromosomes by frequency for per-chromosome counts\n",
    "    top_chr = maf_df[\"CHR\"].value_counts().nlargest(top_k_chr).index\n",
    "    unique_chr = sorted(top_chr)\n",
    "\n",
    "    # EFFECT dummy columns (if already created upstream)\n",
    "    effect_dummy_cols = [c for c in maf_df.columns if c.startswith(\"EFFECT_\")]\n",
    "\n",
    "    agg_dict = {\n",
    "        \"Nmut\": (\"ID\", \"size\"),\n",
    "        \"VAF_avg\": (\"VAF\", \"mean\"),\n",
    "        \"VAF_std\": (\"VAF\", \"std\"),\n",
    "        \"VAF_max\": (\"VAF\", \"max\"),\n",
    "        \"LEN_avg\": (\"LEN\", \"mean\"),\n",
    "        \"LEN_max\": (\"LEN\", \"max\"),\n",
    "        \"DELLEN_sum\": (\"DELLEN\", \"sum\"),\n",
    "        \"DEPTH_avg\": (\"DEPTH\", \"mean\"),\n",
    "        \"DEPTH_std\": (\"DEPTH\", \"std\"),\n",
    "        \"DEPTH_max\": (\"DEPTH\", \"max\"),\n",
    "        \"DEPTH_min\": (\"DEPTH\", \"min\"),\n",
    "        \"CHR_nunique\": (\"CHR\", \"nunique\"),\n",
    "        \"EFFECT_nunique\": (\"EFFECT\", \"nunique\"),\n",
    "        \"EFFECT_FV_count\": (\"EFFECT\", lambda x: (x == \"frameshift_variant\").sum()),\n",
    "        \"EFFECT_SG_count\": (\"EFFECT\", lambda x: (x == \"stop_gained\").sum()),\n",
    "        \"EFFECT_NS_count\": (\"EFFECT\", lambda x: (x == \"non_synonymous_codon\").sum()),\n",
    "    }\n",
    "\n",
    "    # Per-chromosome counts for top_k_chr\n",
    "    for ch in unique_chr:\n",
    "        col_name = f\"CHR_{ch}_count\"\n",
    "        agg_dict[col_name] = (\"CHR\", lambda x, val=ch: (x == val).sum())\n",
    "\n",
    "    # Aggregate EFFECT_* dummy columns (if present)\n",
    "    for col in effect_dummy_cols:\n",
    "        new_name = f\"{col}_count\"\n",
    "        agg_dict[new_name] = (col, \"sum\")\n",
    "\n",
    "    tmp = maf_df.groupby(\"ID\").agg(**agg_dict).reset_index()\n",
    "\n",
    "    # Fill NaN standard deviations (single mutation per patient)\n",
    "    for std_col in [\"VAF_std\", \"DEPTH_std\"]:\n",
    "        if std_col in tmp.columns:\n",
    "            tmp[std_col] = tmp[std_col].fillna(0)\n",
    "\n",
    "    # Loss-of-function: EFFECT_LOF_count & EFFECT_LOF_ratio\n",
    "    lof_effect_cols = [\n",
    "        c\n",
    "        for c in tmp.columns\n",
    "        if c.startswith(\"EFFECT_\")\n",
    "        and c.endswith(\"_count\")\n",
    "        and (\n",
    "            \"frameshift_variant\" in c\n",
    "            or \"stop_gained\" in c\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    if len(lof_effect_cols) > 0:\n",
    "        tmp[\"EFFECT_LOF_count\"] = tmp[lof_effect_cols].sum(axis=1)\n",
    "    else:\n",
    "        tmp[\"EFFECT_LOF_count\"] = 0\n",
    "\n",
    "    tmp[\"EFFECT_LOF_ratio\"] = np.where(\n",
    "        tmp[\"Nmut\"] > 0,\n",
    "        tmp[\"EFFECT_LOF_count\"] / tmp[\"Nmut\"],\n",
    "        0.0,\n",
    "    )\n",
    "\n",
    "    # Merge with clinical data\n",
    "    X_w_mutation = X_df.merge(tmp, on=\"ID\", how=\"left\")\n",
    "\n",
    "    new_cols = [c for c in tmp.columns if c != \"ID\"]\n",
    "    X_w_mutation[new_cols] = X_w_mutation[new_cols].fillna(0)\n",
    "\n",
    "    return X_w_mutation\n",
    "\n",
    "\n",
    "# Apply on train / eval\n",
    "X_w_mutation = compute_mutation_features(maf_df, X)\n",
    "X_eval_w_mutation = compute_mutation_features(maf_eval, X_eval)\n",
    "\n",
    "base_mutation_features = [\n",
    "    \"Nmut\",\n",
    "    \"VAF_avg\",\n",
    "    \"VAF_std\",\n",
    "    \"VAF_max\",\n",
    "    \"LEN_avg\",\n",
    "    \"LEN_max\",\n",
    "    \"DELLEN_sum\",\n",
    "    \"DEPTH_avg\",\n",
    "    \"DEPTH_std\",\n",
    "    \"DEPTH_max\",\n",
    "    \"DEPTH_min\",\n",
    "    \"CHR_nunique\",\n",
    "    \"EFFECT_nunique\",\n",
    "    \"EFFECT_FV_count\",\n",
    "    \"EFFECT_SG_count\",\n",
    "    \"EFFECT_NS_count\",\n",
    "    \"EFFECT_LOF_count\",\n",
    "    \"EFFECT_LOF_ratio\",\n",
    "]\n",
    "\n",
    "chr_count_cols = [\n",
    "    c\n",
    "    for c in X_w_mutation.columns\n",
    "    if c.startswith(\"CHR_\") and c.endswith(\"_count\")\n",
    "]\n",
    "\n",
    "effect_count_cols = [\n",
    "    c\n",
    "    for c in X_w_mutation.columns\n",
    "    if c.startswith(\"EFFECT_\")\n",
    "    and c.endswith(\"_count\")\n",
    "    and c\n",
    "    not in [\n",
    "        \"EFFECT_FV_count\",\n",
    "        \"EFFECT_SG_count\",\n",
    "        \"EFFECT_NS_count\",\n",
    "        \"EFFECT_LOF_count\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "mutation_features = base_mutation_features + chr_count_cols + effect_count_cols\n",
    "\n",
    "# Harmonize columns between train and eval\n",
    "for col in mutation_features:\n",
    "    if col not in X_w_mutation.columns:\n",
    "        X_w_mutation[col] = 0\n",
    "    if col not in X_eval_w_mutation.columns:\n",
    "        X_eval_w_mutation[col] = 0\n",
    "\n",
    "X_w_mutation = X_w_mutation.copy()\n",
    "X_eval_w_mutation = X_eval_w_mutation.copy()\n",
    "X_w_mutation[mutation_features] = X_w_mutation[mutation_features]\n",
    "X_eval_w_mutation[mutation_features] = X_eval_w_mutation[mutation_features]\n",
    "\n",
    "# Scale mutation features\n",
    "mutation_scaler = RobustScaler()\n",
    "print(\n",
    "    f\"Fitting RobustScaler for mutation features \"\n",
    "    f\"({len(mutation_features)} features) on training data.\"\n",
    ")\n",
    "X_w_mutation[mutation_features] = mutation_scaler.fit_transform(\n",
    "    X_w_mutation[mutation_features]\n",
    ")\n",
    "X_eval_w_mutation[mutation_features] = mutation_scaler.transform(\n",
    "    X_eval_w_mutation[mutation_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812afa72",
   "metadata": {},
   "source": [
    "# 5. Cytogenetics Feature Extraction (ISCN parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8aa814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns for ISCN parsing\n",
    "_ISCN_EVENT_RE = re.compile(r\"(del|dup|inv|ins|i|t|add|der)\\s*\\(\", re.IGNORECASE)\n",
    "_MONOSOMY_RE = re.compile(r\"(?<![pq])-(\\d{1,2}|X|Y)(?![pq])\", re.IGNORECASE)\n",
    "_TRISOMY_RE = re.compile(r\"(?<![pq])\\+(\\d{1,2}|X|Y)(?![pq])\", re.IGNORECASE)\n",
    "_CHR_NUM_RE = re.compile(r\"(?<![pq])(\\d{1,2}|X|Y)(?![pq])\", re.IGNORECASE)\n",
    "\n",
    "_MINUS5_OR_DEL5Q_RE = re.compile(\n",
    "    r\"-(?:5)(?![pq])|del\\s*\\(\\s*5\\s*\\)\\s*\\(\\s*q\", re.IGNORECASE\n",
    ")\n",
    "_MINUS7_OR_DEL7Q_RE = re.compile(\n",
    "    r\"-(?:7)(?![pq])|del\\s*\\(\\s*7\\s*\\)\\s*\\(\\s*q\", re.IGNORECASE\n",
    ")\n",
    "_PLUS8_RE = re.compile(r\"\\+8(?![pq])\", re.IGNORECASE)\n",
    "_T_8_21_RE = re.compile(r\"t\\s*\\(\\s*8\\s*;\\s*21\\s*\\)\", re.IGNORECASE)\n",
    "_INV16_OR_T_16_16_RE = re.compile(\n",
    "    r\"(inv\\s*\\(\\s*16\\s*\\)|t\\s*\\(\\s*16\\s*;\\s*16\\s*\\))\", re.IGNORECASE\n",
    ")\n",
    "_T_15_17_RE = re.compile(r\"t\\s*\\(\\s*15\\s*;\\s*17\\s*\\)\", re.IGNORECASE)\n",
    "_STRUCTURAL_RE = re.compile(r\"(del|dup|inv|ins|i|t|add|der)\\s*\\(\", re.IGNORECASE)\n",
    "\n",
    "_INV3_OR_T3_3_RE = re.compile(\n",
    "    r\"(inv\\s*\\(\\s*3\\s*\\)\\s*\\(q21q26\\)|t\\s*\\(\\s*3\\s*;\\s*3\\s*\\)\\s*\\(q21;q26\\))\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "_T_6_9_RE = re.compile(\n",
    "    r\"t\\s*\\(\\s*6\\s*;\\s*9\\s*\\)\\s*\\(p23;q34\\)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "_T_9_22_RE = re.compile(\n",
    "    r\"t\\s*\\(\\s*9\\s*;\\s*22\\s*\\)\\s*\\(q34;q11\\)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "_ABN_17P_RE = re.compile(\n",
    "    r\"(del\\s*\\(\\s*17\\s*\\)\\s*\\(\\s*p|del\\s*\\(\\s*17p\\s*\\)|-17(?![pq])|add\\s*\\(\\s*17\\s*\\)\\s*\\(\\s*p)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "_BASELINE_CHR_RE = re.compile(r\"^\\s*(\\d{2})\\s*,\", re.IGNORECASE)\n",
    "_NORMAL_KARYO_RE = re.compile(\n",
    "    r\"^\\s*46\\s*,\\s*(XX|XY)\\s*(\\[\\d+\\])?\\s*$\", re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def _split_clones(karyo: str) -> List[str]:\n",
    "    \"\"\"Split ISCN string into clones separated by '/'.\"\"\"\n",
    "    return [c.strip() for c in str(karyo).split(\"/\") if c.strip()]\n",
    "\n",
    "\n",
    "def _extract_metaphases(clone: str) -> int:\n",
    "    \"\"\"Extract number of metaphases [n] from a clone.\"\"\"\n",
    "    m = re.search(r\"\\[(\\d+)\\]\", clone)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "\n",
    "def _count_events(clone: str) -> int:\n",
    "    \"\"\"\n",
    "    Count events in a clone (structural + trisomies + autosomal monosomies, ignoring -Y).\n",
    "    \"\"\"\n",
    "    n_struct = len(_ISCN_EVENT_RE.findall(clone))\n",
    "    n_mono = len(_MONOSOMY_RE.findall(clone))\n",
    "    n_tri = len(_TRISOMY_RE.findall(clone))\n",
    "    n_mono_minusY = len(\n",
    "        re.findall(r\"(?<![pq])-(?:Y)(?![pq])\", clone, flags=re.IGNORECASE)\n",
    "    )\n",
    "    return n_struct + n_tri + max(n_mono - n_mono_minusY, 0)\n",
    "\n",
    "\n",
    "def _chromosomes_altered(clone: str) -> int:\n",
    "    \"\"\"Number of distinct altered chromosomes (autosomes + sex chromosomes, ignoring Y).\"\"\"\n",
    "    nums = set()\n",
    "    for m in _MONOSOMY_RE.finditer(clone):\n",
    "        nums.add(m.group(1).upper())\n",
    "    for m in _TRISOMY_RE.finditer(clone):\n",
    "        nums.add(m.group(1).upper())\n",
    "    for ev in re.finditer(\n",
    "        r\"(del|dup|inv|ins|i|t|add|der)\\s*\\(([^)]+)\\)\", clone, flags=re.IGNORECASE\n",
    "    ):\n",
    "        for x in re.split(r\"[;,\\s]+\", ev.group(2)):\n",
    "            if _CHR_NUM_RE.fullmatch(x.strip()):\n",
    "                nums.add(x.strip().upper())\n",
    "    nums.discard(\"Y\")\n",
    "    return len(nums)\n",
    "\n",
    "\n",
    "def _has_structural(clone: str) -> bool:\n",
    "    return bool(_STRUCTURAL_RE.search(clone))\n",
    "\n",
    "\n",
    "def _autosomic_monosomies(clone: str) -> List[int]:\n",
    "    \"\"\"List autosomal monosomies in the clone.\"\"\"\n",
    "    return [\n",
    "        int(m.group(1))\n",
    "        for m in _MONOSOMY_RE.finditer(clone)\n",
    "        if m.group(1).upper() not in (\"X\", \"Y\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def _autosomic_trisomies(clone: str) -> List[int]:\n",
    "    \"\"\"List autosomal trisomies in the clone.\"\"\"\n",
    "    return [\n",
    "        int(m.group(1))\n",
    "        for m in _TRISOMY_RE.finditer(clone)\n",
    "        if m.group(1).upper() not in (\"X\", \"Y\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def _is_monosomal_karyotype(karyo: str) -> bool:\n",
    "    \"\"\"Monosomal karyotype: ≥ 2 autosomal monosomies or 1 autosomal monosomy + structural abnormality.\"\"\"\n",
    "    for c in _split_clones(karyo):\n",
    "        autos = _autosomic_monosomies(c)\n",
    "        has_struct = _has_structural(c)\n",
    "        if len(autos) >= 2:\n",
    "            return True\n",
    "        if len(autos) >= 1 and has_struct:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_complex_karyotype(karyo: str) -> bool:\n",
    "    \"\"\"Complex karyotype: ≥ 3 independent cytogenetic abnormalities.\"\"\"\n",
    "    for c in _split_clones(karyo):\n",
    "        c_wo_minusY = re.sub(\n",
    "            r\"(?<![pq])-(?:Y)(?![pq])\", \"\", c, flags=re.IGNORECASE\n",
    "        )\n",
    "        if _count_events(c_wo_minusY) >= 3:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _extract_baseline_chr_count(karyo: str) -> int:\n",
    "    \"\"\"Extract baseline chromosome count at start of ISCN string (e.g. '46,XX,...').\"\"\"\n",
    "    if not isinstance(karyo, str):\n",
    "        return -1\n",
    "    m = _BASELINE_CHR_RE.match(karyo)\n",
    "    if not m:\n",
    "        return -1\n",
    "    try:\n",
    "        return int(m.group(1))\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def _clone_flags(clone: str) -> Dict[str, bool]:\n",
    "    \"\"\"Compute per-clone flags for prognostic patterns.\"\"\"\n",
    "    return {\n",
    "        \"minus5_or_del5q\": bool(_MINUS5_OR_DEL5Q_RE.search(clone)),\n",
    "        \"minus7_or_del7q\": bool(_MINUS7_OR_DEL7Q_RE.search(clone)),\n",
    "        \"plus8\": bool(_PLUS8_RE.search(clone)),\n",
    "        \"t_8_21\": bool(_T_8_21_RE.search(clone)),\n",
    "        \"inv16_or_t_16_16\": bool(_INV16_OR_T_16_16_RE.search(clone)),\n",
    "        \"t_15_17\": bool(_T_15_17_RE.search(clone)),\n",
    "        \"inv3_or_t3_3\": bool(_INV3_OR_T3_3_RE.search(clone)),\n",
    "        \"t_6_9\": bool(_T_6_9_RE.search(clone)),\n",
    "        \"t_9_22\": bool(_T_9_22_RE.search(clone)),\n",
    "        \"abn17p\": bool(_ABN_17P_RE.search(clone)),\n",
    "        \"has_structural\": _has_structural(clone),\n",
    "        \"events_count\": _count_events(clone),\n",
    "        \"chrs_altered\": _chromosomes_altered(clone),\n",
    "        \"has_any_abn\": bool(\n",
    "            _ISCN_EVENT_RE.search(clone)\n",
    "            or _MONOSOMY_RE.search(clone)\n",
    "            or _TRISOMY_RE.search(clone)\n",
    "        ),\n",
    "        \"n_monosomies\": len(_MONOSOMY_RE.findall(clone)),\n",
    "        \"n_trisomies\": len(_TRISOMY_RE.findall(clone)),\n",
    "        \"n_structural_events\": len(_ISCN_EVENT_RE.findall(clone)),\n",
    "    }\n",
    "\n",
    "\n",
    "def add_cytogenetics_features(\n",
    "    df_input: pd.DataFrame,\n",
    "    col: str = \"CYTOGENETICS\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add cytogenetic features derived from ISCN karyotypes.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for k in df_input[col]:\n",
    "        # Handle missing / failed cytogenetics\n",
    "        if not isinstance(k, str) or not k.strip() or k.strip().lower() in {\n",
    "            \"nan\",\n",
    "            \"na\",\n",
    "            \"nd\",\n",
    "            \"notdone\",\n",
    "            \"failed\",\n",
    "            \"failure\",\n",
    "        }:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"is_cyto_missing_or_failed\": 1,\n",
    "                    \"is_normal_karyotype\": 0,\n",
    "                    \"is_abnormal_karyotype\": 0,\n",
    "                    \"has_any_abnormality\": 0,\n",
    "                    \"n_events\": 0,\n",
    "                    \"n_chromosomes_altered\": 0,\n",
    "                    \"n_monosomies_total\": 0,\n",
    "                    \"n_trisomies_total\": 0,\n",
    "                    \"n_structural_events_total\": 0,\n",
    "                    \"has_minus5_or_del5q\": 0,\n",
    "                    \"has_minus7_or_del7q\": 0,\n",
    "                    \"has_plus8\": 0,\n",
    "                    \"has_t_8_21\": 0,\n",
    "                    \"has_inv16_or_t_16_16\": 0,\n",
    "                    \"has_t_15_17\": 0,\n",
    "                    \"has_inv3_or_t3_3\": 0,\n",
    "                    \"has_t_6_9\": 0,\n",
    "                    \"has_t_9_22\": 0,\n",
    "                    \"has_abn17p\": 0,\n",
    "                    \"is_monosomal_karyotype\": 0,\n",
    "                    \"is_complex_karyotype\": 0,\n",
    "                    \"eln_like_flag_adverse_cyto\": 0,\n",
    "                    \"eln_like_flag_favorable_cyto\": 0,\n",
    "                    \"eln_like_flag_intermediate_cyto\": 0,\n",
    "                    \"eln_like_risk_cyto\": -1,\n",
    "                    \"baseline_chr_count\": -1,\n",
    "                    \"is_hypodiploid\": 0,\n",
    "                    \"is_hyperdiploid\": 0,\n",
    "                    \"is_near_tetraploid\": 0,\n",
    "                    \"total_metaphases\": 0,\n",
    "                    \"max_clone_size\": 0.0,\n",
    "                    \"max_adverse_clone_size\": 0.0,\n",
    "                    \"has_small_adverse_subclone\": 0,\n",
    "                    \"prop_any_abnormal\": 0.0,\n",
    "                    \"prop_adverse_5_7\": 0.0,\n",
    "                    \"prop_plus8\": 0.0,\n",
    "                    \"prop_favorable_core\": 0.0,\n",
    "                    \"n_autosomal_monosomies\": 0,\n",
    "                    \"n_autosomal_trisomies\": 0,\n",
    "                    \"worst_clone_events\": 0,\n",
    "                    \"worst_clone_is_adverse\": 0,\n",
    "                    \"has_any_rare_adverse_cyto\": 0,\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        clones = _split_clones(k)\n",
    "        clone_info = []\n",
    "        total_meta_known = 0\n",
    "\n",
    "        n_autosomal_mono_tot = 0\n",
    "        n_autosomal_tri_tot = 0\n",
    "\n",
    "        for c in clones:\n",
    "            n_meta = _extract_metaphases(c)\n",
    "            flags = _clone_flags(c)\n",
    "\n",
    "            n_autosomal_mono_tot += len(_autosomic_monosomies(c))\n",
    "            n_autosomal_tri_tot += len(_autosomic_trisomies(c))\n",
    "\n",
    "            clone_info.append((c, n_meta, flags))\n",
    "            total_meta_known += n_meta\n",
    "\n",
    "        any_abn = any(f[\"has_any_abn\"] for _, _, f in clone_info)\n",
    "        n_events = sum(f[\"events_count\"] for _, _, f in clone_info)\n",
    "        n_chrs = max([f[\"chrs_altered\"] for _, _, f in clone_info] + [0])\n",
    "\n",
    "        n_mono_tot = sum(f[\"n_monosomies\"] for _, _, f in clone_info)\n",
    "        n_tris_tot = sum(f[\"n_trisomies\"] for _, _, f in clone_info)\n",
    "        n_struct_tot = sum(f[\"n_structural_events\"] for _, _, f in clone_info)\n",
    "\n",
    "        has_minus5_or_del5q = any(\n",
    "            f[\"minus5_or_del5q\"] for _, _, f in clone_info\n",
    "        )\n",
    "        has_minus7_or_del7q = any(\n",
    "            f[\"minus7_or_del7q\"] for _, _, f in clone_info\n",
    "        )\n",
    "        has_plus8 = any(f[\"plus8\"] for _, _, f in clone_info)\n",
    "        has_t_8_21 = any(f[\"t_8_21\"] for _, _, f in clone_info)\n",
    "        has_inv16_or_t_16_16 = any(\n",
    "            f[\"inv16_or_t_16_16\"] for _, _, f in clone_info\n",
    "        )\n",
    "        has_t_15_17 = any(f[\"t_15_17\"] for _, _, f in clone_info)\n",
    "        has_inv3_or_t3_3 = any(f[\"inv3_or_t3_3\"] for _, _, f in clone_info)\n",
    "        has_t_6_9 = any(f[\"t_6_9\"] for _, _, f in clone_info)\n",
    "        has_t_9_22 = any(f[\"t_9_22\"] for _, _, f in clone_info)\n",
    "        has_abn17p = any(f[\"abn17p\"] for _, _, f in clone_info)\n",
    "\n",
    "        is_mk = _is_monosomal_karyotype(k)\n",
    "        is_ck = _is_complex_karyotype(k)\n",
    "\n",
    "        baseline_chr = _extract_baseline_chr_count(k)\n",
    "        is_hypo = int(baseline_chr != -1 and baseline_chr < 46)\n",
    "        is_hyper = int(baseline_chr != -1 and 46 < baseline_chr < 50)\n",
    "        is_near_tet = int(baseline_chr != -1 and baseline_chr >= 80)\n",
    "\n",
    "        is_normal = int(bool(_NORMAL_KARYO_RE.match(k)))\n",
    "        is_abnormal = int(not is_normal and any_abn)\n",
    "        is_missing = 0\n",
    "\n",
    "        eln_favorable = bool(\n",
    "            has_t_8_21 or has_inv16_or_t_16_16 or has_t_15_17\n",
    "        )\n",
    "        eln_adverse_basic = bool(\n",
    "            is_mk or is_ck or has_minus5_or_del5q or has_minus7_or_del7q\n",
    "        )\n",
    "        eln_adverse_extended = bool(\n",
    "            eln_adverse_basic\n",
    "            or has_inv3_or_t3_3\n",
    "            or has_t_6_9\n",
    "            or has_t_9_22\n",
    "            or has_abn17p\n",
    "        )\n",
    "\n",
    "        if is_missing:\n",
    "            eln_risk = -1\n",
    "        else:\n",
    "            if eln_adverse_extended:\n",
    "                eln_risk = 2\n",
    "            elif eln_favorable:\n",
    "                eln_risk = 0\n",
    "            else:\n",
    "                eln_risk = 1\n",
    "\n",
    "        has_any_rare_adverse_cyto = int(\n",
    "            has_inv3_or_t3_3 or has_t_6_9 or has_t_9_22 or has_abn17p\n",
    "        )\n",
    "\n",
    "        # Worst clone by number of events\n",
    "        worst_clone_events = 0\n",
    "        worst_clone_is_adverse = 0\n",
    "        if clone_info:\n",
    "            max_events = -1\n",
    "            worst_is_adverse = 0\n",
    "            for _, _, f in clone_info:\n",
    "                ev = f[\"events_count\"]\n",
    "                is_extended_adverse_clone = (\n",
    "                    f[\"minus5_or_del5q\"]\n",
    "                    or f[\"minus7_or_del7q\"]\n",
    "                    or f[\"inv3_or_t3_3\"]\n",
    "                    or f[\"t_6_9\"]\n",
    "                    or f[\"t_9_22\"]\n",
    "                    or f[\"abn17p\"]\n",
    "                )\n",
    "                if ev > max_events:\n",
    "                    max_events = ev\n",
    "                    worst_is_adverse = int(is_extended_adverse_clone)\n",
    "            worst_clone_events = int(max_events if max_events >= 0 else 0)\n",
    "            worst_clone_is_adverse = int(worst_is_adverse)\n",
    "\n",
    "        def _prop(cond_fn):\n",
    "            if total_meta_known == 0:\n",
    "                return 0.0\n",
    "            pos = sum(\n",
    "                n_meta\n",
    "                for _, n_meta, f in clone_info\n",
    "                if n_meta and cond_fn(f)\n",
    "            )\n",
    "            return pos / total_meta_known if total_meta_known else 0.0\n",
    "\n",
    "        prop_any_abnormal = float(_prop(lambda f: f[\"has_any_abn\"]))\n",
    "        prop_adverse_5_7 = float(\n",
    "            _prop(lambda f: f[\"minus5_or_del5q\"] or f[\"minus7_or_del7q\"])\n",
    "        )\n",
    "        prop_plus8 = float(_prop(lambda f: f[\"plus8\"]))\n",
    "        prop_favorable_core = float(\n",
    "            _prop(lambda f: f[\"t_8_21\"] or f[\"inv16_or_t_16_16\"])\n",
    "        )\n",
    "\n",
    "        max_clone_prop = 0.0\n",
    "        max_adverse_prop = 0.0\n",
    "        has_small_adverse_subclone = 0\n",
    "\n",
    "        if total_meta_known > 0:\n",
    "            for _, n_meta, f in clone_info:\n",
    "                if not n_meta:\n",
    "                    continue\n",
    "                p = n_meta / total_meta_known\n",
    "                if p > max_clone_prop:\n",
    "                    max_clone_prop = p\n",
    "\n",
    "                is_extended_adverse_clone = (\n",
    "                    f[\"minus5_or_del5q\"]\n",
    "                    or f[\"minus7_or_del7q\"]\n",
    "                    or f[\"inv3_or_t3_3\"]\n",
    "                    or f[\"t_6_9\"]\n",
    "                    or f[\"t_9_22\"]\n",
    "                    or f[\"abn17p\"]\n",
    "                )\n",
    "                if is_extended_adverse_clone:\n",
    "                    if p > max_adverse_prop:\n",
    "                        max_adverse_prop = p\n",
    "                    if 0.0 < p < 0.3:\n",
    "                        has_small_adverse_subclone = 1\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"is_cyto_missing_or_failed\": int(is_missing),\n",
    "                \"is_normal_karyotype\": int(is_normal),\n",
    "                \"is_abnormal_karyotype\": int(is_abnormal),\n",
    "                \"has_any_abnormality\": int(any_abn),\n",
    "                \"n_events\": int(n_events),\n",
    "                \"n_chromosomes_altered\": int(n_chrs),\n",
    "                \"n_monosomies_total\": int(n_mono_tot),\n",
    "                \"n_trisomies_total\": int(n_tris_tot),\n",
    "                \"n_structural_events_total\": int(n_struct_tot),\n",
    "                \"has_minus5_or_del5q\": int(has_minus5_or_del5q),\n",
    "                \"has_minus7_or_del7q\": int(has_minus7_or_del7q),\n",
    "                \"has_plus8\": int(has_plus8),\n",
    "                \"has_t_8_21\": int(has_t_8_21),\n",
    "                \"has_inv16_or_t_16_16\": int(has_inv16_or_t_16_16),\n",
    "                \"has_t_15_17\": int(has_t_15_17),\n",
    "                \"has_inv3_or_t3_3\": int(has_inv3_or_t3_3),\n",
    "                \"has_t_6_9\": int(has_t_6_9),\n",
    "                \"has_t_9_22\": int(has_t_9_22),\n",
    "                \"has_abn17p\": int(has_abn17p),\n",
    "                \"is_monosomal_karyotype\": int(is_mk),\n",
    "                \"is_complex_karyotype\": int(is_ck),\n",
    "                \"eln_like_flag_adverse_cyto\": int(eln_adverse_extended),\n",
    "                \"eln_like_flag_favorable_cyto\": int(eln_favorable),\n",
    "                \"eln_like_flag_intermediate_cyto\": int(eln_risk == 1),\n",
    "                \"eln_like_risk_cyto\": int(eln_risk),\n",
    "                \"baseline_chr_count\": int(baseline_chr),\n",
    "                \"is_hypodiploid\": int(is_hypo),\n",
    "                \"is_hyperdiploid\": int(is_hyper),\n",
    "                \"is_near_tetraploid\": int(is_near_tet),\n",
    "                \"total_metaphases\": int(total_meta_known),\n",
    "                \"max_clone_size\": float(max_clone_prop),\n",
    "                \"max_adverse_clone_size\": float(max_adverse_prop),\n",
    "                \"has_small_adverse_subclone\": int(has_small_adverse_subclone),\n",
    "                \"prop_any_abnormal\": float(prop_any_abnormal),\n",
    "                \"prop_adverse_5_7\": float(prop_adverse_5_7),\n",
    "                \"prop_plus8\": float(prop_plus8),\n",
    "                \"prop_favorable_core\": float(prop_favorable_core),\n",
    "                \"n_autosomal_monosomies\": int(n_autosomal_mono_tot),\n",
    "                \"n_autosomal_trisomies\": int(n_autosomal_tri_tot),\n",
    "                \"worst_clone_events\": int(worst_clone_events),\n",
    "                \"worst_clone_is_adverse\": int(worst_clone_is_adverse),\n",
    "                \"has_any_rare_adverse_cyto\": int(has_any_rare_adverse_cyto),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    features_df = pd.DataFrame(rows, index=df_input.index)\n",
    "    return pd.concat([df_input.copy(), features_df], axis=1).drop(columns=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e47098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich clinical data with cytogenetics features\n",
    "X_enhanced = add_cytogenetics_features(X_w_mutation)\n",
    "X_eval_enhanced = add_cytogenetics_features(X_eval_w_mutation)\n",
    "\n",
    "cytogenetics_features = [\n",
    "    \"is_cyto_missing_or_failed\",\n",
    "    \"is_normal_karyotype\",\n",
    "    \"is_abnormal_karyotype\",\n",
    "    \"has_any_abnormality\",\n",
    "    \"n_events\",\n",
    "    \"n_chromosomes_altered\",\n",
    "    \"n_monosomies_total\",\n",
    "    \"n_trisomies_total\",\n",
    "    \"n_structural_events_total\",\n",
    "    \"has_minus5_or_del5q\",\n",
    "    \"has_minus7_or_del7q\",\n",
    "    \"has_plus8\",\n",
    "    \"has_t_8_21\",\n",
    "    \"has_inv16_or_t_16_16\",\n",
    "    \"has_t_15_17\",\n",
    "    \"has_inv3_or_t3_3\",\n",
    "    \"has_t_6_9\",\n",
    "    \"has_t_9_22\",\n",
    "    \"has_abn17p\",\n",
    "    \"is_monosomal_karyotype\",\n",
    "    \"is_complex_karyotype\",\n",
    "    \"eln_like_flag_adverse_cyto\",\n",
    "    \"eln_like_flag_favorable_cyto\",\n",
    "    \"eln_like_flag_intermediate_cyto\",\n",
    "    \"eln_like_risk_cyto\",\n",
    "    \"baseline_chr_count\",\n",
    "    \"is_hypodiploid\",\n",
    "    \"is_hyperdiploid\",\n",
    "    \"is_near_tetraploid\",\n",
    "    \"total_metaphases\",\n",
    "    \"max_clone_size\",\n",
    "    \"max_adverse_clone_size\",\n",
    "    \"has_small_adverse_subclone\",\n",
    "    \"prop_any_abnormal\",\n",
    "    \"prop_adverse_5_7\",\n",
    "    \"prop_plus8\",\n",
    "    \"prop_favorable_core\",\n",
    "    \"n_autosomal_monosomies\",\n",
    "    \"n_autosomal_trisomies\",\n",
    "    \"worst_clone_events\",\n",
    "    \"worst_clone_is_adverse\",\n",
    "    \"has_any_rare_adverse_cyto\",\n",
    "]\n",
    "\n",
    "# Remove near-constant features (>=95% identical) on training\n",
    "nearly_constant_features = []\n",
    "protected = {\n",
    "    \"has_minus5_or_del5q\",\n",
    "    \"has_minus7_or_del7q\",\n",
    "    \"has_abn17p\",\n",
    "    \"has_inv3_or_t3_3\",\n",
    "    \"has_t_6_9\",\n",
    "    \"has_t_9_22\",\n",
    "    \"is_monosomal_karyotype\",\n",
    "    \"is_complex_karyotype\",\n",
    "    \"has_t_15_17\",\n",
    "    \"has_t_8_21\",\n",
    "    \"has_inv16_or_t_16_16\",\n",
    "}\n",
    "\n",
    "for col in cytogenetics_features:\n",
    "    value_counts = X_enhanced[col].value_counts(dropna=False)\n",
    "    if len(value_counts) > 0:\n",
    "        max_proportion = value_counts.iloc[0] / len(X_enhanced)\n",
    "        if max_proportion >= 0.95 and col not in protected:\n",
    "            nearly_constant_features.append(col)\n",
    "            print(\n",
    "                f\"Feature '{col}' is nearly constant \"\n",
    "                f\"({max_proportion:.2%} of samples identical) -> dropped.\"\n",
    "            )\n",
    "\n",
    "X_enhanced = X_enhanced.drop(columns=nearly_constant_features)\n",
    "X_eval_enhanced = X_eval_enhanced.drop(columns=nearly_constant_features)\n",
    "\n",
    "cytogenetics_features = [\n",
    "    f for f in cytogenetics_features if f not in nearly_constant_features\n",
    "]\n",
    "\n",
    "# Scale remaining cytogenetics features\n",
    "cytogenetics_scaler = RobustScaler()\n",
    "print(\n",
    "    f\"Fitting RobustScaler for {len(cytogenetics_features)} cytogenetics features \"\n",
    "    \"on training data.\"\n",
    ")\n",
    "X_enhanced[cytogenetics_features] = cytogenetics_scaler.fit_transform(\n",
    "    X_enhanced[cytogenetics_features]\n",
    ")\n",
    "X_eval_enhanced[cytogenetics_features] = cytogenetics_scaler.transform(\n",
    "    X_eval_enhanced[cytogenetics_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71dd65",
   "metadata": {},
   "source": [
    "## 6. Merge with survival targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02df93c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "df_enhanced = X_enhanced.merge(target_df, on=\"ID\", how=\"left\")\n",
    "df_eval_enhanced = X_eval_enhanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12f5fd",
   "metadata": {},
   "source": [
    "## 7. Gene-level Features (one-hot per gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gene_features(\n",
    "    df_clinical_enhanced: pd.DataFrame,\n",
    "    df_molecular: pd.DataFrame,\n",
    "    gene_list=None,\n",
    "    top_k: int = 150,\n",
    "):\n",
    "    \"\"\"\n",
    "    Add one-hot features for genes.\n",
    "    - If gene_list is None: use top_k most frequent genes in df_molecular\n",
    "      (frequency = number of distinct patients with a mutation in the gene).\n",
    "    - Otherwise: use the provided gene_list (for consistent train/val columns).\n",
    "    \"\"\"\n",
    "    if gene_list is None:\n",
    "        gene_counts = (\n",
    "            df_molecular[[\"ID\", \"GENE\"]]\n",
    "            .drop_duplicates()[\"GENE\"]\n",
    "            .value_counts()\n",
    "        )\n",
    "        gene_list = gene_counts.nlargest(top_k).index.tolist()\n",
    "        print(f\"Number of genes used (top {top_k}): {len(gene_list)}\")\n",
    "\n",
    "    df_filtered = df_molecular[df_molecular[\"GENE\"].isin(gene_list)].copy()\n",
    "\n",
    "    gene_pivot = pd.crosstab(df_filtered[\"ID\"], df_filtered[\"GENE\"])\n",
    "\n",
    "    for g in gene_list:\n",
    "        if g not in gene_pivot.columns:\n",
    "            gene_pivot[g] = 0\n",
    "\n",
    "    gene_pivot = gene_pivot[gene_list]\n",
    "\n",
    "    gene_pivot = (gene_pivot > 0).astype(int)\n",
    "    gene_pivot.columns = [f\"Gene_{col}\" for col in gene_pivot.columns]\n",
    "\n",
    "    df_final = df_clinical_enhanced.merge(gene_pivot, on=\"ID\", how=\"left\")\n",
    "\n",
    "    new_cols = [c for c in df_final.columns if c.startswith(\"Gene_\")]\n",
    "    df_final[new_cols] = df_final[new_cols].fillna(0)\n",
    "\n",
    "    return df_final, gene_list\n",
    "\n",
    "\n",
    "mol_train_raw = pd.read_csv(\"../../data/molecular_train.csv\")\n",
    "mol_val_raw = pd.read_csv(\"../../data/molecular_val.csv\")\n",
    "\n",
    "df_train_pivot, gene_list_ref = add_gene_features(\n",
    "    df_clinical_enhanced=df_enhanced,\n",
    "    df_molecular=mol_train_raw,\n",
    "    gene_list=None,\n",
    "    top_k=150,\n",
    ")\n",
    "\n",
    "df_val_pivot, _ = add_gene_features(\n",
    "    df_clinical_enhanced=df_eval_enhanced,\n",
    "    df_molecular=mol_val_raw,\n",
    "    gene_list=gene_list_ref,\n",
    ")\n",
    "\n",
    "# Harmonize columns between train and val\n",
    "gene_cols = [c for c in df_train_pivot.columns if c.startswith(\"Gene_\")]\n",
    "\n",
    "train_clinical_cols = [\n",
    "    c for c in df_train_pivot.columns if not c.startswith(\"Gene_\")\n",
    "]\n",
    "val_clinical_cols = [\n",
    "    c for c in df_val_pivot.columns if not c.startswith(\"Gene_\")\n",
    "]\n",
    "\n",
    "train_clinical_cols_for_common = [\n",
    "    c for c in train_clinical_cols if c not in [\"OS_YEARS\", \"OS_STATUS\"]\n",
    "]\n",
    "\n",
    "common_clinical = sorted(\n",
    "    set(train_clinical_cols_for_common).intersection(val_clinical_cols)\n",
    ")\n",
    "\n",
    "final_features_train = common_clinical + gene_cols + [\"OS_YEARS\", \"OS_STATUS\"]\n",
    "final_features_val = common_clinical + gene_cols\n",
    "\n",
    "df_train_pivot = df_train_pivot[final_features_train]\n",
    "df_val_pivot = df_val_pivot[final_features_val]\n",
    "\n",
    "print(f\"Final train columns (with targets): {len(final_features_train)}\")\n",
    "print(f\"Final val columns (without targets): {len(final_features_val)}\")\n",
    "print(f\"Train shape: {df_train_pivot.shape}\")\n",
    "print(f\"Val shape:   {df_val_pivot.shape}\")\n",
    "print(f\"'OS_YEARS' in train: {'OS_YEARS' in df_train_pivot.columns}\")\n",
    "print(f\"'OS_STATUS' in train: {'OS_STATUS' in df_train_pivot.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88811e4b",
   "metadata": {},
   "source": [
    "# 8. Molecular Risk Score (regularized Cox model on gene features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd26f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cols = [c for c in df_train_pivot.columns if c.startswith(\"Gene_\")]\n",
    "\n",
    "cox_cols_train = [\"OS_YEARS\", \"OS_STATUS\"] + gene_cols\n",
    "cox_df_train = df_train_pivot[cox_cols_train].copy()\n",
    "cox_df_train[\"OS_STATUS\"] = cox_df_train[\"OS_STATUS\"].astype(int)\n",
    "\n",
    "gene_mat = cox_df_train[gene_cols]\n",
    "prevalence = gene_mat.mean(axis=0)\n",
    "\n",
    "min_prop = 0.005\n",
    "max_prop = 0.99\n",
    "\n",
    "valid_genes = prevalence[\n",
    "    (prevalence >= min_prop) & (prevalence <= max_prop)\n",
    "].index.tolist()\n",
    "print(f\"Initial number of genes: {len(gene_cols)}\")\n",
    "print(f\"Number of genes kept: {len(valid_genes)}\")\n",
    "\n",
    "cox_df_train = cox_df_train[[\"OS_YEARS\", \"OS_STATUS\"] + valid_genes].copy()\n",
    "\n",
    "X_all = cox_df_train[valid_genes].to_numpy(dtype=float)\n",
    "y_time_all = cox_df_train[\"OS_YEARS\"].to_numpy(dtype=float)\n",
    "y_event_all = cox_df_train[\"OS_STATUS\"].to_numpy(dtype=int)\n",
    "\n",
    "outer_splits = 5\n",
    "inner_splits = 3\n",
    "n_trials_nested = 30\n",
    "\n",
    "outer_cv = KFold(n_splits=outer_splits, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=inner_splits, shuffle=True, random_state=43)\n",
    "\n",
    "outer_c_indices = []\n",
    "best_params_per_outer_fold = []\n",
    "\n",
    "for fold_idx, (train_outer_idx, val_outer_idx) in enumerate(\n",
    "    outer_cv.split(X_all), start=1\n",
    "):\n",
    "    print(f\"\\n=== Outer fold {fold_idx}/{outer_splits} ===\")\n",
    "\n",
    "    X_train_outer = X_all[train_outer_idx]\n",
    "    y_time_train_outer = y_time_all[train_outer_idx]\n",
    "    y_event_train_outer = y_event_all[train_outer_idx]\n",
    "\n",
    "    X_val_outer = X_all[val_outer_idx]\n",
    "    y_time_val_outer = y_time_all[val_outer_idx]\n",
    "    y_event_val_outer = y_event_all[val_outer_idx]\n",
    "\n",
    "    def objective(trial):\n",
    "        penalizer = trial.suggest_float(\"penalizer\", 1e-4, 10.0, log=True)\n",
    "        l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0)\n",
    "\n",
    "        inner_c_indices = []\n",
    "\n",
    "        for train_inner_idx, val_inner_idx in inner_cv.split(X_train_outer):\n",
    "            X_train_inner = X_train_outer[train_inner_idx]\n",
    "            y_time_train_inner = y_time_train_outer[train_inner_idx]\n",
    "            y_event_train_inner = y_event_train_outer[train_inner_idx]\n",
    "\n",
    "            X_val_inner = X_train_outer[val_inner_idx]\n",
    "            y_time_val_inner = y_time_train_outer[val_inner_idx]\n",
    "            y_event_val_inner = y_event_train_outer[val_inner_idx]\n",
    "\n",
    "            scaler_inner = StandardScaler()\n",
    "            X_train_inner_scaled = scaler_inner.fit_transform(X_train_inner)\n",
    "            X_val_inner_scaled = scaler_inner.transform(X_val_inner)\n",
    "\n",
    "            train_df_inner = pd.DataFrame(\n",
    "                X_train_inner_scaled, columns=valid_genes\n",
    "            )\n",
    "            train_df_inner[\"OS_YEARS\"] = y_time_train_inner\n",
    "            train_df_inner[\"OS_STATUS\"] = y_event_train_inner\n",
    "\n",
    "            val_df_inner = pd.DataFrame(\n",
    "                X_val_inner_scaled, columns=valid_genes\n",
    "            )\n",
    "            val_df_inner[\"OS_YEARS\"] = y_time_val_inner\n",
    "            val_df_inner[\"OS_STATUS\"] = y_event_val_inner\n",
    "\n",
    "            cph_inner = CoxPHFitter(\n",
    "                penalizer=penalizer,\n",
    "                l1_ratio=l1_ratio,\n",
    "            )\n",
    "            cph_inner.fit(\n",
    "                train_df_inner,\n",
    "                duration_col=\"OS_YEARS\",\n",
    "                event_col=\"OS_STATUS\",\n",
    "            )\n",
    "\n",
    "            risk_scores_inner = -cph_inner.predict_partial_hazard(\n",
    "                val_df_inner[valid_genes]\n",
    "            ).values.ravel()\n",
    "\n",
    "            c_idx_inner = concordance_index(\n",
    "                y_time_val_inner,\n",
    "                risk_scores_inner,\n",
    "                y_event_val_inner,\n",
    "            )\n",
    "            inner_c_indices.append(c_idx_inner)\n",
    "\n",
    "        return float(np.mean(inner_c_indices))\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials_nested)\n",
    "\n",
    "    print(\"  Best inner CV C-index:\", study.best_value)\n",
    "    print(\"  Best params:\", study.best_params)\n",
    "    best_params_per_outer_fold.append(study.best_params)\n",
    "\n",
    "    scaler_outer = StandardScaler()\n",
    "    X_train_outer_scaled = scaler_outer.fit_transform(X_train_outer)\n",
    "    X_val_outer_scaled = scaler_outer.transform(X_val_outer)\n",
    "\n",
    "    train_df_outer = pd.DataFrame(\n",
    "        X_train_outer_scaled, columns=valid_genes\n",
    "    )\n",
    "    train_df_outer[\"OS_YEARS\"] = y_time_train_outer\n",
    "    train_df_outer[\"OS_STATUS\"] = y_event_train_outer\n",
    "\n",
    "    val_df_outer = pd.DataFrame(\n",
    "        X_val_outer_scaled, columns=valid_genes\n",
    "    )\n",
    "    val_df_outer[\"OS_YEARS\"] = y_time_val_outer\n",
    "    val_df_outer[\"OS_STATUS\"] = y_event_val_outer\n",
    "\n",
    "    cph_outer = CoxPHFitter(\n",
    "        penalizer=study.best_params[\"penalizer\"],\n",
    "        l1_ratio=study.best_params[\"l1_ratio\"],\n",
    "    )\n",
    "    cph_outer.fit(\n",
    "        train_df_outer,\n",
    "        duration_col=\"OS_YEARS\",\n",
    "        event_col=\"OS_STATUS\",\n",
    "    )\n",
    "\n",
    "    risk_scores_outer = -cph_outer.predict_partial_hazard(\n",
    "        val_df_outer[valid_genes]\n",
    "    ).values.ravel()\n",
    "\n",
    "    c_idx_outer = concordance_index(\n",
    "        y_time_val_outer,\n",
    "        risk_scores_outer,\n",
    "        y_event_val_outer,\n",
    "    )\n",
    "    outer_c_indices.append(c_idx_outer)\n",
    "    print(f\"  Outer fold {fold_idx} C-index (nested): {c_idx_outer:.4f}\")\n",
    "\n",
    "print(\"\\n=== Nested CV results ===\")\n",
    "print(\"Mean outer C-index:\", np.mean(outer_c_indices))\n",
    "print(\"Std outer C-index: \", np.std(outer_c_indices))\n",
    "print(\"\\nBest params per outer fold:\")\n",
    "for i, p in enumerate(best_params_per_outer_fold, start=1):\n",
    "    print(f\"  Fold {i}: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc14644",
   "metadata": {},
   "source": [
    "## 9. Final Cox model on all training data (gene features only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ff995",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_penalizers = [p[\"penalizer\"] for p in best_params_per_outer_fold]\n",
    "best_l1_ratios = [p[\"l1_ratio\"] for p in best_params_per_outer_fold]\n",
    "\n",
    "final_penalizer = float(np.median(best_penalizers))\n",
    "final_l1_ratio = float(np.median(best_l1_ratios))\n",
    "\n",
    "print(\"\\nFinal hyperparameters (median over outer folds):\")\n",
    "print(f\"  penalizer = {final_penalizer:.4g}\")\n",
    "print(f\"  l1_ratio  = {final_l1_ratio:.4f}\")\n",
    "\n",
    "scaler_final = StandardScaler()\n",
    "X_all_scaled = scaler_final.fit_transform(X_all)\n",
    "\n",
    "cox_df_train_scaled = pd.DataFrame(X_all_scaled, columns=valid_genes)\n",
    "cox_df_train_scaled[\"OS_YEARS\"] = y_time_all\n",
    "cox_df_train_scaled[\"OS_STATUS\"] = y_event_all\n",
    "\n",
    "cph_best = CoxPHFitter(\n",
    "    penalizer=final_penalizer,\n",
    "    l1_ratio=final_l1_ratio,\n",
    ")\n",
    "cph_best.fit(\n",
    "    cox_df_train_scaled,\n",
    "    duration_col=\"OS_YEARS\",\n",
    "    event_col=\"OS_STATUS\",\n",
    ")\n",
    "\n",
    "print(\"\\nFinal CoxPH model (cph_best) trained on full training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d49e3",
   "metadata": {},
   "source": [
    "## 10. Compute Molecular Risk Score and Quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f411bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk score on training data\n",
    "X_train_genes = cox_df_train_scaled[valid_genes]\n",
    "raw_score = cph_best.predict_log_partial_hazard(X_train_genes)\n",
    "\n",
    "cindex_pos = concordance_index(\n",
    "    cox_df_train_scaled[\"OS_YEARS\"].values,\n",
    "    raw_score.values,\n",
    "    cox_df_train_scaled[\"OS_STATUS\"].values,\n",
    ")\n",
    "\n",
    "cindex_neg = concordance_index(\n",
    "    cox_df_train_scaled[\"OS_YEARS\"].values,\n",
    "    (-raw_score).values,\n",
    "    cox_df_train_scaled[\"OS_STATUS\"].values,\n",
    ")\n",
    "\n",
    "if cindex_neg > cindex_pos:\n",
    "    print(\n",
    "        f\"Inverting score orientation \"\n",
    "        f\"(C-index {cindex_pos:.3f} -> {cindex_neg:.3f})\"\n",
    "    )\n",
    "    genetic_risk = -raw_score\n",
    "    cindex_final = cindex_neg\n",
    "else:\n",
    "    print(f\"Score orientation kept (C-index {cindex_pos:.3f})\")\n",
    "    genetic_risk = raw_score\n",
    "    cindex_final = cindex_pos\n",
    "\n",
    "print(\"Final C-index of genetic risk score (train):\", cindex_final)\n",
    "\n",
    "df_train_pivot[\"RiskScore\"] = genetic_risk.values\n",
    "\n",
    "# Risk score for validation data (scaled with same scaler)\n",
    "features_used = valid_genes\n",
    "X_val_features = df_val_pivot[features_used].to_numpy(dtype=float)\n",
    "X_val_scaled = scaler_final.transform(X_val_features)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=features_used)\n",
    "\n",
    "df_val_pivot[\"RiskScore\"] = cph_best.predict_log_partial_hazard(\n",
    "    X_val_scaled_df\n",
    ")\n",
    "\n",
    "# Quartile-based risk groups (same cutpoints for train/val)\n",
    "qs = np.quantile(df_train_pivot[\"RiskScore\"], [0, 0.25, 0.5, 0.75, 1.0])\n",
    "\n",
    "df_train_pivot[\"RiskQuartile\"] = pd.cut(\n",
    "    df_train_pivot[\"RiskScore\"],\n",
    "    bins=qs,\n",
    "    labels=[\"Q1 (lowest)\", \"Q2\", \"Q3\", \"Q4 (highest)\"],\n",
    "    include_lowest=True,\n",
    ")\n",
    "df_val_pivot[\"RiskQuartile\"] = pd.cut(\n",
    "    df_val_pivot[\"RiskScore\"],\n",
    "    bins=qs,\n",
    "    labels=[\"Q1 (lowest)\", \"Q2\", \"Q3\", \"Q4 (highest)\"],\n",
    "    include_lowest=True,\n",
    ")\n",
    "\n",
    "df_train_pivot = pd.get_dummies(\n",
    "    df_train_pivot,\n",
    "    columns=[\"RiskQuartile\"],\n",
    "    drop_first=False,\n",
    ")\n",
    "df_val_pivot = pd.get_dummies(\n",
    "    df_val_pivot,\n",
    "    columns=[\"RiskQuartile\"],\n",
    "    drop_first=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b98c44",
   "metadata": {},
   "source": [
    "# 11. Additional Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04649365",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cytogenetic risk score\n",
    "df_train_pivot[\"cyto_risk_score\"] = (\n",
    "    3 * df_train_pivot[\"is_monosomal_karyotype\"]\n",
    "    + 3 * df_train_pivot[\"is_complex_karyotype\"]\n",
    "    + 2 * df_train_pivot[\"has_minus7_or_del7q\"]\n",
    "    + 2 * df_train_pivot[\"has_minus5_or_del5q\"]\n",
    "    + 1 * df_train_pivot[\"has_plus8\"]\n",
    ")\n",
    "\n",
    "df_val_pivot[\"cyto_risk_score\"] = (\n",
    "    3 * df_val_pivot[\"is_monosomal_karyotype\"]\n",
    "    + 3 * df_val_pivot[\"is_complex_karyotype\"]\n",
    "    + 2 * df_val_pivot[\"has_minus7_or_del7q\"]\n",
    "    + 2 * df_val_pivot[\"has_minus5_or_del5q\"]\n",
    "    + 1 * df_val_pivot[\"has_plus8\"]\n",
    ")\n",
    "\n",
    "# Gene–cytogenetic interaction terms\n",
    "df_train_pivot[\"TP53_complex_interaction\"] = (\n",
    "    df_train_pivot[\"Gene_TP53\"] * df_train_pivot[\"is_complex_karyotype\"]\n",
    ")\n",
    "df_train_pivot[\"ASXL1_minus7_interaction\"] = (\n",
    "    df_train_pivot[\"Gene_ASXL1\"] * df_train_pivot[\"has_minus7_or_del7q\"]\n",
    ")\n",
    "df_train_pivot[\"NPM1_normal_interaction\"] = df_train_pivot[\"Gene_NPM1\"] * (\n",
    "    1 - df_train_pivot[\"prop_any_abnormal\"]\n",
    ")\n",
    "\n",
    "df_val_pivot[\"TP53_complex_interaction\"] = (\n",
    "    df_val_pivot[\"Gene_TP53\"] * df_val_pivot[\"is_complex_karyotype\"]\n",
    ")\n",
    "df_val_pivot[\"ASXL1_minus7_interaction\"] = (\n",
    "    df_val_pivot[\"Gene_ASXL1\"] * df_val_pivot[\"has_minus7_or_del7q\"]\n",
    ")\n",
    "df_val_pivot[\"NPM1_normal_interaction\"] = df_val_pivot[\"Gene_NPM1\"] * (\n",
    "    1 - df_val_pivot[\"prop_any_abnormal\"]\n",
    ")\n",
    "\n",
    "# High-risk chromosome load\n",
    "df_train_pivot[\"high_risk_chr_load\"] = (\n",
    "    df_train_pivot[\"CHR_5_count\"]\n",
    "    + df_train_pivot[\"CHR_7_count\"]\n",
    "    + df_train_pivot[\"CHR_17_count\"]\n",
    ")\n",
    "\n",
    "df_val_pivot[\"high_risk_chr_load\"] = (\n",
    "    df_val_pivot[\"CHR_5_count\"]\n",
    "    + df_val_pivot[\"CHR_7_count\"]\n",
    "    + df_val_pivot[\"CHR_17_count\"]\n",
    ")\n",
    "\n",
    "# Simple gene-based risk scores\n",
    "df_train_pivot[\"risk_score_high_genes\"] = (\n",
    "    df_train_pivot[\"Gene_TP53\"]\n",
    "    + df_train_pivot[\"Gene_ASXL1\"]\n",
    "    + df_train_pivot[\"Gene_RUNX1\"]\n",
    ")\n",
    "df_train_pivot[\"risk_score_favorable_genes\"] = (\n",
    "    df_train_pivot[\"Gene_NPM1\"] + df_train_pivot[\"Gene_CEBPA\"]\n",
    ")\n",
    "\n",
    "df_val_pivot[\"risk_score_high_genes\"] = (\n",
    "    df_val_pivot[\"Gene_TP53\"]\n",
    "    + df_val_pivot[\"Gene_ASXL1\"]\n",
    "    + df_val_pivot[\"Gene_RUNX1\"]\n",
    ")\n",
    "df_val_pivot[\"risk_score_favorable_genes\"] = (\n",
    "    df_val_pivot[\"Gene_NPM1\"] + df_val_pivot[\"Gene_CEBPA\"]\n",
    ")\n",
    "\n",
    "# Splicing and signaling mutation counts\n",
    "df_train_pivot[\"n_splicing_mut\"] = df_train_pivot[\n",
    "    [\"Gene_U2AF1\", \"Gene_SRSF2\", \"Gene_SF3B1\", \"Gene_ZRSR2\"]\n",
    "].sum(axis=1)\n",
    "df_train_pivot[\"n_signaling_mut\"] = df_train_pivot[\n",
    "    [\"Gene_NRAS\", \"Gene_KRAS\", \"Gene_JAK2\", \"Gene_CBL\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "df_val_pivot[\"n_splicing_mut\"] = df_val_pivot[\n",
    "    [\"Gene_U2AF1\", \"Gene_SRSF2\", \"Gene_SF3B1\", \"Gene_ZRSR2\"]\n",
    "].sum(axis=1)\n",
    "df_val_pivot[\"n_signaling_mut\"] = df_val_pivot[\n",
    "    [\"Gene_NRAS\", \"Gene_KRAS\", \"Gene_JAK2\", \"Gene_CBL\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "# TP53–VAF interaction\n",
    "df_train_pivot[\"TP53_VAF_interaction\"] = (\n",
    "    df_train_pivot[\"Gene_TP53\"] * df_train_pivot[\"VAF_avg\"]\n",
    ")\n",
    "df_val_pivot[\"TP53_VAF_interaction\"] = (\n",
    "    df_val_pivot[\"Gene_TP53\"] * df_val_pivot[\"VAF_avg\"]\n",
    ")\n",
    "\n",
    "# Clinical ratios\n",
    "df_train_pivot[\"ANC_WBC_ratio\"] = df_train_pivot[\"ANC\"] / (\n",
    "    df_train_pivot[\"WBC\"] + 1\n",
    ")\n",
    "df_train_pivot[\"BLAST_WBC_ratio\"] = df_train_pivot[\"BM_BLAST\"] / (\n",
    "    df_train_pivot[\"WBC\"] + 1\n",
    ")\n",
    "\n",
    "df_val_pivot[\"ANC_WBC_ratio\"] = df_val_pivot[\"ANC\"] / (\n",
    "    df_val_pivot[\"WBC\"] + 1\n",
    ")\n",
    "df_val_pivot[\"BLAST_WBC_ratio\"] = df_val_pivot[\"BM_BLAST\"] / (\n",
    "    df_val_pivot[\"WBC\"] + 1\n",
    ")\n",
    "\n",
    "# Subclonality proxies\n",
    "df_train_pivot[\"major_clone_VAF\"] = df_train_pivot[\"VAF_max\"]\n",
    "df_train_pivot[\"subclonality\"] = df_train_pivot[\"VAF_std\"] / (\n",
    "    df_train_pivot[\"VAF_avg\"] + 1e-6\n",
    ")\n",
    "\n",
    "df_val_pivot[\"major_clone_VAF\"] = df_val_pivot[\"VAF_max\"]\n",
    "df_val_pivot[\"subclonality\"] = df_val_pivot[\"VAF_std\"] / (\n",
    "    df_val_pivot[\"VAF_avg\"] + 1e-6\n",
    ")\n",
    "\n",
    "# Additional karyotype score\n",
    "df_train_pivot[\"karyo_score_clinical\"] = (\n",
    "    3 * df_train_pivot[\"is_monosomal_karyotype\"]\n",
    "    + 2 * df_train_pivot[\"is_complex_karyotype\"]\n",
    "    + 2 * df_train_pivot[\"has_minus7_or_del7q\"]\n",
    "    + 1 * df_train_pivot[\"has_plus8\"]\n",
    ")\n",
    "\n",
    "df_val_pivot[\"karyo_score_clinical\"] = (\n",
    "    3 * df_val_pivot[\"is_monosomal_karyotype\"]\n",
    "    + 2 * df_val_pivot[\"is_complex_karyotype\"]\n",
    "    + 2 * df_val_pivot[\"has_minus7_or_del7q\"]\n",
    "    + 1 * df_val_pivot[\"has_plus8\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ea540",
   "metadata": {},
   "source": [
    "## 12. VAF Entropy (distribution of clone sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988b468",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_vaf_entropy(df_mut: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute Shannon entropy of VAF distribution per patient.\n",
    "    df_mut must contain at least:\n",
    "      - 'ID'\n",
    "      - 'VAF'\n",
    "    \"\"\"\n",
    "\n",
    "    def entropy_from_vaf(vaf_list):\n",
    "        vaf_arr = np.array(vaf_list, dtype=float)\n",
    "        total = vaf_arr.sum()\n",
    "        if total <= 0:\n",
    "            return 0.0\n",
    "        p = vaf_arr / total\n",
    "        p = p[p > 0]\n",
    "        return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "    entropy_per_patient = (\n",
    "        df_mut.groupby(\"ID\")[\"VAF\"]\n",
    "        .apply(entropy_from_vaf)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"VAF\": \"vaf_entropy\"})\n",
    "    )\n",
    "    return entropy_per_patient\n",
    "\n",
    "\n",
    "entropy_train = compute_vaf_entropy(maf_df)\n",
    "entropy_eval = compute_vaf_entropy(maf_eval)\n",
    "\n",
    "df_train_pivot = df_train_pivot.merge(entropy_train, on=\"ID\", how=\"left\")\n",
    "df_val_pivot = df_val_pivot.merge(entropy_eval, on=\"ID\", how=\"left\")\n",
    "\n",
    "df_train_pivot[\"vaf_entropy\"] = df_train_pivot[\"vaf_entropy\"].fillna(0)\n",
    "df_val_pivot[\"vaf_entropy\"] = df_val_pivot[\"vaf_entropy\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1102f1ef",
   "metadata": {},
   "source": [
    "## 13. Additional ratios and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_ratio(num: pd.Series, den: pd.Series) -> pd.Series:\n",
    "    num = num.astype(float)\n",
    "    den = den.astype(float)\n",
    "    res = num / den\n",
    "    res[~np.isfinite(res)] = np.nan\n",
    "    return res\n",
    "\n",
    "\n",
    "# Replace sentinel HB == -1 with NaN\n",
    "for df_ in [df_train_pivot, df_val_pivot]:\n",
    "    df_.loc[df_[\"HB\"] == -1, \"HB\"] = np.nan\n",
    "\n",
    "hb_median = df_train_pivot[\"HB\"].median()\n",
    "\n",
    "df_train_pivot[\"HB\"] = df_train_pivot[\"HB\"].fillna(hb_median)\n",
    "df_val_pivot[\"HB\"] = df_val_pivot[\"HB\"].fillna(hb_median)\n",
    "\n",
    "for df_ in [df_train_pivot, df_val_pivot]:\n",
    "    df_[\"PLT_HB_ratio\"] = safe_ratio(df_[\"PLT\"], df_[\"HB\"] + 1)\n",
    "\n",
    "# Log transforms for selected features\n",
    "for df_ in [df_train_pivot, df_val_pivot]:\n",
    "    for col in [\n",
    "        \"WBC\",\n",
    "        \"ANC\",\n",
    "        \"PLT\",\n",
    "        \"MONOCYTES\",\n",
    "        \"BM_BLAST\",\n",
    "        \"Nmut\",\n",
    "        \"DEPTH_avg\",\n",
    "    ]:\n",
    "        if col in df_.columns:\n",
    "            df_[f\"log1p_{col}\"] = np.log1p(df_[col].clip(lower=0))\n",
    "\n",
    "# Drop very rare mutation effect counts\n",
    "mutation_cols = [\n",
    "    c\n",
    "    for c in df_train_pivot.columns\n",
    "    if c.startswith(\"EFFECT_\") and c.endswith(\"_count\")\n",
    "]\n",
    "\n",
    "low_freq_cols = []\n",
    "for c in mutation_cols:\n",
    "    if (df_train_pivot[c] > 0).mean() < 0.01:\n",
    "        low_freq_cols.append(c)\n",
    "\n",
    "df_train_pivot = df_train_pivot.drop(columns=low_freq_cols)\n",
    "df_val_pivot = df_val_pivot.drop(\n",
    "    columns=[c for c in low_freq_cols if c in df_val_pivot.columns]\n",
    ")\n",
    "\n",
    "print(\"EFFECT_LOF_count in train:\", \"EFFECT_LOF_count\" in df_train_pivot.columns)\n",
    "print([c for c in df_train_pivot.columns if \"EFFECT\" in c])\n",
    "\n",
    "# Global mutation burden score\n",
    "for df_ in [df_train_pivot, df_val_pivot]:\n",
    "    df_[\"mutation_burden_score\"] = (\n",
    "        0.5 * df_[\"Nmut\"]\n",
    "        + 1.5 * df_[\"EFFECT_LOF_ratio\"]\n",
    "        + 1.0 * df_[\"vaf_entropy\"]\n",
    "    )\n",
    "\n",
    "# Log transforms for some cytogenetic count features\n",
    "for df_ in [df_train_pivot, df_val_pivot]:\n",
    "    for col in [\n",
    "        \"n_events\",\n",
    "        \"n_chromosomes_altered\",\n",
    "        \"n_monosomies_total\",\n",
    "        \"n_trisomies_total\",\n",
    "    ]:\n",
    "        if col in df_.columns:\n",
    "            df_[f\"log1p_{col}\"] = np.log1p(df_[col])\n",
    "\n",
    "# Check remaining missing values\n",
    "na_counts = df_train_pivot.isna().sum()\n",
    "print(\"Columns with missing values in train:\")\n",
    "print(na_counts[na_counts > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b374e1",
   "metadata": {},
   "source": [
    "## 14. Final scaling of continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77273d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = df_train_pivot.columns.tolist()\n",
    "\n",
    "binary_like = [\n",
    "    c\n",
    "    for c in all_cols\n",
    "    if c.startswith(\"Gene_\")\n",
    "    or c.startswith(\"RiskQuartile_\")\n",
    "    or df_train_pivot[c].dropna().isin([0, 1]).all()\n",
    "]\n",
    "\n",
    "target_cols = [\"OS_YEARS\", \"OS_STATUS\"]\n",
    "\n",
    "cont_cols = [\n",
    "    c for c in all_cols if c not in binary_like + target_cols + [\"ID\"]\n",
    "]\n",
    "\n",
    "scaler_final_features = RobustScaler()\n",
    "df_train_pivot[cont_cols] = scaler_final_features.fit_transform(\n",
    "    df_train_pivot[cont_cols]\n",
    ")\n",
    "df_val_pivot[cont_cols] = scaler_final_features.transform(\n",
    "    df_val_pivot[cont_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706e315",
   "metadata": {},
   "source": [
    "## 15. Save final enhanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d70943",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_train_pivot.to_csv(\"../../data/train_enhanced.csv\", index=False)\n",
    "df_val_pivot.to_csv(\"../../data/eval_enhanced.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
