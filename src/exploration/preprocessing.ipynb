{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Preprocessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1783,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T22:05:58.732805Z",
     "start_time": "2025-12-03T22:05:57.613337Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.util import Surv\n",
    "\n",
    "df = pd.read_csv(\"../../data/clinical_train.csv\")\n",
    "df_eval = pd.read_csv(\"../../data/clinical_val.csv\")\n",
    "maf_df = pd.read_csv(\"../../data/molecular_train.csv\")\n",
    "maf_eval = pd.read_csv(\"../../data/molecular_val.csv\")\n",
    "target_df = pd.read_csv(\"../../data/target_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1784,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf_df2 = pd.read_csv(\"../../data/molecular_train.csv\")\n",
    "maf_eval2 = pd.read_csv(\"../../data/molecular_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "CHR               114\n",
       "START             114\n",
       "END               114\n",
       "REF               114\n",
       "ALT               114\n",
       "GENE                0\n",
       "PROTEIN_CHANGE     12\n",
       "EFFECT              0\n",
       "VAF                89\n",
       "DEPTH             114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maf_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1786,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T22:06:05.796542Z",
     "start_time": "2025-12-03T22:06:05.786793Z"
    }
   },
   "outputs": [],
   "source": [
    "target = ['OS_YEARS', 'OS_STATUS']\n",
    "\n",
    "# Drop rows where 'OS_YEARS' is NaN if conversion caused any issues\n",
    "target_df.dropna(subset=target, inplace=True)\n",
    "target_df['OS_YEARS'] = pd.to_numeric(target_df['OS_YEARS'], errors='coerce')\n",
    "target_df['OS_STATUS'] = target_df['OS_STATUS'].astype(bool)\n",
    "\n",
    "# Select features\n",
    "features = ['ID', 'BM_BLAST', 'WBC', 'ANC', 'MONOCYTES', 'HB', 'PLT', 'CYTOGENETICS']\n",
    "\n",
    "# Create the survival data format\n",
    "X = df.loc[df['ID'].isin(target_df['ID']), features]\n",
    "X_eval = df_eval.loc[:, features]\n",
    "y = Surv.from_dataframe('OS_STATUS', 'OS_YEARS', target_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 12:06:11,492] A new study created in memory with name: no-name-23ed9c2f-384d-4e1e-9fa4-a790a4c4e3cd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning feature 1/6 : BM_BLAST\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2257e1363e674c76bb622695576ee2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM_BLAST:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 12:06:12,088] Trial 0 finished with value: 46.69465195713341 and parameters: {'max_depth': 7, 'learning_rate': 0.01689159647526214, 'n_estimators': 291, 'subsample': 0.6620127324208509, 'colsample_bytree': 0.5588092341457913}. Best is trial 0 with value: 46.69465195713341.\n",
      "[I 2025-12-09 12:06:12,224] Trial 1 finished with value: 37.748426145769514 and parameters: {'max_depth': 3, 'learning_rate': 0.11828402898336147, 'n_estimators': 318, 'subsample': 0.7465520005165277, 'colsample_bytree': 0.7252672803024504}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:12,362] Trial 2 finished with value: 55.75175307363229 and parameters: {'max_depth': 6, 'learning_rate': 0.2254210340812137, 'n_estimators': 164, 'subsample': 0.7122750845669046, 'colsample_bytree': 0.7606738711859645}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:12,606] Trial 3 finished with value: 52.16733656446621 and parameters: {'max_depth': 4, 'learning_rate': 0.02644829457682145, 'n_estimators': 419, 'subsample': 0.8364405481755981, 'colsample_bytree': 0.6580301786495827}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:12,661] Trial 4 finished with value: 48.80830575063316 and parameters: {'max_depth': 3, 'learning_rate': 0.2122162710193314, 'n_estimators': 128, 'subsample': 0.7123122698409725, 'colsample_bytree': 0.8505578277558734}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:12,986] Trial 5 finished with value: 94.99852617110969 and parameters: {'max_depth': 7, 'learning_rate': 0.021451321566803086, 'n_estimators': 301, 'subsample': 0.5510949198319726, 'colsample_bytree': 0.7773150804223115}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:13,139] Trial 6 finished with value: 50.8104895769571 and parameters: {'max_depth': 8, 'learning_rate': 0.29224627766502037, 'n_estimators': 100, 'subsample': 0.9455737424968782, 'colsample_bytree': 0.656811587921662}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:13,295] Trial 7 finished with value: 73.67878439271503 and parameters: {'max_depth': 5, 'learning_rate': 0.18898021804459225, 'n_estimators': 238, 'subsample': 0.5427681670788704, 'colsample_bytree': 0.5620343847353236}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:13,429] Trial 8 finished with value: 67.92248049992858 and parameters: {'max_depth': 3, 'learning_rate': 0.01342347526543841, 'n_estimators': 397, 'subsample': 0.7051377643729639, 'colsample_bytree': 0.5335519418088982}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:13,531] Trial 9 finished with value: 62.991578970521054 and parameters: {'max_depth': 3, 'learning_rate': 0.011180617763317923, 'n_estimators': 278, 'subsample': 0.6936788507155249, 'colsample_bytree': 0.7929726963619518}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:14,782] Trial 10 finished with value: 61.616643845435625 and parameters: {'max_depth': 10, 'learning_rate': 0.07900789578131033, 'n_estimators': 499, 'subsample': 0.8638163561669082, 'colsample_bytree': 0.9592095486076062}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:15,411] Trial 11 finished with value: 76.09438679058263 and parameters: {'max_depth': 9, 'learning_rate': 0.08387244798900023, 'n_estimators': 349, 'subsample': 0.6149888290935808, 'colsample_bytree': 0.6476522470978293}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:15,629] Trial 12 finished with value: 47.7557283044462 and parameters: {'max_depth': 6, 'learning_rate': 0.0445079181370289, 'n_estimators': 213, 'subsample': 0.8096244829148225, 'colsample_bytree': 0.5932611658270864}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:15,766] Trial 13 finished with value: 93.39462145668111 and parameters: {'max_depth': 2, 'learning_rate': 0.11397161862058032, 'n_estimators': 349, 'subsample': 0.6197178143124427, 'colsample_bytree': 0.8849364128088417}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:15,850] Trial 14 finished with value: 62.54425317929334 and parameters: {'max_depth': 7, 'learning_rate': 0.041925811195253945, 'n_estimators': 53, 'subsample': 0.7744928899680761, 'colsample_bytree': 0.5099288813526623}. Best is trial 1 with value: 37.748426145769514.\n",
      "[I 2025-12-09 12:06:15,990] Trial 15 finished with value: 65.19631724358786 and parameters: {'max_depth': 5, 'learning_rate': 0.11645708566771404, 'n_estimators': 205, 'subsample': 0.6111098306027168, 'colsample_bytree': 0.7065827484570126}. Best is trial 1 with value: 37.748426145769514.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from tqdm.notebook import tqdm\n",
    "import xgboost as xgb\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0. On suppose que X et X_eval existent déjà (DataFrames)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# ----------------------\n",
    "# 1. Colonnes numériques + indicateurs manquants\n",
    "# ----------------------\n",
    "num_cols = X.select_dtypes(\"number\").columns\n",
    "\n",
    "# Indicateurs de NA AVANT imputation\n",
    "for df_ in (X, X_eval):\n",
    "    for col in num_cols:\n",
    "        df_[f\"{col}_missing\"] = df_[col].isna().astype(\"int8\")\n",
    "\n",
    "# Matrice numpy (seulement les colonnes numériques originales)\n",
    "X_num = X[num_cols].to_numpy(dtype=float)\n",
    "n_rows, n_features = X_num.shape\n",
    "\n",
    "# ----------------------\n",
    "# 2. Tuning Optuna PAR FEATURE (1000 trials + barre de progression par feature)\n",
    "# ----------------------\n",
    "base_xgb_params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "n_trials = 1000       # <---- 1000 TRIALS\n",
    "mask_frac = 0.1\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "best_params_per_feature = {}\n",
    "\n",
    "\n",
    "def make_objective_for_feature(j):\n",
    "    \"\"\"Objective Optuna pour la feature j uniquement.\"\"\"\n",
    "    def objective(trial):\n",
    "        # Hyperparamètres XGBoost à optimiser\n",
    "        xgb_params_trial = {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        }\n",
    "\n",
    "        y = X_num[:, j]\n",
    "        X_features = np.delete(X_num, j, axis=1)\n",
    "\n",
    "        # on ne garde que les valeurs non-NA de la colonne\n",
    "        not_nan_y = ~np.isnan(y)\n",
    "        if not_nan_y.sum() < 20:\n",
    "            # pas assez de données : on renvoie un score pourri\n",
    "            return 1e6\n",
    "\n",
    "        y_obs = y[not_nan_y]\n",
    "        X_obs = X_features[not_nan_y]\n",
    "\n",
    "        # masque aléatoire pour l'évaluation\n",
    "        mask_eval = rng.rand(len(y_obs)) < mask_frac\n",
    "        if mask_eval.sum() == 0 or (~mask_eval).sum() < 10:\n",
    "            return 1e6\n",
    "\n",
    "        X_train = X_obs[~mask_eval]\n",
    "        y_train = y_obs[~mask_eval]\n",
    "        X_eval_local = X_obs[mask_eval]\n",
    "        y_eval_local = y_obs[mask_eval]\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            **base_xgb_params,\n",
    "            **xgb_params_trial\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_eval_local)\n",
    "        mse = np.mean((y_pred - y_eval_local) ** 2)\n",
    "        return mse\n",
    "\n",
    "    return objective\n",
    "\n",
    "\n",
    "# Boucle Optuna sur chaque feature\n",
    "for j, col in enumerate(num_cols):\n",
    "    # Vérification rapide pour éviter de lancer Optuna si trop peu de données\n",
    "    y_col = X_num[:, j]\n",
    "    if (~np.isnan(y_col)).sum() < 20:\n",
    "        continue\n",
    "\n",
    "    print(f\"Tuning feature {j+1}/{len(num_cols)} : {col}\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    # Barre de progression POUR CETTE FEATURE\n",
    "    with tqdm(total=n_trials, desc=f\"{col}\") as pbar:\n",
    "        def callback(study_, trial_):\n",
    "            # Affiche la meilleure MSE courante dans la description\n",
    "            pbar.set_description(f\"{col} | best MSE={study_.best_value:.5f}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "        study.optimize(\n",
    "            make_objective_for_feature(j),\n",
    "            n_trials=n_trials,\n",
    "            callbacks=[callback],\n",
    "        )\n",
    "\n",
    "    # On stocke les meilleurs hyperparamètres pour cette feature\n",
    "    best_params_per_feature[col] = {**base_xgb_params, **study.best_params}\n",
    "\n",
    "print(\"Nombre de features tunées :\", len(best_params_per_feature))\n",
    "\n",
    "# ----------------------\n",
    "# 3. Fit des imputers finaux (un modèle par feature)\n",
    "# ----------------------\n",
    "def fit_xgb_imputers_per_feature(X_df, num_cols, best_params_per_feature):\n",
    "    models = {}\n",
    "    X_values = X_df[num_cols].to_numpy(dtype=float)\n",
    "\n",
    "    for j, col in enumerate(num_cols):\n",
    "        y = X_values[:, j]\n",
    "        missing_mask = np.isnan(y)\n",
    "        not_missing = ~missing_mask\n",
    "\n",
    "        if not_missing.sum() < 20:\n",
    "            # Pas assez de données pour entraîner un modèle\n",
    "            continue\n",
    "\n",
    "        # Si on n'a pas de params Optuna pour cette colonne, on skip\n",
    "        if col not in best_params_per_feature:\n",
    "            continue\n",
    "\n",
    "        X_features = np.delete(X_values, j, axis=1)\n",
    "\n",
    "        X_train = X_features[not_missing]\n",
    "        y_train = y[not_missing]\n",
    "\n",
    "        params = best_params_per_feature[col]\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        models[col] = model\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def transform_with_xgb_imputers(X_df, num_cols, models):\n",
    "    X_values = X_df[num_cols].to_numpy(dtype=float)\n",
    "\n",
    "    for j, col in enumerate(num_cols):\n",
    "        missing_mask = np.isnan(X_values[:, j])\n",
    "\n",
    "        if missing_mask.any():\n",
    "            if col in models:\n",
    "                X_features = np.delete(X_values, j, axis=1)\n",
    "                X_missing = X_features[missing_mask]\n",
    "                preds = models[col].predict(X_missing)\n",
    "                X_values[missing_mask, j] = preds\n",
    "            else:\n",
    "                # fallback : médiane de la colonne\n",
    "                X_values[missing_mask, j] = np.nanmedian(X_values[:, j])\n",
    "\n",
    "    return pd.DataFrame(X_values, columns=num_cols, index=X_df.index)\n",
    "\n",
    "# ----------------------\n",
    "# 4. Imputation finale\n",
    "# ----------------------\n",
    "xgb_models = fit_xgb_imputers_per_feature(X, num_cols, best_params_per_feature)\n",
    "\n",
    "X[num_cols] = transform_with_xgb_imputers(X, num_cols, xgb_models)\n",
    "X_eval[num_cols] = transform_with_xgb_imputers(X_eval, num_cols, xgb_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "# num_cols = X.select_dtypes(\"number\").columns # Sauvegarde des indicateurs de manquant AVANT imputation \n",
    "# for df_ in [X, X_eval]: \n",
    "#     for col in num_cols: \n",
    "#         df_[f\"{col}_missing\"] = df_[col].isna().astype(int) \n",
    "\n",
    "# imputer = SimpleImputer(strategy=\"median\") \n",
    "# X[num_cols] = imputer.fit_transform(X[num_cols]) \n",
    "# X_eval[num_cols] = imputer.transform(X_eval[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Mutation Features\n",
    "\n",
    "Computing mutation count (Nmut) along with VAF statistics (average, std, max) and length statistics (average, std, max) for both training and evaluation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the user request and the selected cell, I need to apply RobustScaler to the new mutation features (Nmut, VAF_avg, VAF_std, VAF_max, LEN_avg, LEN_std, LEN_max) that are computed in the `compute_mutation_features` function.\n",
    "\n",
    "The task is to:\n",
    "1. Apply RobustScaler to the newly computed mutation features after merging them with the dataframe\n",
    "2. Fit the scaler on training data and transform both training and evaluation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T21:19:57.783812Z",
     "start_time": "2025-11-12T21:19:57.762578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CHR</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>GENE</th>\n",
       "      <th>PROTEIN_CHANGE</th>\n",
       "      <th>VAF</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>EFFECT_ITD</th>\n",
       "      <th>EFFECT_PTD</th>\n",
       "      <th>EFFECT_frameshift_variant</th>\n",
       "      <th>EFFECT_inframe_codon_gain</th>\n",
       "      <th>EFFECT_inframe_codon_loss</th>\n",
       "      <th>EFFECT_non_synonymous_codon</th>\n",
       "      <th>EFFECT_stop_gained</th>\n",
       "      <th>EFFECT_stop_lost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KYW961</td>\n",
       "      <td>1</td>\n",
       "      <td>1747229.0</td>\n",
       "      <td>1747229.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>GNB1</td>\n",
       "      <td>p.K57E</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>485.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KYW142</td>\n",
       "      <td>1</td>\n",
       "      <td>1747229.0</td>\n",
       "      <td>1747229.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>GNB1</td>\n",
       "      <td>p.K57E</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>527.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KYW453</td>\n",
       "      <td>1</td>\n",
       "      <td>1747229.0</td>\n",
       "      <td>1747229.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>GNB1</td>\n",
       "      <td>p.K57E</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>277.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KYW982</td>\n",
       "      <td>1</td>\n",
       "      <td>1747229.0</td>\n",
       "      <td>1747229.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>GNB1</td>\n",
       "      <td>p.K57E</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>821.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KYW845</td>\n",
       "      <td>1</td>\n",
       "      <td>36932209.0</td>\n",
       "      <td>36932209.0</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>CSF3R</td>\n",
       "      <td>p.Q754X</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>358.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>KYW1077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MLL</td>\n",
       "      <td>MLL_PTD</td>\n",
       "      <td>0.4231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>KYW1084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MLL</td>\n",
       "      <td>MLL_PTD</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>KYW1082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MLL</td>\n",
       "      <td>MLL_PTD</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>KYW1085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MLL</td>\n",
       "      <td>MLL_PTD</td>\n",
       "      <td>0.2941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3088</th>\n",
       "      <td>KYW1095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MLL</td>\n",
       "      <td>MLL_PTD</td>\n",
       "      <td>0.3272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3089 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  CHR       START         END  REF  ALT   GENE PROTEIN_CHANGE  \\\n",
       "0      KYW961    1   1747229.0   1747229.0    T    C   GNB1         p.K57E   \n",
       "1      KYW142    1   1747229.0   1747229.0    T    C   GNB1         p.K57E   \n",
       "2      KYW453    1   1747229.0   1747229.0    T    C   GNB1         p.K57E   \n",
       "3      KYW982    1   1747229.0   1747229.0    T    C   GNB1         p.K57E   \n",
       "4      KYW845    1  36932209.0  36932209.0    G    A  CSF3R        p.Q754X   \n",
       "...       ...  ...         ...         ...  ...  ...    ...            ...   \n",
       "3084  KYW1077  NaN         NaN         NaN  NaN  NaN    MLL        MLL_PTD   \n",
       "3085  KYW1084  NaN         NaN         NaN  NaN  NaN    MLL        MLL_PTD   \n",
       "3086  KYW1082  NaN         NaN         NaN  NaN  NaN    MLL        MLL_PTD   \n",
       "3087  KYW1085  NaN         NaN         NaN  NaN  NaN    MLL        MLL_PTD   \n",
       "3088  KYW1095  NaN         NaN         NaN  NaN  NaN    MLL        MLL_PTD   \n",
       "\n",
       "         VAF  DEPTH  EFFECT_ITD  EFFECT_PTD  EFFECT_frameshift_variant  \\\n",
       "0     0.2620  485.0       False       False                      False   \n",
       "1     0.0280  527.0       False       False                      False   \n",
       "2     0.2920  277.0       False       False                      False   \n",
       "3     0.0970  821.0       False       False                      False   \n",
       "4     0.4300  358.0       False       False                      False   \n",
       "...      ...    ...         ...         ...                        ...   \n",
       "3084  0.4231    NaN       False        True                      False   \n",
       "3085  0.0176    NaN       False        True                      False   \n",
       "3086  0.2273    NaN       False        True                      False   \n",
       "3087  0.2941    NaN       False        True                      False   \n",
       "3088  0.3272    NaN       False        True                      False   \n",
       "\n",
       "      EFFECT_inframe_codon_gain  EFFECT_inframe_codon_loss  \\\n",
       "0                         False                      False   \n",
       "1                         False                      False   \n",
       "2                         False                      False   \n",
       "3                         False                      False   \n",
       "4                         False                      False   \n",
       "...                         ...                        ...   \n",
       "3084                      False                      False   \n",
       "3085                      False                      False   \n",
       "3086                      False                      False   \n",
       "3087                      False                      False   \n",
       "3088                      False                      False   \n",
       "\n",
       "      EFFECT_non_synonymous_codon  EFFECT_stop_gained  EFFECT_stop_lost  \n",
       "0                            True               False             False  \n",
       "1                            True               False             False  \n",
       "2                            True               False             False  \n",
       "3                            True               False             False  \n",
       "4                           False                True             False  \n",
       "...                           ...                 ...               ...  \n",
       "3084                        False               False             False  \n",
       "3085                        False               False             False  \n",
       "3086                        False               False             False  \n",
       "3087                        False               False             False  \n",
       "3088                        False               False             False  \n",
       "\n",
       "[3089 rows x 18 columns]"
      ]
     },
     "execution_count": 1761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(maf_df, columns=[\"EFFECT\"])\n",
    "pd.get_dummies(maf_eval, columns=[\"EFFECT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the user request, I need to add CHR count columns for the chromosomes specified in the list: ['4', '2', '17', 'X', '20', '21', '12', '7', '5', '1', '11', '15', '19', '18', '9', '3', '16'].\n",
    "\n",
    "Currently, the code only has `CHR_X_count`. I need to add similar count columns for all the other chromosomes in the list.\n",
    "\n",
    "Here's the modified cell code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T21:40:45.508055Z",
     "start_time": "2025-11-12T21:40:40.864269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting RobustScaler for mutation features (28 features) on training data, transforming training and evaluation data.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "def compute_mutation_features(maf_df, X_df, top_k_chr=10):\n",
    "    maf_df = maf_df.copy()\n",
    "\n",
    "    # --- 0) Vérif de la colonne CHR ---\n",
    "    if 'CHR' not in maf_df.columns:\n",
    "        raise ValueError(\n",
    "            \"La colonne 'CHR' est absente de maf_df. \"\n",
    "            \"Ne fais pas pd.get_dummies(maf_df, columns=['CHR']) \"\n",
    "            \"avant d'appeler compute_mutation_features, ou passe la version brute.\"\n",
    "        )\n",
    "\n",
    "    # --- 1) Longueur de la mutation & 'deletion length' ---\n",
    "    maf_df['LEN'] = maf_df['END'] - maf_df['START'] + 1\n",
    "    maf_df['DELLEN'] = maf_df['LEN'] - maf_df['REF'].apply(lambda x: len(str(x)))\n",
    "\n",
    "    # --- 1bis) CHR : ne garder que les top_k_chr pour les features de comptage ---\n",
    "    # (mais CHR_nunique sera quand même calculé sur tous les CHR)\n",
    "    top_chr = maf_df['CHR'].value_counts().nlargest(top_k_chr).index\n",
    "    unique_chr = sorted(top_chr)\n",
    "\n",
    "    # Colonnes one-hot d'EFFECT déjà présentes\n",
    "    effect_dummy_cols = [c for c in maf_df.columns if c.startswith('EFFECT_')]\n",
    "\n",
    "    # --- 2) Dictionnaire d'agrégation de base ---\n",
    "    agg_dict = {\n",
    "        'Nmut': ('ID', 'size'),\n",
    "        'VAF_avg': ('VAF', 'mean'),\n",
    "        'VAF_std': ('VAF', 'std'),\n",
    "        'VAF_max': ('VAF', 'max'),\n",
    "        'LEN_avg': ('LEN', 'mean'),\n",
    "        'LEN_max': ('LEN', 'max'),\n",
    "        'DELLEN_sum': ('DELLEN', 'sum'),\n",
    "        'DEPTH_avg': ('DEPTH', 'mean'),\n",
    "        'DEPTH_std': ('DEPTH', 'std'),\n",
    "        'DEPTH_max': ('DEPTH', 'max'),\n",
    "        'DEPTH_min': ('DEPTH', 'min'),\n",
    "        'CHR_nunique': ('CHR', 'nunique'),\n",
    "        'EFFECT_nunique': ('EFFECT', 'nunique'),\n",
    "        'EFFECT_FV_count': ('EFFECT', lambda x: (x == 'frameshift_variant').sum()),\n",
    "        'EFFECT_SG_count': ('EFFECT', lambda x: (x == 'stop_gained').sum()),\n",
    "        'EFFECT_NS_count': ('EFFECT', lambda x: (x == 'non_synonymous_codon').sum()),\n",
    "    }\n",
    "\n",
    "    # --- 3) Colonnes CHR_*_count seulement pour les top_k_chr ---\n",
    "    for ch in unique_chr:\n",
    "        col_name = f'CHR_{ch}_count'\n",
    "        agg_dict[col_name] = ('CHR', lambda x, val=ch: (x == val).sum())\n",
    "\n",
    "    # --- 4) Agrégation des dummies EFFECT_* au niveau patient ---\n",
    "    # -> EFFECT_*_count = nombre de mutations de ce type par patient\n",
    "    for col in effect_dummy_cols:\n",
    "        new_name = f'{col}_count'\n",
    "        agg_dict[new_name] = (col, 'sum')\n",
    "\n",
    "    # --- 5) Agrégation globale par ID ---\n",
    "    tmp = maf_df.groupby('ID').agg(**agg_dict).reset_index()\n",
    "\n",
    "    # Remplir NaN des std (quand une seule mutation par patient)\n",
    "    for std_col in ['VAF_std', 'DEPTH_std']:\n",
    "        if std_col in tmp.columns:\n",
    "            tmp[std_col] = tmp[std_col].fillna(0)\n",
    "\n",
    "    # --- 6) Loss-of-function : EFFECT_LOF_count & EFFECT_LOF_ratio ---\n",
    "    # On repère les colonnes *_count associées à frameshift / stop_gained\n",
    "    lof_effect_cols = [\n",
    "        c for c in tmp.columns\n",
    "        if c.startswith('EFFECT_')\n",
    "        and c.endswith('_count')\n",
    "        and (\n",
    "            'frameshift_variant' in c\n",
    "            or 'stop_gained' in c\n",
    "            # tu peux ajouter d'autres patterns ici si besoin : 'splice', etc.\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    if len(lof_effect_cols) > 0:\n",
    "        tmp['EFFECT_LOF_count'] = tmp[lof_effect_cols].sum(axis=1)\n",
    "    else:\n",
    "        tmp['EFFECT_LOF_count'] = 0\n",
    "\n",
    "    tmp['EFFECT_LOF_ratio'] = np.where(\n",
    "        tmp['Nmut'] > 0,\n",
    "        tmp['EFFECT_LOF_count'] / tmp['Nmut'],\n",
    "        0.0\n",
    "    )\n",
    "\n",
    "    # --- 7) Merge avec X_df ---\n",
    "    X_w_mutation = X_df.merge(tmp, on='ID', how='left')\n",
    "\n",
    "    # Colonnes ajoutées par l'agg (à remplir avec 0 pour les patients sans mutation)\n",
    "    new_cols = [c for c in tmp.columns if c != 'ID']\n",
    "    X_w_mutation[new_cols] = X_w_mutation[new_cols].fillna(0)\n",
    "\n",
    "    return X_w_mutation\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ================ UTILISATION TRAIN / EVAL ==================\n",
    "# ============================================================\n",
    "\n",
    "# On suppose que maf_df, maf_eval, X, X_eval existent déjà.\n",
    "\n",
    "# 1) Ajout des features mutationnelles au train et à l'eval\n",
    "X_w_mutation = compute_mutation_features(maf_df, X)\n",
    "X_eval_w_mutation = compute_mutation_features(maf_eval, X_eval)\n",
    "\n",
    "# 2) Construction de la liste des features de mutation (à partir du train)\n",
    "base_mutation_features = [\n",
    "    'Nmut', 'VAF_avg', 'VAF_std', 'VAF_max',\n",
    "    'LEN_avg', 'LEN_max', 'DELLEN_sum',\n",
    "    'DEPTH_avg', 'DEPTH_std', 'DEPTH_max', 'DEPTH_min',\n",
    "    'CHR_nunique',\n",
    "    'EFFECT_nunique', 'EFFECT_FV_count', 'EFFECT_SG_count', 'EFFECT_NS_count',\n",
    "    'EFFECT_LOF_count', 'EFFECT_LOF_ratio',  # nouveau\n",
    "]\n",
    "\n",
    "# Colonnes CHR_*_count dans le TRAIN\n",
    "chr_count_cols = [\n",
    "    c for c in X_w_mutation.columns\n",
    "    if c.startswith('CHR_') and c.endswith('_count')\n",
    "]\n",
    "\n",
    "# Colonnes EFFECT_*_count générées automatiquement depuis les dummies\n",
    "effect_count_cols = [\n",
    "    c for c in X_w_mutation.columns\n",
    "    if c.startswith('EFFECT_') and c.endswith('_count')\n",
    "    and c not in ['EFFECT_FV_count', 'EFFECT_SG_count', 'EFFECT_NS_count', 'EFFECT_LOF_count']\n",
    "]\n",
    "\n",
    "mutation_features = base_mutation_features + chr_count_cols + effect_count_cols\n",
    "\n",
    "# 3) Harmoniser les colonnes entre train et eval\n",
    "for col in mutation_features:\n",
    "    if col not in X_w_mutation.columns:\n",
    "        X_w_mutation[col] = 0\n",
    "    if col not in X_eval_w_mutation.columns:\n",
    "        X_eval_w_mutation[col] = 0\n",
    "\n",
    "# (Optionnel) s'assurer que les colonnes sont bien dans le même ordre\n",
    "X_w_mutation = X_w_mutation.copy()\n",
    "X_eval_w_mutation = X_eval_w_mutation.copy()\n",
    "X_w_mutation[mutation_features] = X_w_mutation[mutation_features]\n",
    "X_eval_w_mutation[mutation_features] = X_eval_w_mutation[mutation_features]\n",
    "\n",
    "# 4) RobustScaler sur ces features\n",
    "mutation_scaler = RobustScaler()\n",
    "print(\n",
    "    f\"Fitting RobustScaler for mutation features ({len(mutation_features)} features) \"\n",
    "    \"on training data, transforming training and evaluation data.\"\n",
    ")\n",
    "X_w_mutation[mutation_features] = mutation_scaler.fit_transform(X_w_mutation[mutation_features])\n",
    "X_eval_w_mutation[mutation_features] = mutation_scaler.transform(X_eval_w_mutation[mutation_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Cytogenetics\n",
    "\n",
    "sorte de \"rules engine\" basé sur la littérature clinique AML pour extraire des features pronostiques clés depuis l’ISCN. On détecte\n",
    "\n",
    "* **Monosomal karyotype (MK)**: au moins 2 monosomies autosomiques, ou 1 monosomie autosomique + au moins une anomalie structurale. C’est franchement mauvais pronostic. ([PubMed Central][1])\n",
    "* **Complex karyotype**: typiquement ≥ 3 anomalies cytogénétiques indépendantes, pronostic défavorable. ([Cancer Network][2])\n",
    "* **Chromosomes 5 et 7**: perte de 5 ou 7, ou del(5q)/del(7q), classés défavorables dans ELN 2022. ([ASH Publications][3])\n",
    "* **+8 (trisomie 8)**: très fréquente, plutôt risque intermédiaire en AML. ([MDPI][4])\n",
    "* **Translocations favorables classiques**: t(8;21), inv(16)/t(16;16); APL t(15;17) est à part. On les isole comme features car elles portent un signal fort. ([ASH Publications][3])\n",
    "* **Proportions clonales** via les crochets `[n]` pour calculer la part de métaphases portant une anomalie donnée. La sémantique des notations vient de l’ISCN. ([PubMed][5])\n",
    "\n",
    "Pour chaque karyotype, on construit:\n",
    "\n",
    "* `has_any_abnormality`, `n_events`, `n_chromosomes_altered`\n",
    "* `has_minus5_or_del5q`, `has_minus7_or_del7q`, `has_plus8`\n",
    "* `has_t_8_21`, `has_inv16_or_t_16_16`, `has_t_15_17`\n",
    "* `is_complex_karyotype`, `is_monosomal_karyotype`\n",
    "* `total_metaphases` et des **proportions clonales**: `prop_any_abnormal`, `prop_adverse_5_7`, `prop_plus8`, etc.\n",
    "* Option: `eln_like_flag_adverse_cyto` basé ici sur 5/7 ou MK ou complex.\n",
    "\n",
    "[1]: https://pmc.ncbi.nlm.nih.gov/articles/PMC3069222/?utm_source=chatgpt.com \"Acute myeloid leukemia with monosomal karyotype at the ...\"\n",
    "[2]: https://www.cancernetwork.com/view/unfavorable-complex-and-monosomal-karyotypes-most-challenging-forms-acute-myeloid-leukemia?utm_source=chatgpt.com \"Unfavorable, Complex, and Monosomal Karyotypes\"\n",
    "[3]: https://ashpublications.org/blood/article/140/12/1345/485817/Diagnosis-and-management-of-AML-in-adults-2022?utm_source=chatgpt.com \"Diagnosis and management of AML in adults - ASH Publications\"\n",
    "[4]: https://www.mdpi.com/2072-6694/13/22/5679?utm_source=chatgpt.com \"Risk Stratification, Measurable Residual Disease, and ...\"\n",
    "[5]: https://pubmed.ncbi.nlm.nih.gov/34839499/?utm_source=chatgpt.com \"[Introduction and interpretation of the updated contents of ...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T21:40:56.262156Z",
     "start_time": "2025-11-12T21:40:56.249241Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "# --- REGEX patterns ---\n",
    "_ISCN_EVENT_RE = re.compile(r'(del|dup|inv|ins|i|t|add|der)\\s*\\(', re.IGNORECASE)\n",
    "_MONOSOMY_RE   = re.compile(r'(?<![pq])-(\\d{1,2}|X|Y)(?![pq])', re.IGNORECASE)\n",
    "_TRISOMY_RE    = re.compile(r'(?<![pq])\\+(\\d{1,2}|X|Y)(?![pq])', re.IGNORECASE)\n",
    "_CHR_NUM_RE    = re.compile(r'(?<![pq])(\\d{1,2}|X|Y)(?![pq])', re.IGNORECASE)\n",
    "\n",
    "# specific adverse/favorable (de base)\n",
    "_MINUS5_OR_DEL5Q_RE = re.compile(r'-(?:5)(?![pq])|del\\s*\\(\\s*5\\s*\\)\\s*\\(\\s*q', re.IGNORECASE)\n",
    "_MINUS7_OR_DEL7Q_RE = re.compile(r'-(?:7)(?![pq])|del\\s*\\(\\s*7\\s*\\)\\s*\\(\\s*q', re.IGNORECASE)\n",
    "_PLUS8_RE           = re.compile(r'\\+8(?![pq])', re.IGNORECASE)\n",
    "_T_8_21_RE          = re.compile(r't\\s*\\(\\s*8\\s*;\\s*21\\s*\\)', re.IGNORECASE)\n",
    "_INV16_OR_T_16_16_RE= re.compile(r'(inv\\s*\\(\\s*16\\s*\\)|t\\s*\\(\\s*16\\s*;\\s*16\\s*\\))', re.IGNORECASE)\n",
    "_T_15_17_RE         = re.compile(r't\\s*\\(\\s*15\\s*;\\s*17\\s*\\)', re.IGNORECASE)\n",
    "_STRUCTURAL_RE      = re.compile(r'(del|dup|inv|ins|i|t|add|der)\\s*\\(', re.IGNORECASE)\n",
    "\n",
    "# nouveaux patterns ELN-like\n",
    "_INV3_OR_T3_3_RE    = re.compile(r'(inv\\s*\\(\\s*3\\s*\\)\\s*\\(q21q26\\)|t\\s*\\(\\s*3\\s*;\\s*3\\s*\\)\\s*\\(q21;q26\\))', re.IGNORECASE)\n",
    "_T_6_9_RE           = re.compile(r't\\s*\\(\\s*6\\s*;\\s*9\\s*\\)\\s*\\(p23;q34\\)', re.IGNORECASE)\n",
    "_T_9_22_RE          = re.compile(r't\\s*\\(\\s*9\\s*;\\s*22\\s*\\)\\s*\\(q34;q11\\)', re.IGNORECASE)\n",
    "_ABN_17P_RE         = re.compile(r'(del\\s*\\(\\s*17\\s*\\)\\s*\\(\\s*p|del\\s*\\(\\s*17p\\s*\\)|-17(?![pq])|add\\s*\\(\\s*17\\s*\\)\\s*\\(\\s*p)', re.IGNORECASE)\n",
    "\n",
    "# baseline chr count (ex: 46,XX,del(5q)... -> 46)\n",
    "_BASELINE_CHR_RE    = re.compile(r'^\\s*(\\d{2})\\s*,', re.IGNORECASE)\n",
    "\n",
    "# normal karyotype (simple)\n",
    "_NORMAL_KARYO_RE    = re.compile(r'^\\s*46\\s*,\\s*(XX|XY)\\s*(\\[\\d+\\])?\\s*$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "# --- helpers ---\n",
    "def _split_clones(karyo: str) -> List[str]:\n",
    "    \"\"\"Split ISCN string into clones separated by '/'.\"\"\"\n",
    "    return [c.strip() for c in str(karyo).split('/') if c.strip()]\n",
    "\n",
    "\n",
    "def _extract_metaphases(clone: str) -> int:\n",
    "    \"\"\"Extract number of metaphases from [n] in clone.\"\"\"\n",
    "    m = re.search(r'\\[(\\d+)\\]', clone)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "\n",
    "def _count_events(clone: str) -> int:\n",
    "    \"\"\"\n",
    "    Count events in a clone:\n",
    "    structural + trisomies + autosomal monosomies (ignore -Y).\n",
    "    \"\"\"\n",
    "    n_struct = len(_ISCN_EVENT_RE.findall(clone))\n",
    "    n_mono   = len(_MONOSOMY_RE.findall(clone))\n",
    "    n_tri    = len(_TRISOMY_RE.findall(clone))\n",
    "    n_mono_minusY = len(re.findall(r'(?<![pq])-(?:Y)(?![pq])', clone, flags=re.IGNORECASE))\n",
    "    return n_struct + n_tri + max(n_mono - n_mono_minusY, 0)\n",
    "\n",
    "\n",
    "def _chromosomes_altered(clone: str) -> int:\n",
    "    \"\"\"Number of distinct autosomes/sex chromosomes altered (ignore Y).\"\"\"\n",
    "    nums = set()\n",
    "    for m in _MONOSOMY_RE.finditer(clone):\n",
    "        nums.add(m.group(1).upper())\n",
    "    for m in _TRISOMY_RE.finditer(clone):\n",
    "        nums.add(m.group(1).upper())\n",
    "    for ev in re.finditer(r'(del|dup|inv|ins|i|t|add|der)\\s*\\(([^)]+)\\)', clone, flags=re.IGNORECASE):\n",
    "        for x in re.split(r'[;,\\s]+', ev.group(2)):\n",
    "            if _CHR_NUM_RE.fullmatch(x.strip()):\n",
    "                nums.add(x.strip().upper())\n",
    "    nums.discard('Y')\n",
    "    return len(nums)\n",
    "\n",
    "\n",
    "def _has_structural(clone: str) -> bool:\n",
    "    return bool(_STRUCTURAL_RE.search(clone))\n",
    "\n",
    "\n",
    "def _autosomic_monosomies(clone: str) -> List[int]:\n",
    "    \"\"\"List of autosomal monosomies in clone (ignore X/Y).\"\"\"\n",
    "    return [int(m.group(1)) for m in _MONOSOMY_RE.finditer(clone) if m.group(1).upper() not in ('X', 'Y')]\n",
    "\n",
    "\n",
    "def _is_monosomal_karyotype(karyo: str) -> bool:\n",
    "    clones = _split_clones(karyo)\n",
    "    autosomal_monosomies = set()\n",
    "    any_struct = False\n",
    "    for c in clones:\n",
    "        autosomal_monosomies.update(_autosomic_monosomies(c))\n",
    "        any_struct = any_struct or _has_structural(c)\n",
    "    return (len(autosomal_monosomies) >= 2) or (len(autosomal_monosomies) >= 1 and any_struct)\n",
    "\n",
    "\n",
    "def _is_complex_karyotype(karyo: str) -> bool:\n",
    "    clones = _split_clones(karyo)\n",
    "    total_events = 0\n",
    "    for c in clones:\n",
    "        c_wo_minusY = re.sub(r'(?<![pq])-(?:Y)(?![pq])', '', c, flags=re.IGNORECASE)\n",
    "        total_events += _count_events(c_wo_minusY)\n",
    "    return total_events >= 3\n",
    "\n",
    "\n",
    "def _extract_baseline_chr_count(karyo: str) -> int:\n",
    "    \"\"\"\n",
    "    Extract baseline chromosome number at start of ISCN (ex: 46,XX,... -> 46).\n",
    "    Return -1 if not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(karyo, str):\n",
    "        return -1\n",
    "    m = _BASELINE_CHR_RE.match(karyo)\n",
    "    if not m:\n",
    "        return -1\n",
    "    try:\n",
    "        return int(m.group(1))\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def _clone_flags(clone: str) -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Flags at clone-level (adverse/favorable, counts, etc.)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        # anomalies \"classiques\"\n",
    "        'minus5_or_del5q': bool(_MINUS5_OR_DEL5Q_RE.search(clone)),\n",
    "        'minus7_or_del7q': bool(_MINUS7_OR_DEL7Q_RE.search(clone)),\n",
    "        'plus8':           bool(_PLUS8_RE.search(clone)),\n",
    "        't_8_21':          bool(_T_8_21_RE.search(clone)),\n",
    "        'inv16_or_t_16_16':bool(_INV16_OR_T_16_16_RE.search(clone)),\n",
    "        't_15_17':         bool(_T_15_17_RE.search(clone)),\n",
    "\n",
    "        # anomalies ELN \"rares\" supplémentaires\n",
    "        'inv3_or_t3_3':    bool(_INV3_OR_T3_3_RE.search(clone)),\n",
    "        't_6_9':           bool(_T_6_9_RE.search(clone)),\n",
    "        't_9_22':          bool(_T_9_22_RE.search(clone)),\n",
    "        'abn17p':          bool(_ABN_17P_RE.search(clone)),\n",
    "\n",
    "        # structure & events\n",
    "        'has_structural':        _has_structural(clone),\n",
    "        'events_count':          _count_events(clone),\n",
    "        'chrs_altered':          _chromosomes_altered(clone),\n",
    "        'has_any_abn':           bool(_ISCN_EVENT_RE.search(clone) or\n",
    "                                      _MONOSOMY_RE.search(clone) or\n",
    "                                      _TRISOMY_RE.search(clone)),\n",
    "        # counts plus détaillés\n",
    "        'n_monosomies':          len(_MONOSOMY_RE.findall(clone)),\n",
    "        'n_trisomies':           len(_TRISOMY_RE.findall(clone)),\n",
    "        'n_structural_events':   len(_ISCN_EVENT_RE.findall(clone)),\n",
    "    }\n",
    "\n",
    "\n",
    "# --- main featurizer ---\n",
    "def add_cytogenetics_features(df: pd.DataFrame, col: str = \"CYTOGENETICS\") -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    for k in df[col]:\n",
    "        # --- cas \"missing / not done\" ---\n",
    "        if not isinstance(k, str) or not k.strip() or k.strip().lower() in {\"nan\", \"na\", \"nd\", \"notdone\", \"failed\", \"failure\"}:\n",
    "            rows.append({\n",
    "                'is_cyto_missing_or_failed': 1,\n",
    "                'is_normal_karyotype': 0,\n",
    "                'is_abnormal_karyotype': 0,\n",
    "                'has_any_abnormality': 0,\n",
    "                'n_events': 0,\n",
    "                'n_chromosomes_altered': 0,\n",
    "                'n_monosomies_total': 0,\n",
    "                'n_trisomies_total': 0,\n",
    "                'n_structural_events_total': 0,\n",
    "                'has_minus5_or_del5q': 0,\n",
    "                'has_minus7_or_del7q': 0,\n",
    "                'has_plus8': 0,\n",
    "                'has_t_8_21': 0,\n",
    "                'has_inv16_or_t_16_16': 0,\n",
    "                'has_t_15_17': 0,\n",
    "                'has_inv3_or_t3_3': 0,\n",
    "                'has_t_6_9': 0,\n",
    "                'has_t_9_22': 0,\n",
    "                'has_abn17p': 0,\n",
    "                'is_monosomal_karyotype': 0,\n",
    "                'is_complex_karyotype': 0,\n",
    "                'eln_like_flag_adverse_cyto': 0,\n",
    "                'eln_like_flag_favorable_cyto': 0,\n",
    "                'eln_like_flag_intermediate_cyto': 0,\n",
    "                'eln_like_risk_cyto': -1,   # -1 = missing\n",
    "                'baseline_chr_count': -1,\n",
    "                'is_hypodiploid': 0,\n",
    "                'is_hyperdiploid': 0,\n",
    "                'is_near_tetraploid': 0,\n",
    "                'total_metaphases': 0,\n",
    "                'max_clone_size': 0.0,\n",
    "                'max_adverse_clone_size': 0.0,\n",
    "                'has_small_adverse_subclone': 0,\n",
    "                'prop_any_abnormal': 0.0,\n",
    "                'prop_adverse_5_7': 0.0,\n",
    "                'prop_plus8': 0.0,\n",
    "                'prop_favorable_core': 0.0,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        clones = _split_clones(k)\n",
    "        clone_info = []\n",
    "        total_meta_known = 0\n",
    "\n",
    "        for c in clones:\n",
    "            n_meta = _extract_metaphases(c)\n",
    "            flags = _clone_flags(c)\n",
    "            clone_info.append((c, n_meta, flags))\n",
    "            total_meta_known += n_meta\n",
    "\n",
    "        # base stats\n",
    "        any_abn   = any(f['has_any_abn'] for _, _, f in clone_info)\n",
    "        n_events  = sum(f['events_count'] for _, _, f in clone_info)\n",
    "        n_chrs    = max([f['chrs_altered'] for _, _, f in clone_info] + [0])\n",
    "\n",
    "        n_mono_tot   = sum(f['n_monosomies'] for _, _, f in clone_info)\n",
    "        n_tris_tot   = sum(f['n_trisomies'] for _, _, f in clone_info)\n",
    "        n_struct_tot = sum(f['n_structural_events'] for _, _, f in clone_info)\n",
    "\n",
    "        # anomalies spécifiques\n",
    "        has_minus5_or_del5q = any(f['minus5_or_del5q'] for _, _, f in clone_info)\n",
    "        has_minus7_or_del7q = any(f['minus7_or_del7q'] for _, _, f in clone_info)\n",
    "        has_plus8           = any(f['plus8'] for _, _, f in clone_info)\n",
    "        has_t_8_21          = any(f['t_8_21'] for _, _, f in clone_info)\n",
    "        has_inv16_or_t_16_16= any(f['inv16_or_t_16_16'] for _, _, f in clone_info)\n",
    "        has_t_15_17         = any(f['t_15_17'] for _, _, f in clone_info)\n",
    "        has_inv3_or_t3_3    = any(f['inv3_or_t3_3'] for _, _, f in clone_info)\n",
    "        has_t_6_9           = any(f['t_6_9'] for _, _, f in clone_info)\n",
    "        has_t_9_22          = any(f['t_9_22'] for _, _, f in clone_info)\n",
    "        has_abn17p          = any(f['abn17p'] for _, _, f in clone_info)\n",
    "\n",
    "        # MK / complex\n",
    "        is_mk  = _is_monosomal_karyotype(k)\n",
    "        is_ck  = _is_complex_karyotype(k)\n",
    "\n",
    "        # baseline chr + ploidie\n",
    "        baseline_chr = _extract_baseline_chr_count(k)\n",
    "        is_hypo      = int(baseline_chr != -1 and baseline_chr < 46)\n",
    "        is_hyper     = int(baseline_chr != -1 and 46 < baseline_chr < 50)\n",
    "        is_near_tet  = int(baseline_chr != -1 and baseline_chr >= 80)\n",
    "\n",
    "        # normal / abnormal / missing\n",
    "        is_normal    = int(bool(_NORMAL_KARYO_RE.match(k)))\n",
    "        is_abnormal  = int(not is_normal and any_abn)\n",
    "        is_missing   = 0  # déjà filtré avant\n",
    "\n",
    "        # ELN-like : favorable / adverse / intermédiaire\n",
    "        eln_favorable = bool(has_t_8_21 or has_inv16_or_t_16_16 or has_t_15_17)\n",
    "        eln_adverse_basic = bool(is_mk or is_ck or has_minus5_or_del5q or has_minus7_or_del7q)\n",
    "        eln_adverse_extended = bool(\n",
    "            eln_adverse_basic\n",
    "            or has_inv3_or_t3_3\n",
    "            or has_t_6_9\n",
    "            or has_t_9_22\n",
    "            or has_abn17p\n",
    "        )\n",
    "\n",
    "        if is_missing:\n",
    "            eln_risk = -1\n",
    "        else:\n",
    "            if eln_adverse_extended:\n",
    "                eln_risk = 2\n",
    "            elif eln_favorable:\n",
    "                eln_risk = 0\n",
    "            else:\n",
    "                # le reste (avec info cytogénétique) = intermédiaire\n",
    "                eln_risk = 1\n",
    "\n",
    "        # clonality helpers\n",
    "        def _prop(cond_fn):\n",
    "            if total_meta_known == 0:\n",
    "                return 0.0\n",
    "            pos = sum(n_meta for _, n_meta, f in clone_info if n_meta and cond_fn(f))\n",
    "            return pos / total_meta_known if total_meta_known else 0.0\n",
    "\n",
    "        # proportion de clones anormaux, défavorables, etc.\n",
    "        prop_any_abnormal = float(_prop(lambda f: f['has_any_abn']))\n",
    "        prop_adverse_5_7  = float(_prop(lambda f: f['minus5_or_del5q'] or f['minus7_or_del7q']))\n",
    "        prop_plus8        = float(_prop(lambda f: f['plus8']))\n",
    "        prop_favorable_core = float(_prop(lambda f: f['t_8_21'] or f['inv16_or_t_16_16']))\n",
    "\n",
    "        # clonality plus fine : max clone, max clone \"adverse\"\n",
    "        max_clone_prop = 0.0\n",
    "        max_adverse_prop = 0.0\n",
    "        has_small_adverse_subclone = 0\n",
    "\n",
    "        if total_meta_known > 0:\n",
    "            for _, n_meta, f in clone_info:\n",
    "                if not n_meta:\n",
    "                    continue\n",
    "                p = n_meta / total_meta_known\n",
    "                if p > max_clone_prop:\n",
    "                    max_clone_prop = p\n",
    "\n",
    "                is_extended_adverse_clone = (\n",
    "                    f['minus5_or_del5q'] or f['minus7_or_del7q'] or\n",
    "                    f['inv3_or_t3_3'] or f['t_6_9'] or f['t_9_22'] or f['abn17p']\n",
    "                )\n",
    "                if is_extended_adverse_clone:\n",
    "                    if p > max_adverse_prop:\n",
    "                        max_adverse_prop = p\n",
    "                    if 0.0 < p < 0.3:\n",
    "                        has_small_adverse_subclone = 1\n",
    "\n",
    "        rows.append({\n",
    "            'is_cyto_missing_or_failed': int(is_missing),\n",
    "            'is_normal_karyotype': int(is_normal),\n",
    "            'is_abnormal_karyotype': int(is_abnormal),\n",
    "            'has_any_abnormality': int(any_abn),\n",
    "            'n_events': int(n_events),\n",
    "            'n_chromosomes_altered': int(n_chrs),\n",
    "            'n_monosomies_total': int(n_mono_tot),\n",
    "            'n_trisomies_total': int(n_tris_tot),\n",
    "            'n_structural_events_total': int(n_struct_tot),\n",
    "            'has_minus5_or_del5q': int(has_minus5_or_del5q),\n",
    "            'has_minus7_or_del7q': int(has_minus7_or_del7q),\n",
    "            'has_plus8': int(has_plus8),\n",
    "            'has_t_8_21': int(has_t_8_21),\n",
    "            'has_inv16_or_t_16_16': int(has_inv16_or_t_16_16),\n",
    "            'has_t_15_17': int(has_t_15_17),\n",
    "            'has_inv3_or_t3_3': int(has_inv3_or_t3_3),\n",
    "            'has_t_6_9': int(has_t_6_9),\n",
    "            'has_t_9_22': int(has_t_9_22),\n",
    "            'has_abn17p': int(has_abn17p),\n",
    "            'is_monosomal_karyotype': int(is_mk),\n",
    "            'is_complex_karyotype': int(is_ck),\n",
    "            'eln_like_flag_adverse_cyto': int(eln_adverse_extended),\n",
    "            'eln_like_flag_favorable_cyto': int(eln_favorable),\n",
    "            'eln_like_flag_intermediate_cyto': int(eln_risk == 1),\n",
    "            'eln_like_risk_cyto': int(eln_risk),\n",
    "            'baseline_chr_count': int(baseline_chr),\n",
    "            'is_hypodiploid': int(is_hypo),\n",
    "            'is_hyperdiploid': int(is_hyper),\n",
    "            'is_near_tetraploid': int(is_near_tet),\n",
    "            'total_metaphases': int(total_meta_known),\n",
    "            'max_clone_size': float(max_clone_prop),\n",
    "            'max_adverse_clone_size': float(max_adverse_prop),\n",
    "            'has_small_adverse_subclone': int(has_small_adverse_subclone),\n",
    "            'prop_any_abnormal': float(prop_any_abnormal),\n",
    "            'prop_adverse_5_7': float(prop_adverse_5_7),\n",
    "            'prop_plus8': float(prop_plus8),\n",
    "            'prop_favorable_core': float(prop_favorable_core),\n",
    "        })\n",
    "\n",
    "    features_df = pd.DataFrame(rows, index=df.index)\n",
    "    # On enlève la colonne brute CYTOGENETICS (comme tu le faisais)\n",
    "    return pd.concat([df.copy(), features_df], axis=1).drop(columns=[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T21:40:57.722575Z",
     "start_time": "2025-11-12T21:40:57.562887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing has_t_8_21: 100.00% of values are 0\n",
      "Removing has_inv16_or_t_16_16: 100.00% of values are 0\n",
      "Removing has_t_15_17: 99.97% of values are 0\n",
      "Removing has_inv3_or_t3_3: 99.81% of values are 0\n",
      "Removing has_t_6_9: 99.97% of values are 0\n",
      "Removing has_t_9_22: 100.00% of values are 0\n",
      "Removing has_abn17p: 97.86% of values are 0\n",
      "Removing eln_like_flag_favorable_cyto: 99.97% of values are 0\n",
      "Removing is_near_tetraploid: 99.94% of values are 0\n",
      "Removing has_small_adverse_subclone: 96.56% of values are 0\n",
      "Removing prop_favorable_core: 100.00% of values are 0.0\n",
      "Fitting RobustScaler for 26 cytogenetics features on training data, transforming training and evaluation data.\n"
     ]
    }
   ],
   "source": [
    "# === Enrichissement avec les features cytogénétiques ===\n",
    "X_enhanced = add_cytogenetics_features(X_w_mutation)\n",
    "X_eval_enhanced = add_cytogenetics_features(X_eval_w_mutation)\n",
    "\n",
    "# Liste élargie de features cytogénétiques (toutes créées par add_cytogenetics_features)\n",
    "cytogenetics_features = [\n",
    "    # statut global / qualité\n",
    "    'is_cyto_missing_or_failed',\n",
    "    'is_normal_karyotype',\n",
    "    'is_abnormal_karyotype',\n",
    "\n",
    "    # complexité / volume d’anomalies\n",
    "    'has_any_abnormality',\n",
    "    'n_events',\n",
    "    'n_chromosomes_altered',\n",
    "    'n_monosomies_total',\n",
    "    'n_trisomies_total',\n",
    "    'n_structural_events_total',\n",
    "\n",
    "    # anomalies spécifiques défavorables / favorables\n",
    "    'has_minus5_or_del5q',\n",
    "    'has_minus7_or_del7q',\n",
    "    'has_plus8',\n",
    "    'has_t_8_21',\n",
    "    'has_inv16_or_t_16_16',\n",
    "    'has_t_15_17',\n",
    "    'has_inv3_or_t3_3',\n",
    "    'has_t_6_9',\n",
    "    'has_t_9_22',\n",
    "    'has_abn17p',\n",
    "\n",
    "    # MK / complexe\n",
    "    'is_monosomal_karyotype',\n",
    "    'is_complex_karyotype',\n",
    "\n",
    "    # résumé type ELN-like\n",
    "    'eln_like_flag_adverse_cyto',\n",
    "    'eln_like_flag_favorable_cyto',\n",
    "    'eln_like_flag_intermediate_cyto',\n",
    "    'eln_like_risk_cyto',\n",
    "\n",
    "    # ploidie\n",
    "    'baseline_chr_count',\n",
    "    'is_hypodiploid',\n",
    "    'is_hyperdiploid',\n",
    "    'is_near_tetraploid',\n",
    "\n",
    "    # clonalité\n",
    "    'total_metaphases',\n",
    "    'max_clone_size',\n",
    "    'max_adverse_clone_size',\n",
    "    'has_small_adverse_subclone',\n",
    "\n",
    "    # proportions clonales sur certains patterns\n",
    "    'prop_any_abnormal',\n",
    "    'prop_adverse_5_7',\n",
    "    'prop_plus8',\n",
    "    'prop_favorable_core',\n",
    "]\n",
    "\n",
    "# === 1) Détection des features quasi constantes (>=95% identique) sur le TRAIN ===\n",
    "nearly_constant_features = []\n",
    "for col in cytogenetics_features:\n",
    "    value_counts = X_enhanced[col].value_counts(dropna=False)\n",
    "    if len(value_counts) > 0:\n",
    "        max_proportion = value_counts.iloc[0] / len(X_enhanced)\n",
    "        if max_proportion >= 0.95:\n",
    "            nearly_constant_features.append(col)\n",
    "            print(f\"Removing {col}: {max_proportion:.2%} of values are {value_counts.index[0]}\")\n",
    "\n",
    "# === 2) Suppression des features quasi constantes dans train et eval ===\n",
    "X_enhanced = X_enhanced.drop(columns=nearly_constant_features)\n",
    "X_eval_enhanced = X_eval_enhanced.drop(columns=nearly_constant_features)\n",
    "\n",
    "# Mettre à jour la liste des features cyto réellement utilisées\n",
    "cytogenetics_features = [f for f in cytogenetics_features if f not in nearly_constant_features]\n",
    "\n",
    "# === 3) RobustScaler sur les features cyto restantes ===\n",
    "cytogenetics_scaler = RobustScaler()\n",
    "print(\n",
    "    f\"Fitting RobustScaler for {len(cytogenetics_features)} cytogenetics features \"\n",
    "    \"on training data, transforming training and evaluation data.\"\n",
    ")\n",
    "X_enhanced[cytogenetics_features] = cytogenetics_scaler.fit_transform(\n",
    "    X_enhanced[cytogenetics_features]\n",
    ")\n",
    "X_eval_enhanced[cytogenetics_features] = cytogenetics_scaler.transform(\n",
    "    X_eval_enhanced[cytogenetics_features]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T21:41:10.283134Z",
     "start_time": "2025-11-12T21:41:10.156809Z"
    }
   },
   "outputs": [],
   "source": [
    "df_enhanced = X_enhanced.merge(target_df, on='ID', how='left')\n",
    "df_eval_enhanced = X_eval_enhanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de gènes utilisés (top 70) : 70\n",
      "Nombre de colonnes finales train (avec targets) : 139\n",
      "Nombre de colonnes finales val (sans targets) : 137\n",
      "Shape train : (3173, 139)\n",
      "Shape val   : (1193, 137)\n",
      "OS_YEARS dans train : True\n",
      "OS_STATUS dans train : True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def add_gene_features(df_clinical_enhanced, df_molecular, gene_list=None, top_k=10):\n",
    "    \"\"\"\n",
    "    Ajoute des features one-hot pour les gènes :\n",
    "    - Si gene_list est None : utilise les top_k gènes les plus fréquents dans df_molecular\n",
    "      (fréquence = nombre de patients distincts dans lesquels le gène apparaît).\n",
    "    - Sinon : force l'usage de gene_list (pour que train et val aient les mêmes colonnes)\n",
    "    \"\"\"\n",
    "    # 1) Définir la liste de gènes de référence\n",
    "    if gene_list is None:\n",
    "        # fréquence par gène = nb d'ID distincts où le gène est présent\n",
    "        gene_counts = (\n",
    "            df_molecular[['ID', 'GENE']]\n",
    "            .drop_duplicates()['GENE']\n",
    "            .value_counts()\n",
    "        )\n",
    "        # on garde les top_k gènes les plus fréquents\n",
    "        gene_list = gene_counts.nlargest(top_k).index.tolist()\n",
    "        print(f\"Nombre de gènes utilisés (top {top_k}) : {len(gene_list)}\")\n",
    "\n",
    "    # 2) Ne garder que les lignes correspondant aux gènes de la liste\n",
    "    df_filtered = df_molecular[df_molecular['GENE'].isin(gene_list)].copy()\n",
    "\n",
    "    # 3) Table ID x GENE (one-hot)\n",
    "    gene_pivot = pd.crosstab(df_filtered['ID'], df_filtered['GENE'])\n",
    "\n",
    "    # 4) S'assurer que toutes les colonnes de gene_list existent\n",
    "    for g in gene_list:\n",
    "        if g not in gene_pivot.columns:\n",
    "            gene_pivot[g] = 0\n",
    "\n",
    "    # 5) Réordonner les colonnes exactement selon gene_list\n",
    "    gene_pivot = gene_pivot[gene_list]\n",
    "\n",
    "    # 6) Binariser et renommer les colonnes\n",
    "    gene_pivot = (gene_pivot > 0).astype(int)\n",
    "    gene_pivot.columns = [f'Gene_{col}' for col in gene_pivot.columns]\n",
    "\n",
    "    # 7) Merge avec les features cliniques\n",
    "    df_final = df_clinical_enhanced.merge(gene_pivot, on='ID', how='left')\n",
    "\n",
    "    # 8) Remplir les NaN des colonnes de gènes avec 0\n",
    "    new_cols = [c for c in df_final.columns if c.startswith('Gene_')]\n",
    "    df_final[new_cols] = df_final[new_cols].fillna(0)\n",
    "\n",
    "    return df_final, gene_list\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# Lecture des données\n",
    "# ======================================================================\n",
    "mol_train_raw = pd.read_csv(\"../../data/molecular_train.csv\")\n",
    "mol_val_raw   = pd.read_csv(\"../../data/molecular_val.csv\")\n",
    "\n",
    "# df_enhanced et df_df_val_pivot sont tes jeux cliniques déjà préparés\n",
    "# df_enhanced      : train clinique\n",
    "# df_df_val_pivotal clinique\n",
    "\n",
    "# ======================================================================\n",
    "# Construction des features gènes pour train et val\n",
    "# ======================================================================\n",
    "df_train_pivot, gene_list_ref = add_gene_features(\n",
    "    df_clinical_enhanced=df_enhanced,\n",
    "    df_molecular=mol_train_raw,\n",
    "    gene_list=None,  # on déduit les top 70 gènes du train\n",
    "    top_k=70\n",
    ")\n",
    "\n",
    "df_val_pivot, _ = add_gene_features(\n",
    "    df_clinical_enhanced=df_eval_enhanced,\n",
    "    df_molecular=mol_val_raw,\n",
    "    gene_list=gene_list_ref  # même liste (top 70) que le train\n",
    ")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# Harmonisation des colonnes entre train et val\n",
    "# ======================================================================\n",
    "\n",
    "# Colonnes de gènes (identiques pour les deux par construction)\n",
    "gene_cols = [c for c in df_train_pivot.columns if c.startswith('Gene_')]\n",
    "\n",
    "# Colonnes cliniques pour les données d'entraînement (incluant OS_YEARS et OS_STATUS)\n",
    "train_clinical_cols = [c for c in df_train_pivot.columns if not c.startswith('Gene_')]\n",
    "\n",
    "# Colonnes cliniques pour les données de validation (excluant OS_YEARS et OS_STATUS)\n",
    "val_clinical_cols = [c for c in df_val_pivot.columns if not c.startswith('Gene_')]\n",
    "\n",
    "# Pour l'harmonisation, on exclut temporairement OS_YEARS et OS_STATUS\n",
    "train_clinical_cols_for_common = [c for c in train_clinical_cols if c not in ['OS_YEARS', 'OS_STATUS']]\n",
    "\n",
    "# On garde uniquement les colonnes cliniques communes (sans les targets)\n",
    "common_clinical = sorted(set(train_clinical_cols_for_common).intersection(val_clinical_cols))\n",
    "\n",
    "# Ordre final pour les données d'entraînement : colonnes cliniques communes + gènes + targets\n",
    "final_features_train = common_clinical + gene_cols + ['OS_YEARS', 'OS_STATUS']\n",
    "\n",
    "# Ordre final pour les données de validation : colonnes cliniques communes + gènes (pas de targets)\n",
    "final_features_val = common_clinical + gene_cols\n",
    "\n",
    "# Sélection des colonnes dans le bon ordre pour train et val\n",
    "df_train_pivot = df_train_pivot[final_features_train]\n",
    "df_val_pivot = df_val_pivot[final_features_val]\n",
    "\n",
    "print(f\"Nombre de colonnes finales train (avec targets) : {len(final_features_train)}\")\n",
    "print(f\"Nombre de colonnes finales val (sans targets) : {len(final_features_val)}\")\n",
    "print(f\"Shape train : {df_train_pivot.shape}\")\n",
    "print(f\"Shape val   : {df_val_pivot.shape}\")\n",
    "\n",
    "# Vérification que OS_YEARS et OS_STATUS sont bien présents dans les données d'entraînement\n",
    "print(f\"OS_YEARS dans train : {'OS_YEARS' in df_train_pivot.columns}\")\n",
    "print(f\"OS_STATUS dans train : {'OS_STATUS' in df_train_pivot.columns}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pivot['cyto_risk_score'] = (\n",
    "    3 * df_train_pivot['is_monosomal_karyotype'] +\n",
    "    3 * df_train_pivot['is_complex_karyotype'] +\n",
    "    2 * df_train_pivot['has_minus7_or_del7q'] +\n",
    "    2 * df_train_pivot['has_minus5_or_del5q'] +\n",
    "    1 * df_train_pivot['has_plus8']\n",
    ")\n",
    "\n",
    "df_val_pivot['cyto_risk_score'] = (\n",
    "    3 * df_val_pivot['is_monosomal_karyotype'] +\n",
    "    3 * df_val_pivot['is_complex_karyotype'] +\n",
    "    2 * df_val_pivot['has_minus7_or_del7q'] +\n",
    "    2 * df_val_pivot['has_minus5_or_del5q'] +\n",
    "    1 * df_val_pivot['has_plus8']\n",
    ")\n",
    "\n",
    "df_train_pivot['TP53_complex_interaction'] = df_train_pivot['Gene_TP53'] * df_train_pivot['is_complex_karyotype']\n",
    "df_train_pivot['ASXL1_minus7_interaction'] = df_train_pivot['Gene_ASXL1'] * df_train_pivot['has_minus7_or_del7q']\n",
    "df_train_pivot['NPM1_normal_interaction'] = df_train_pivot['Gene_NPM1'] * (1 - df_train_pivot['prop_any_abnormal'])\n",
    "\n",
    "df_val_pivot['TP53_complex_interaction'] = df_val_pivot['Gene_TP53'] * df_val_pivot['is_complex_karyotype']\n",
    "df_val_pivot['ASXL1_minus7_interaction'] = df_val_pivot['Gene_ASXL1'] * df_val_pivot['has_minus7_or_del7q']\n",
    "df_val_pivot['NPM1_normal_interaction'] = df_val_pivot['Gene_NPM1'] * (1 - df_val_pivot['prop_any_abnormal'])\n",
    "\n",
    "df_train_pivot['high_risk_chr_load'] = (\n",
    "    (df_val_pivot[\"CHR_5_count\"]) +\n",
    "    df_train_pivot['CHR_7_count'] +\n",
    "    df_train_pivot['CHR_17_count']\n",
    ")\n",
    "\n",
    "df_val_pivot['high_risk_chr_load'] = (\n",
    "    (df_val_pivot[\"CHR_5_count\"]) +\n",
    "    df_val_pivot['CHR_7_count'] +\n",
    "    df_val_pivot['CHR_17_count']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pivot['risk_score_high_genes'] = (\n",
    "    df_train_pivot['Gene_TP53'] +\n",
    "    df_train_pivot['Gene_ASXL1'] +\n",
    "    df_train_pivot['Gene_RUNX1']\n",
    ")\n",
    "\n",
    "df_train_pivot['risk_score_favorable_genes'] = df_train_pivot['Gene_NPM1'] + df_train_pivot['Gene_CEBPA']\n",
    "\n",
    "df_val_pivot['risk_score_high_genes'] = (\n",
    "    df_val_pivot['Gene_TP53'] +    \n",
    "    df_val_pivot['Gene_ASXL1'] +\n",
    "    df_val_pivot['Gene_RUNX1']\n",
    ")\n",
    "\n",
    "df_val_pivot['risk_score_favorable_genes'] = df_val_pivot['Gene_NPM1'] + df_val_pivot['Gene_CEBPA']\n",
    "\n",
    "\n",
    "df_train_pivot['n_splicing_mut'] = df_train_pivot[['Gene_U2AF1','Gene_SRSF2','Gene_SF3B1','Gene_ZRSR2']].sum(axis=1)\n",
    "df_train_pivot['n_signaling_mut'] = df_train_pivot[['Gene_NRAS','Gene_KRAS','Gene_JAK2','Gene_CBL']].sum(axis=1)\n",
    "\n",
    "df_val_pivot['n_splicing_mut'] = df_val_pivot[['Gene_U2AF1','Gene_SRSF2','Gene_SF3B1','Gene_ZRSR2']].sum(axis=1)\n",
    "df_val_pivot['n_signaling_mut'] = df_val_pivot[['Gene_NRAS','Gene_KRAS','Gene_JAK2','Gene_CBL']].sum(axis=1)\n",
    "\n",
    "df_train_pivot['TP53_VAF_interaction'] = df_train_pivot['Gene_TP53'] * df_train_pivot['VAF_avg']\n",
    "df_val_pivot['TP53_VAF_interaction'] = df_val_pivot['Gene_TP53'] * df_val_pivot['VAF_avg']\n",
    "\n",
    "\n",
    "df_train_pivot['ANC_WBC_ratio'] = df_train_pivot['ANC'] / (df_train_pivot['WBC']+1)\n",
    "df_train_pivot['BLAST_WBC_ratio'] = df_train_pivot['BM_BLAST'] / (df_train_pivot['WBC']+1)\n",
    "\n",
    "df_val_pivot['ANC_WBC_ratio'] = df_val_pivot['ANC'] / (df_val_pivot['WBC']+1)\n",
    "df_val_pivot['BLAST_WBC_ratio'] = df_val_pivot['BM_BLAST'] / (df_val_pivot['WBC']+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_train_pivot['major_clone_VAF'] = df_train_pivot['VAF_max']\n",
    "df_train_pivot['subclonality'] = df_train_pivot['VAF_std'] / (df_train_pivot['VAF_avg']+1e-6)\n",
    "\n",
    "df_val_pivot['major_clone_VAF'] = df_val_pivot['VAF_max']\n",
    "df_val_pivot['subclonality'] = df_val_pivot['VAF_std'] / (df_val_pivot['VAF_avg']+1e-6)\n",
    "\n",
    "\n",
    "df_train_pivot['karyo_score_clinical'] = (\n",
    "    3 * df_train_pivot['is_monosomal_karyotype'] +\n",
    "    2 * df_train_pivot['is_complex_karyotype'] +\n",
    "    2 * df_train_pivot['has_minus7_or_del7q'] +\n",
    "    1 * df_train_pivot['has_plus8']\n",
    ")\n",
    "\n",
    "df_val_pivot['karyo_score_clinical'] = (\n",
    "    3 * df_val_pivot['is_monosomal_karyotype'] +\n",
    "    2 * df_val_pivot['is_complex_karyotype'] +\n",
    "    2 * df_val_pivot['has_minus7_or_del7q'] +\n",
    "    1 * df_val_pivot['has_plus8']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_vaf_entropy(df_mut):\n",
    "    \"\"\"\n",
    "    df_mut doit contenir au minimum :\n",
    "        - 'ID'  : identifiant patient\n",
    "        - 'VAF' : fraction variant allele\n",
    "    Retourne un dataframe avec une seule ligne par patient :\n",
    "        ID | vaf_entropy\n",
    "    \"\"\"\n",
    "\n",
    "    # Fonction locale de Shannon entropy\n",
    "    def entropy_from_vaf(vaf_list):\n",
    "        vaf_arr = np.array(vaf_list)\n",
    "        \n",
    "        # Normalisation → proportions p_i\n",
    "        p = vaf_arr / vaf_arr.sum()\n",
    "\n",
    "        # Somme seulement sur p_i > 0 sinon log pose pb.\n",
    "        p = p[p > 0]\n",
    "\n",
    "        return -np.sum(p * np.log(p))\n",
    "\n",
    "    entropy_per_patient = (\n",
    "        df_mut.groupby('ID')['VAF']\n",
    "              .apply(entropy_from_vaf)\n",
    "              .reset_index()\n",
    "              .rename(columns={'VAF': 'vaf_entropy'})\n",
    "    )\n",
    "\n",
    "    return entropy_per_patient\n",
    "\n",
    "\n",
    "entropy_train = compute_vaf_entropy(maf_df)\n",
    "entropy_eval = compute_vaf_entropy(maf_eval)\n",
    "    \n",
    "df_train_pivot = df_train_pivot.merge(entropy_train, on='ID', how='left')\n",
    "df_val_pivot = df_val_pivot.merge(entropy_eval, on='ID', how='left')\n",
    "\n",
    "df_train_pivot[\"vaf_entropy\"] = df_train_pivot[\"vaf_entropy\"].fillna(0)\n",
    "df_val_pivot[\"vaf_entropy\"] = df_val_pivot[\"vaf_entropy\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pivot[\"vaf_entropy\"] = df_train_pivot[\"vaf_entropy\"].fillna(0)\n",
    "df_val_pivot[\"vaf_entropy\"] = df_val_pivot[\"vaf_entropy\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_ratio(num, den):\n",
    "    num = num.astype(float)\n",
    "    den = den.astype(float)\n",
    "    res = num / den\n",
    "    res[~np.isfinite(res)] = np.nan   # remplace inf, -inf, nan par NaN\n",
    "    return res\n",
    "\n",
    "# 1) Remplacer les -1 par NaN dans HB\n",
    "for df in [df_train_pivot, df_val_pivot]:\n",
    "    df.loc[df[\"HB\"] == -1, \"HB\"] = np.nan\n",
    "\n",
    "# 2) Calculer la médiane de HB sur le train\n",
    "hb_median = df_train_pivot[\"HB\"].median()\n",
    "\n",
    "# 3) Imputer les NaN de HB avec cette médiane (même valeur pour train et eval)\n",
    "df_train_pivot[\"HB\"] = df_train_pivot[\"HB\"].fillna(hb_median)\n",
    "df_val_pivot[\"HB\"]  = df_val_pivot[\"HB\"].fillna(hb_median)\n",
    "\n",
    "# 4) Calculer le ratio PLT_HB_ratio\n",
    "for df in [df_train_pivot, df_val_pivot]:\n",
    "    df[\"PLT_HB_ratio\"] = safe_ratio(df[\"PLT\"], df[\"HB\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_ in [df_train_pivot, df_val_pivot]:\n",
    "    for col in ['WBC', 'ANC', 'PLT', 'MONOCYTES', 'BM_BLAST', 'Nmut', 'DEPTH_avg']:\n",
    "        if col in df_.columns:\n",
    "            df_[f\"log1p_{col}\"] = np.log1p(df_[col].clip(lower=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_cols = [c for c in df_train_pivot.columns if c.startswith(\"EFFECT_\") and c.endswith(\"_count\")]\n",
    "\n",
    "low_freq_cols = []\n",
    "for c in mutation_cols:\n",
    "    if (df_train_pivot[c] > 0).mean() < 0.01:   # présent chez <1% des patients\n",
    "        low_freq_cols.append(c)\n",
    "\n",
    "df_train_pivot = df_train_pivot.drop(columns=low_freq_cols)\n",
    "df_val_pivot   = df_val_pivot.drop(columns=[c for c in low_freq_cols if c in df_val_pivot.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "['EFFECT_FV_count', 'EFFECT_LOF_ratio', 'EFFECT_NS_count', 'EFFECT_SG_count', 'EFFECT_nunique']\n"
     ]
    }
   ],
   "source": [
    "print(\"EFFECT_LOF_count\" in df_train_pivot.columns)\n",
    "print([c for c in df_train_pivot.columns if \"EFFECT\" in c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_ in [df_train_pivot, df_val_pivot]:\n",
    "    df_[\"mutation_burden_score\"] = (\n",
    "        0.5 * df_[\"Nmut\"] +\n",
    "        1.5 * df_[\"EFFECT_LOF_ratio\"] +\n",
    "        1.0 * df_[\"vaf_entropy\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_ in [df_train_pivot, df_val_pivot]:\n",
    "    for col in [\"n_events\", \"n_chromosomes_altered\", \"n_monosomies_total\", \"n_trisomies_total\"]:\n",
    "        if col in df_.columns:\n",
    "            df_[f\"log1p_{col}\"] = np.log1p(df_[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Sauvegarde\n",
    "# ======================================================================\n",
    "df_train_pivot.to_csv('../../data/train_pivot4.csv', index=False)\n",
    "df_val_pivot.to_csv('../../data/eval_pivot4.csv',  index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
