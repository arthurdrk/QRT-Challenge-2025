{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7ebffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. Imports & hypothèses de base\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sksurv.util import Surv\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    integrated_brier_score,\n",
    "    cumulative_dynamic_auc,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cbd93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/train_enhanced.csv\")\n",
    "df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "time_col = \"OS_YEARS\"\n",
    "event_col = \"OS_STATUS\"\n",
    "\n",
    "cytogenetics_features = [\n",
    "    # statut global / qualité\n",
    "    'is_cyto_missing_or_failed',\n",
    "    'is_normal_karyotype',\n",
    "    'is_abnormal_karyotype',\n",
    "\n",
    "    # complexité / volume d’anomalies\n",
    "    'has_any_abnormality',\n",
    "    'n_events',\n",
    "    'n_chromosomes_altered',\n",
    "    'n_monosomies_total',\n",
    "    'n_trisomies_total',\n",
    "    'n_structural_events_total',\n",
    "\n",
    "    # anomalies spécifiques défavorables / favorables\n",
    "    'has_minus5_or_del5q',\n",
    "    'has_minus7_or_del7q',\n",
    "    'has_plus8',\n",
    "    'has_t_8_21',\n",
    "    'has_inv16_or_t_16_16',\n",
    "    'has_t_15_17',\n",
    "    'has_inv3_or_t3_3',\n",
    "    'has_t_6_9',\n",
    "    'has_t_9_22',\n",
    "    'has_abn17p',\n",
    "\n",
    "    # MK / complexe\n",
    "    'is_monosomal_karyotype',\n",
    "    'is_complex_karyotype',\n",
    "\n",
    "    # résumé type ELN-like\n",
    "    'eln_like_flag_adverse_cyto',\n",
    "    'eln_like_flag_intermediate_cyto',\n",
    "    'eln_like_risk_cyto',\n",
    "\n",
    "    # ploidie\n",
    "    'baseline_chr_count',\n",
    "    'is_hypodiploid',\n",
    "    'is_hyperdiploid',\n",
    "\n",
    "    # clonalité\n",
    "    'total_metaphases',\n",
    "    'max_clone_size',\n",
    "    'max_adverse_clone_size',\n",
    "\n",
    "    # proportions clonales\n",
    "    'prop_any_abnormal',\n",
    "    'prop_adverse_5_7',\n",
    "    'prop_plus8',\n",
    "\n",
    "    # NEW\n",
    "    'n_autosomal_monosomies',\n",
    "    'n_autosomal_trisomies',\n",
    "    'worst_clone_events',\n",
    "    'worst_clone_is_adverse',\n",
    "]\n",
    "\n",
    "all_features = [c for c in df.columns if c not in [time_col, event_col]]\n",
    "clinical_mutation_features = sorted(list(set(all_features) - set(cytogenetics_features)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035ca22",
   "metadata": {},
   "source": [
    "## Cox univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cd53b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthr\\Desktop\\ENSAE\\QRT-Challenge-2025\\.venv\\Lib\\site-packages\\lifelines\\utils\\__init__.py:1100: ConvergenceWarning: Column(s) ['has_t_8_21'] have very low variance. This may harm convergence. 1) Are you using formula's? Did you mean to add '-1' to the end. 2) Try dropping this redundant column before fitting if convergence fails.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "c:\\Users\\arthr\\Desktop\\ENSAE\\QRT-Challenge-2025\\.venv\\Lib\\site-packages\\lifelines\\utils\\__init__.py:797: RuntimeWarning: invalid value encountered in divide\n",
      "  return (X - mean) / std\n",
      "c:\\Users\\arthr\\Desktop\\ENSAE\\QRT-Challenge-2025\\.venv\\Lib\\site-packages\\lifelines\\utils\\__init__.py:1100: ConvergenceWarning: Column(s) ['has_inv16_or_t_16_16'] have very low variance. This may harm convergence. 1) Are you using formula's? Did you mean to add '-1' to the end. 2) Try dropping this redundant column before fitting if convergence fails.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "c:\\Users\\arthr\\Desktop\\ENSAE\\QRT-Challenge-2025\\.venv\\Lib\\site-packages\\lifelines\\utils\\__init__.py:797: RuntimeWarning: invalid value encountered in divide\n",
      "  return (X - mean) / std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Cox convergence failed for has_t_8_21: Convergence halted due to matrix inversion problems. Suspicion is high collinearity. Please see the following tips in the lifelines documentation: https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-modelMatrix is singular.\n",
      "[WARN] Cox convergence failed for has_inv16_or_t_16_16: Convergence halted due to matrix inversion problems. Suspicion is high collinearity. Please see the following tips in the lifelines documentation: https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-modelMatrix is singular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthr\\Desktop\\ENSAE\\QRT-Challenge-2025\\.venv\\Lib\\site-packages\\lifelines\\utils\\__init__.py:1100: ConvergenceWarning: Column(s) ['has_t_9_22'] have very low variance. This may harm convergence. 1) Are you using formula's? Did you mean to add '-1' to the end. 2) Try dropping this redundant column before fitting if convergence fails.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "c:\\Users\\arthr\\Desktop\\ENSAE\\QRT-Challenge-2025\\.venv\\Lib\\site-packages\\lifelines\\utils\\__init__.py:797: RuntimeWarning: invalid value encountered in divide\n",
      "  return (X - mean) / std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Cox convergence failed for has_t_9_22: Convergence halted due to matrix inversion problems. Suspicion is high collinearity. Please see the following tips in the lifelines documentation: https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-modelMatrix is singular.\n",
      "                       feature        HR  CI_lower_95  CI_upper_95  \\\n",
      "32          worst_clone_events  1.166563     1.146919     1.186542   \n",
      "5        n_chromosomes_altered  1.217920     1.191417     1.245012   \n",
      "4                     n_events  1.132965     1.117225     1.148927   \n",
      "30      n_autosomal_monosomies  1.286198     1.244992     1.328768   \n",
      "17        is_complex_karyotype  3.147919     2.712936     3.652646   \n",
      "8    n_structural_events_total  1.179770     1.153909     1.206210   \n",
      "16      is_monosomal_karyotype  3.269894     2.787992     3.835093   \n",
      "6           n_monosomies_total  1.269493     1.227778     1.312624   \n",
      "10         has_minus7_or_del7q  2.698768     2.298143     3.169231   \n",
      "18  eln_like_flag_adverse_cyto  1.807399     1.620439     2.015929   \n",
      "\n",
      "         p_value   c_index     n  n_events method  \n",
      "32  9.938954e-71  0.596223  3173      1600    cox  \n",
      "5   4.766652e-69  0.596616  3173      1600    cox  \n",
      "4   1.734295e-68  0.596314  3173      1600    cox  \n",
      "30  7.576736e-52  0.564015  3173      1600    cox  \n",
      "17  1.314110e-51  0.558992  3173      1600    cox  \n",
      "8   2.112634e-48  0.570222  3173      1600    cox  \n",
      "16  4.734979e-48  0.552947  3173      1600    cox  \n",
      "6   1.606533e-44  0.557657  3173      1600    cox  \n",
      "10  9.463397e-34  0.543257  3173      1600    cox  \n",
      "18  2.298092e-26  0.568459  3173      1600    cox  \n"
     ]
    }
   ],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "from lifelines.exceptions import ConvergenceError\n",
    "from lifelines.statistics import logrank_test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def run_univariate_cox(\n",
    "    df: pd.DataFrame,\n",
    "    features: list,\n",
    "    time_col: str,\n",
    "    event_col: str,\n",
    "    min_events: int = 10,\n",
    "    penalizer: float = 0.1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pour chaque feature dans `features`, fit un Cox PH univarié.\n",
    "    Retourne HR, IC 95 %, p-value, C-index.\n",
    "    - utilise un penalizer (ridge) pour stabiliser les fit\n",
    "    - fallback log-rank pour les variables binaires qui ne convergent pas\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for feat in features:\n",
    "        sub = df[[time_col, event_col, feat]].copy().dropna()\n",
    "\n",
    "        # nb d'événements total sur ce sous-échantillon\n",
    "        n_events = int(sub[event_col].sum())\n",
    "        if n_events < min_events:\n",
    "            # trop peu d'événements pour quelque chose de stable\n",
    "            continue\n",
    "\n",
    "        # détecter si la feature est binaire (0/1 ou deux valeurs distinctes)\n",
    "        unique_vals = sub[feat].unique()\n",
    "        is_binary = (len(unique_vals) == 2)\n",
    "\n",
    "        # essayer d'abord un CoxPH avec pénalisation\n",
    "        try:\n",
    "            cph = CoxPHFitter(penalizer=penalizer)\n",
    "            cph.fit(sub, duration_col=time_col, event_col=event_col)\n",
    "\n",
    "            summary = cph.summary.loc[feat]\n",
    "\n",
    "            # lifelines donne directement les HR et IC exponentiés\n",
    "            hr = summary[\"exp(coef)\"]\n",
    "            ci_lower = summary[\"exp(coef) lower 95%\"]\n",
    "            ci_upper = summary[\"exp(coef) upper 95%\"]\n",
    "            p_value = summary[\"p\"]\n",
    "\n",
    "            c_index = cph.concordance_index_\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"feature\": feat,\n",
    "                    \"HR\": float(hr),\n",
    "                    \"CI_lower_95\": float(ci_lower),\n",
    "                    \"CI_upper_95\": float(ci_upper),\n",
    "                    \"p_value\": float(p_value),\n",
    "                    \"c_index\": float(c_index),\n",
    "                    \"n\": len(sub),\n",
    "                    \"n_events\": n_events,\n",
    "                    \"method\": \"cox\",\n",
    "                }\n",
    "            )\n",
    "            continue  # on passe à la feature suivante si tout s'est bien passé\n",
    "\n",
    "        except ConvergenceError as e:\n",
    "            print(f\"[WARN] Cox convergence failed for {feat}: {e}\")\n",
    "            # on tente un fallback seulement pour les variables binaires\n",
    "            if not is_binary:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not fit Cox for {feat}: {e}\")\n",
    "            if not is_binary:\n",
    "                continue\n",
    "\n",
    "        # ================================\n",
    "        # Fallback : test de log-rank binaire\n",
    "        # ================================\n",
    "        # On suppose ici que les valeurs sont 0 / 1 (sinon on mappe)\n",
    "        vals = sorted(unique_vals)\n",
    "        v0, v1 = vals[0], vals[1]\n",
    "\n",
    "        grp0 = sub[sub[feat] == v0]\n",
    "        grp1 = sub[sub[feat] == v1]\n",
    "\n",
    "        # il faut des événements dans *les deux* groupes pour que ce soit informatif\n",
    "        if grp0[event_col].sum() == 0 or grp1[event_col].sum() == 0:\n",
    "            # séparation complète : HR non estimable proprement\n",
    "            # mais tu peux quand même garder l'info : direction suggérée mais instable\n",
    "            results.append(\n",
    "                {\n",
    "                    \"feature\": feat,\n",
    "                    \"HR\": np.nan,\n",
    "                    \"CI_lower_95\": np.nan,\n",
    "                    \"CI_upper_95\": np.nan,\n",
    "                    \"p_value\": np.nan,\n",
    "                    \"c_index\": np.nan,\n",
    "                    \"n\": len(sub),\n",
    "                    \"n_events\": n_events,\n",
    "                    \"method\": \"failed_separation\",\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # log-rank test\n",
    "        lr = logrank_test(\n",
    "            grp0[time_col],\n",
    "            grp1[time_col],\n",
    "            event_observed_A=grp0[event_col],\n",
    "            event_observed_B=grp1[event_col],\n",
    "        )\n",
    "\n",
    "        # HR approximatif = ratio des taux d'événements par unité de temps\n",
    "        rate0 = grp0[event_col].sum() / grp0[time_col].sum()\n",
    "        rate1 = grp1[event_col].sum() / grp1[time_col].sum()\n",
    "        hr_approx = rate1 / rate0 if rate0 > 0 else np.nan\n",
    "\n",
    "        # C-index simple basé sur le score binaire (0/1)\n",
    "        # -> plus 1 = plus à risque\n",
    "        # on utilise la définition classique : fraction de paires concordantes\n",
    "        # lifelines a une fonction utilitaire, mais on peut faire simple :\n",
    "        from lifelines.utils import concordance_index\n",
    "\n",
    "        c_index_bin = concordance_index(\n",
    "            sub[time_col].values,\n",
    "            -sub[feat].values,  # plus grand score = plus faible survie\n",
    "            sub[event_col].values,\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"feature\": feat,\n",
    "                \"HR\": float(hr_approx),\n",
    "                \"CI_lower_95\": np.nan,  # pas trivial en fallback → on laisse NaN\n",
    "                \"CI_upper_95\": np.nan,\n",
    "                \"p_value\": float(lr.p_value),\n",
    "                \"c_index\": float(c_index_bin),\n",
    "                \"n\": len(sub),\n",
    "                \"n_events\": n_events,\n",
    "                \"method\": \"logrank_fallback\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "    if not res_df.empty:\n",
    "        # trier par p-value, en mettant les NaN à la fin\n",
    "        res_df = res_df.sort_values(\"p_value\", na_position=\"last\")\n",
    "    return res_df\n",
    "\n",
    "\n",
    "univ_results = run_univariate_cox(\n",
    "    df=df,\n",
    "    features=cytogenetics_features,\n",
    "    time_col=time_col,\n",
    "    event_col=event_col,\n",
    "    min_events=10,\n",
    "    penalizer=0.1,   # tu peux jouer avec 0.01 / 0.1 / 1\n",
    ")\n",
    "\n",
    "print(univ_results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8bcd273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 2. Préparation données survie\n",
    "# ===============================\n",
    "\n",
    "y = Surv.from_dataframe(event=event_col, time=time_col, data=df)\n",
    "\n",
    "X_clinical = df[clinical_mutation_features].copy()\n",
    "X_clinical_plus_cyto = df[clinical_mutation_features + cytogenetics_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    integrated_brier_score,\n",
    "    cumulative_dynamic_auc,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def make_cindex_scorer(event_field, time_field):\n",
    "    \"\"\"\n",
    "    Scorer sklearn pour C-index, compatible avec scikit-survival.\n",
    "    event_field et time_field viennent de y.dtype.names (ex: 'OS_STATUS', 'OS_YEARS').\n",
    "    \"\"\"\n",
    "    def cindex_scorer(estimator, X, y):\n",
    "        est = clone(estimator)\n",
    "        est.fit(X, y)\n",
    "        risk_scores = est.predict(X)\n",
    "        event = y[event_field].astype(bool)\n",
    "        time = y[time_field]\n",
    "        cindex = concordance_index_censored(event, time, risk_scores)[0]\n",
    "        return cindex\n",
    "\n",
    "    return cindex_scorer\n",
    "\n",
    "\n",
    "def nested_cv_survival(\n",
    "    X: pd.DataFrame,\n",
    "    y,\n",
    "    times_auc=(1.0, 2.0, 3.0),   # OS_YEARS en années\n",
    "    n_splits_outer=5,\n",
    "    n_splits_inner=3,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Nested CV pour CoxnetSurvivalAnalysis, avec:\n",
    "    - standardisation\n",
    "    - chemin d'alphas \"safe\"\n",
    "    - fit_baseline_model=True pour pouvoir appeler predict_survival_function.\n",
    "    \"\"\"\n",
    "\n",
    "    # noms de champs dans y, par ex. ('OS_STATUS', 'OS_YEARS')\n",
    "    event_field, time_field = y.dtype.names\n",
    "\n",
    "    # chemin d'alphas \"safe\" : de 0.1 à 100, suffisamment régularisé\n",
    "    safe_alphas = np.logspace(-1, 2, 40)  # 0.1 ... 100\n",
    "\n",
    "    # pipeline : standardisation + Coxnet\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "            (\"coxnet\", CoxnetSurvivalAnalysis(\n",
    "                alphas=safe_alphas,\n",
    "                l1_ratio=0.5,\n",
    "                fit_baseline_model=True,   # IMPORTANT pour predict_survival_function\n",
    "            )),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        \"coxnet__l1_ratio\": [0.1, 0.5, 0.9],\n",
    "    }\n",
    "\n",
    "    # Stratification sur l'indicateur d'événement réel (OS_STATUS)\n",
    "    event_indicator = y[event_field].astype(int)\n",
    "\n",
    "    outer_cv = StratifiedKFold(\n",
    "        n_splits=n_splits_outer,\n",
    "        shuffle=True,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    fold_results = {\n",
    "        \"c_index\": [],\n",
    "        \"ibs\": [],\n",
    "        \"auc_per_time\": [],\n",
    "        \"times_auc\": np.array(times_auc, dtype=float),\n",
    "    }\n",
    "\n",
    "    scorer = make_cindex_scorer(event_field, time_field)\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(\n",
    "        outer_cv.split(X, event_indicator)\n",
    "    ):\n",
    "        print(f\"Outer fold {fold_idx + 1}/{n_splits_outer}\")\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        inner_cv = KFold(\n",
    "            n_splits=n_splits_inner,\n",
    "            shuffle=True,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        gs = GridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scorer,\n",
    "            cv=inner_cv,\n",
    "            n_jobs=-1,\n",
    "            # pendant le debug tu peux mettre error_score=\"raise\"\n",
    "        )\n",
    "        gs.fit(X_train, y_train)\n",
    "\n",
    "        best_model = gs.best_estimator_\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        # --- C-index sur le fold test ---\n",
    "        risk_scores_test = best_model.predict(X_test)\n",
    "        cindex_test = concordance_index_censored(\n",
    "            y_test[event_field].astype(bool),\n",
    "            y_test[time_field],\n",
    "            risk_scores_test,\n",
    "        )[0]\n",
    "\n",
    "         # --- IBS ---\n",
    "        times_grid = np.linspace(\n",
    "            np.percentile(y_train[time_field], 5),\n",
    "            np.percentile(y_train[time_field], 95),\n",
    "            50,\n",
    "        )\n",
    "\n",
    "        surv_funcs_test = best_model.predict_survival_function(X_test)\n",
    "        pred_surv_test = np.row_stack(\n",
    "            [fn(times_grid) for fn in surv_funcs_test]\n",
    "        )\n",
    "\n",
    "        # contrainte sksurv : les temps du test doivent être <= max des temps du train\n",
    "        max_time_train = y_train[time_field].max()\n",
    "\n",
    "        # on coupe la grille de temps pour l'IBS\n",
    "        times_grid_ib = times_grid[times_grid <= max_time_train]\n",
    "\n",
    "        # on filtre les patients de test avec un temps > max_time_train\n",
    "        mask_ib = y_test[time_field] <= max_time_train\n",
    "        y_test_ib = y_test[mask_ib]\n",
    "        pred_surv_test_ib = pred_surv_test[mask_ib, :]\n",
    "\n",
    "        # si trop peu de points, on met NaN\n",
    "        if (len(times_grid_ib) < 2) or (len(y_test_ib) == 0):\n",
    "            ibs = np.nan\n",
    "        else:\n",
    "            ibs = integrated_brier_score(\n",
    "                y_train,\n",
    "                y_test_ib,\n",
    "                pred_surv_test_ib,\n",
    "                times_grid_ib,\n",
    "            )\n",
    "\n",
    "        # --- AUC(t) ---\n",
    "        _, auc_values = cumulative_dynamic_auc(\n",
    "            y_train,\n",
    "            y_test,\n",
    "            risk_scores_test,\n",
    "            np.array(times_auc, dtype=float),\n",
    "        )\n",
    "\n",
    "        fold_results[\"c_index\"].append(cindex_test)\n",
    "        fold_results[\"ibs\"].append(ibs)\n",
    "        fold_results[\"auc_per_time\"].append(auc_values)\n",
    "\n",
    "    return fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c19f3e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Nested CV: modèle SANS cytogénétique ===\n",
      "Outer fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthr\\AppData\\Local\\Temp\\ipykernel_73260\\2601403825.py:130: DeprecationWarning: `row_stack` alias is deprecated. Use `np.vstack` directly.\n",
      "  pred_surv_test = np.row_stack(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthr\\AppData\\Local\\Temp\\ipykernel_73260\\2601403825.py:130: DeprecationWarning: `row_stack` alias is deprecated. Use `np.vstack` directly.\n",
      "  pred_surv_test = np.row_stack(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthr\\AppData\\Local\\Temp\\ipykernel_73260\\2601403825.py:130: DeprecationWarning: `row_stack` alias is deprecated. Use `np.vstack` directly.\n",
      "  pred_surv_test = np.row_stack(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time must be smaller than largest observed time point: 17.3753424657534",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m times_auc = (\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m, \u001b[32m3.0\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Nested CV: modèle SANS cytogénétique ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results_no_cyto = \u001b[43mnested_cv_survival\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_clinical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimes_auc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimes_auc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_splits_outer\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_splits_inner\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Nested CV: modèle AVEC cytogénétique ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m results_with_cyto = nested_cv_survival(\n\u001b[32m     20\u001b[39m     X=X_clinical_plus_cyto,\n\u001b[32m     21\u001b[39m     y=y,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     random_state=\u001b[32m42\u001b[39m,\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mnested_cv_survival\u001b[39m\u001b[34m(X, y, times_auc, n_splits_outer, n_splits_inner, random_state)\u001b[39m\n\u001b[32m    129\u001b[39m surv_funcs_test = best_model.predict_survival_function(X_test)\n\u001b[32m    130\u001b[39m pred_surv_test = np.row_stack(\n\u001b[32m    131\u001b[39m     [fn(times_grid) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m surv_funcs_test]\n\u001b[32m    132\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m ibs = \u001b[43mintegrated_brier_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_surv_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes_grid\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# --- AUC(t) ---\u001b[39;00m\n\u001b[32m    139\u001b[39m _, auc_values = cumulative_dynamic_auc(\n\u001b[32m    140\u001b[39m     y_train,\n\u001b[32m    141\u001b[39m     y_test,\n\u001b[32m    142\u001b[39m     risk_scores_test,\n\u001b[32m    143\u001b[39m     np.array(times_auc, dtype=\u001b[38;5;28mfloat\u001b[39m),\n\u001b[32m    144\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthr\\Desktop\\ENSAE\\QRT-Challenge-2025\\.venv\\Lib\\site-packages\\sksurv\\metrics.py:736\u001b[39m, in \u001b[36mintegrated_brier_score\u001b[39m\u001b[34m(survival_train, survival_test, estimate, times)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The Integrated Brier Score (IBS) provides an overall calculation of\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[33;03mthe model performance at all available times :math:`t_1 \\\\leq t \\\\leq t_\\\\text{max}`.\u001b[39;00m\n\u001b[32m    641\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    733\u001b[39m \u001b[33;03m       Statistics in Medicine, vol. 18, no. 17-18, pp. 2529–2545, 1999.\u001b[39;00m\n\u001b[32m    734\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    735\u001b[39m \u001b[38;5;66;03m# Computing the brier scores\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m times, brier_scores = \u001b[43mbrier_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurvival_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurvival_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m times.shape[\u001b[32m0\u001b[39m] < \u001b[32m2\u001b[39m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least two time points must be given\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthr\\Desktop\\ENSAE\\QRT-Challenge-2025\\.venv\\Lib\\site-packages\\sksurv\\metrics.py:620\u001b[39m, in \u001b[36mbrier_score\u001b[39m\u001b[34m(survival_train, survival_test, estimate, times)\u001b[39m\n\u001b[32m    618\u001b[39m prob_cens_t[prob_cens_t == \u001b[32m0\u001b[39m] = np.inf\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# calculate inverse probability of censoring weights at observed time point\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m prob_cens_y = \u001b[43mcens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m prob_cens_y[prob_cens_y == \u001b[32m0\u001b[39m] = np.inf\n\u001b[32m    623\u001b[39m \u001b[38;5;66;03m# Calculating the brier scores at each time point\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthr\\Desktop\\ENSAE\\QRT-Challenge-2025\\.venv\\Lib\\site-packages\\sksurv\\nonparametric.py:531\u001b[39m, in \u001b[36mSurvivalFunctionEstimator.predict_proba\u001b[39m\u001b[34m(self, time, return_conf_int)\u001b[39m\n\u001b[32m    529\u001b[39m extends = time > \u001b[38;5;28mself\u001b[39m.unique_time_[-\u001b[32m1\u001b[39m]\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prob_[-\u001b[32m1\u001b[39m] > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m extends.any():\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtime must be smaller than largest observed time point: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.unique_time_[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# beyond last time point is zero probability\u001b[39;00m\n\u001b[32m    534\u001b[39m Shat = np.empty(time.shape, dtype=\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: time must be smaller than largest observed time point: 17.3753424657534"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 4. Lancer les évaluations\n",
    "# ===============================\n",
    "\n",
    "# OS_YEARS est en années -> 1, 2, 3 ans\n",
    "times_auc = (1.0, 2.0, 3.0)\n",
    "\n",
    "print(\"=== Nested CV: modèle SANS cytogénétique ===\")\n",
    "results_no_cyto = nested_cv_survival(\n",
    "    X=X_clinical,\n",
    "    y=y,\n",
    "    times_auc=times_auc,\n",
    "    n_splits_outer=5,\n",
    "    n_splits_inner=3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Nested CV: modèle AVEC cytogénétique ===\")\n",
    "results_with_cyto = nested_cv_survival(\n",
    "    X=X_clinical_plus_cyto,\n",
    "    y=y,\n",
    "    times_auc=times_auc,\n",
    "    n_splits_outer=5,\n",
    "    n_splits_inner=3,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 5. Résumé des performances\n",
    "# ===============================\n",
    "\n",
    "def summarize_nested_results(results, label=\"model\"):\n",
    "    cidx = np.array(results[\"c_index\"])\n",
    "    ibs = np.array(results[\"ibs\"])\n",
    "    auc_per_time = np.vstack(results[\"auc_per_time\"])  # shape (n_folds, n_times)\n",
    "    times_auc = results[\"times_auc\"]\n",
    "\n",
    "    summary = {\n",
    "        \"label\": label,\n",
    "        \"c_index_mean\": float(cidx.mean()),\n",
    "        \"c_index_std\": float(cidx.std()),\n",
    "        \"ibs_mean\": float(ibs.mean()),\n",
    "        \"ibs_std\": float(ibs.std()),\n",
    "    }\n",
    "\n",
    "    # AUC(t) : moyenne et std par temps\n",
    "    for i, t in enumerate(times_auc):\n",
    "        summary[f\"auc_{t:.0f}_mean\"] = float(auc_per_time[:, i].mean())\n",
    "        summary[f\"auc_{t:.0f}_std\"] = float(auc_per_time[:, i].std())\n",
    "\n",
    "    return pd.Series(summary)\n",
    "\n",
    "\n",
    "summary_no_cyto = summarize_nested_results(results_no_cyto, label=\"no_cyto\")\n",
    "summary_with_cyto = summarize_nested_results(results_with_cyto, label=\"with_cyto\")\n",
    "\n",
    "comparison_df = pd.concat([summary_no_cyto, summary_with_cyto], axis=1)\n",
    "print(\"\\n=== Résumé performances (nested CV) ===\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab13c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 6. Δ C-index / Δ IBS / Δ AUC\n",
    "# ===============================\n",
    "\n",
    "delta = pd.Series(\n",
    "    {\n",
    "        \"delta_c_index\": summary_with_cyto[\"c_index_mean\"] - summary_no_cyto[\"c_index_mean\"],\n",
    "        \"delta_ibs\": summary_with_cyto[\"ibs_mean\"] - summary_no_cyto[\"ibs_mean\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "for t in times_auc:\n",
    "    delta[f\"delta_auc_{t:.0f}\"] = (\n",
    "        summary_with_cyto[f\"auc_{t:.0f}_mean\"]\n",
    "        - summary_no_cyto[f\"auc_{t:.0f}_mean\"]\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Deltas (with_cyto - no_cyto) ===\")\n",
    "print(delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b867bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 7. Calibration plot (optionnel)\n",
    "# ===============================\n",
    "# On refit un modèle final AVEC cyto sur tout le dataset,\n",
    "# puis on trace la calibration à un temps donné (ex: 2 ans).\n",
    "\n",
    "calib_time = 730.0  # 2 ans si jours\n",
    "\n",
    "# Refit modèle final avec meilleurs hyperparam (on refait un GridSearch sur tout le jeu)\n",
    "pipe_final = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"coxnet\", CoxnetSurvivalAnalysis(l1_ratio=0.5, n_alphas=50)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid_final = {\"coxnet__l1_ratio\": [0.1, 0.5, 0.9]}\n",
    "\n",
    "gs_final = GridSearchCV(\n",
    "    estimator=pipe_final,\n",
    "    param_grid=param_grid_final,\n",
    "    scoring=make_cindex_scorer(),\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1,\n",
    ")\n",
    "gs_final.fit(X_clinical_plus_cyto, y)\n",
    "best_final = gs_final.best_estimator_\n",
    "best_final.fit(X_clinical_plus_cyto, y)\n",
    "\n",
    "# Probabilité de survie à calib_time\n",
    "surv_funcs_all = best_final.predict_survival_function(X_clinical_plus_cyto)\n",
    "# pour chaque patient, on prend S(t_calib)\n",
    "surv_at_calib = np.array([fn(calib_time) for fn in surv_funcs_all])\n",
    "\n",
    "# Observé vs prédit : on peut faire des \"bins\" de risque\n",
    "n_bins = 10\n",
    "quantiles = np.quantile(surv_at_calib, np.linspace(0, 1, n_bins + 1))\n",
    "\n",
    "bin_ids = np.digitize(surv_at_calib, quantiles[1:-1], right=True)\n",
    "\n",
    "bin_pred_surv = []\n",
    "bin_obs_surv = []\n",
    "\n",
    "for b in range(n_bins):\n",
    "    mask = bin_ids == b\n",
    "    if mask.sum() < 5:\n",
    "        continue\n",
    "    # survie prédite moyenne\n",
    "    bin_pred_surv.append(surv_at_calib[mask].mean())\n",
    "    # survie observée à calib_time via KM\n",
    "    # on calcule la probabilité de survie observée dans ce bin à ce temps\n",
    "    from sksurv.nonparametric import kaplan_meier_estimator\n",
    "\n",
    "    t, s = kaplan_meier_estimator(\n",
    "        y[\"event\"][mask], y[\"time\"][mask]\n",
    "    )\n",
    "    # survie observée = valeur de S(t) au plus proche de calib_time\n",
    "    obs = s[t <= calib_time][-1] if np.any(t <= calib_time) else 1.0\n",
    "    bin_obs_surv.append(obs)\n",
    "\n",
    "bin_pred_surv = np.array(bin_pred_surv)\n",
    "bin_obs_surv = np.array(bin_obs_surv)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(bin_pred_surv, bin_obs_surv, marker=\"o\")\n",
    "plt.plot([0, 1], [0, 1], \"--\", label=\"Perfect calibration\")\n",
    "plt.xlabel(\"Predicted survival probability at t={:.0f}\".format(calib_time))\n",
    "plt.ylabel(\"Observed survival (KM)\")\n",
    "plt.title(\"Calibration curve at t={:.0f}\".format(calib_time))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
