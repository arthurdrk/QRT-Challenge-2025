{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04abbc46",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# MTLR + 5-Model Ensemble (Independent Seeds & Hyperparameters)\n",
    "\n",
    "- Model: MTLR (torchmtlr)\n",
    "- Ensemble: 5 models, each with its own Optuna optimization (different seeds)\n",
    "- Metric: IPCW C-index (sksurv)\n",
    "- Validation: nested CV (outer for evaluation, inner for hyperparameter tuning)\n",
    "- Ensembling: rank-based aggregation of risk scores\n",
    "- Statistical evaluation: mean, std, 95% CI, paired t-test (ensemble vs single model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cfdf22",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchmtlr import MTLR, mtlr_neg_log_likelihood\n",
    "from torchmtlr.utils import encode_survival, make_time_bins\n",
    "\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Global seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Main configuration\n",
    "N_MODELS_ENSEMBLE = 5        # number of models in the ensemble\n",
    "N_SPLITS_OUTER = 5           # number of outer folds\n",
    "N_SPLITS_INNER = 3           # number of inner folds (Optuna CV)\n",
    "N_TRIALS_INNER = 30          # number of Optuna trials per model per outer fold\n",
    "TAU_CINDEX = 7.0             # horizon for IPCW C-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b23968",
   "metadata": {},
   "source": [
    "# 1. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../../data/train_enhanced.csv\")\n",
    "df_eval = pd.read_csv(\"../../data/eval_enhanced.csv\")\n",
    "\n",
    "print(f\"Train data: {df_train.shape}\")\n",
    "print(f\"Eval data : {df_eval.shape}\")\n",
    "print(\"\\nTrain columns:\")\n",
    "print(df_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\"OS_STATUS\", \"OS_YEARS\"]\n",
    "\n",
    "# Train features\n",
    "X = df_train.drop(columns=target_cols + [\"ID\"])\n",
    "X = pd.get_dummies(X, drop_first=True).astype(float)\n",
    "\n",
    "# Eval features (aligned with train)\n",
    "X_eval = df_eval.drop(columns=[\"ID\"])\n",
    "X_eval = pd.get_dummies(X_eval, drop_first=True)\n",
    "X_eval = X_eval.reindex(columns=X.columns, fill_value=0).astype(float)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index,\n",
    ")\n",
    "X_eval_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_eval),\n",
    "    columns=X_eval.columns,\n",
    "    index=X_eval.index,\n",
    ")\n",
    "\n",
    "X = X_scaled.fillna(0)\n",
    "X_eval = X_eval_scaled.fillna(0)\n",
    "\n",
    "# Targets (torch tensors)\n",
    "y_time = torch.tensor(df_train[\"OS_YEARS\"].values, dtype=torch.float32)\n",
    "y_event = torch.tensor(df_train[\"OS_STATUS\"].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"X shape      : {X.shape}\")\n",
    "print(f\"X_eval shape : {X_eval.shape}\")\n",
    "print(f\"NaN in X      : {X.isna().sum().sum()}\")\n",
    "print(f\"NaN in X_eval : {X_eval.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3600955",
   "metadata": {},
   "source": [
    "# 2. MTLR Utilities & Risk Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeee340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(seed: int):\n",
    "    \"\"\"Set NumPy and PyTorch seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def build_mtlr_model(input_dim: int, time_bins, params: dict) -> nn.Module:\n",
    "    \"\"\"Build an MTLR neural network from a hyperparameter dictionary.\"\"\"\n",
    "    if params[\"activation\"] == \"relu\":\n",
    "        activation = nn.ReLU()\n",
    "    elif params[\"activation\"] == \"leaky_relu\":\n",
    "        activation = nn.LeakyReLU()\n",
    "    else:\n",
    "        activation = nn.ELU()\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_dim, params[\"n_hidden1\"]),\n",
    "        nn.BatchNorm1d(params[\"n_hidden1\"]),\n",
    "        activation,\n",
    "        nn.Dropout(params[\"dropout1\"]),\n",
    "        nn.Linear(params[\"n_hidden1\"], params[\"n_hidden2\"]),\n",
    "        nn.BatchNorm1d(params[\"n_hidden2\"]),\n",
    "        activation,\n",
    "        nn.Dropout(params[\"dropout2\"]),\n",
    "        MTLR(params[\"n_hidden2\"], len(time_bins)),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_optimizer(model: nn.Module, params: dict):\n",
    "    \"\"\"Return an optimizer configured with the given hyperparameters.\"\"\"\n",
    "    if params[\"optimizer\"] == \"adamw\":\n",
    "        return torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=params[\"lr\"],\n",
    "            weight_decay=params[\"weight_decay\"],\n",
    "        )\n",
    "    else:\n",
    "        return torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=params[\"lr\"],\n",
    "            weight_decay=params[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "\n",
    "def train_mtlr_model(\n",
    "    model: nn.Module,\n",
    "    optimizer,\n",
    "    X_tensor: torch.Tensor,\n",
    "    y_time_fold: torch.Tensor,\n",
    "    y_event_fold: torch.Tensor,\n",
    "    time_bins,\n",
    "    C1: float,\n",
    "    n_epochs: int,\n",
    "    clip_grad: float = 1.0,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train an MTLR model on a given dataset.\n",
    "    Returns the trained model and the list of losses.\n",
    "    If NaN occurs in training loss, returns (None, None).\n",
    "    \"\"\"\n",
    "    target = encode_survival(y_time_fold, y_event_fold, time_bins)\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_tensor)\n",
    "        loss = mtlr_neg_log_likelihood(\n",
    "            logits, target, model[-1], C1=C1, average=True\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            # signal failure\n",
    "            return None, None\n",
    "\n",
    "        loss.backward()\n",
    "        if clip_grad is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if verbose and ((epoch + 1) % max(1, n_epochs // 5) == 0):\n",
    "            print(f\"  Epoch {epoch+1}/{n_epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "\n",
    "def mtlr_risk_scores(model: nn.Module, X_tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute MTLR risk scores using log-sum-exp over logits.\n",
    "    Higher score = higher risk.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        risk = torch.logsumexp(logits, dim=1).cpu().numpy()\n",
    "    return risk\n",
    "\n",
    "\n",
    "def rank_based_ensemble(risk_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rank-based ensembling:\n",
    "      - risk_matrix: shape (n_models, n_samples)\n",
    "      - For each model, convert risk scores to ranks (1 = lowest risk, N = highest risk),\n",
    "        then average ranks across models.\n",
    "    \"\"\"\n",
    "    n_models, n_samples = risk_matrix.shape\n",
    "    ranks_all = np.zeros_like(risk_matrix, dtype=float)\n",
    "\n",
    "    for m in range(n_models):\n",
    "        scores = risk_matrix[m]\n",
    "        order = np.argsort(scores)  # ascending\n",
    "        ranks = np.empty_like(order, dtype=float)\n",
    "        ranks[order] = np.arange(1, n_samples + 1)\n",
    "        ranks_all[m] = ranks\n",
    "\n",
    "    mean_ranks = ranks_all.mean(axis=0)\n",
    "    return mean_ranks\n",
    "\n",
    "\n",
    "def compute_ipcw_cindex(y_train_struct, y_test_struct, risk_scores, tau: float):\n",
    "    \"\"\"Compute IPCW C-index on test data.\"\"\"\n",
    "    return concordance_index_ipcw(\n",
    "        y_train_struct,\n",
    "        y_test_struct,\n",
    "        risk_scores,\n",
    "        tau=tau,\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e14090",
   "metadata": {},
   "source": [
    "# 3. Nested Cross-Validation: Outer (Evaluation) / Inner (Optuna) + 5-Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_kf = KFold(n_splits=N_SPLITS_OUTER, shuffle=True, random_state=123)\n",
    "\n",
    "cv_single_scores = []    # single model = first model in the ensemble\n",
    "cv_ensemble_scores = []  # ensemble of 5 models (rank-based)\n",
    "\n",
    "print(\"=== Starting outer cross-validation ===\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(outer_kf.split(X), 1):\n",
    "    print(f\"\\n######## Outer Fold {fold}/{N_SPLITS_OUTER} ########\")\n",
    "\n",
    "    # --- Outer fold data ---\n",
    "    X_train_outer = X.iloc[train_idx].reset_index(drop=True)\n",
    "    X_test_outer = X.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    y_time_train_outer = y_time[train_idx]\n",
    "    y_time_test_outer = y_time[test_idx]\n",
    "    y_event_train_outer = y_event[train_idx]\n",
    "    y_event_test_outer = y_event[test_idx]\n",
    "\n",
    "    X_train_outer_tensor = torch.tensor(\n",
    "        X_train_outer.values, dtype=torch.float32\n",
    "    )\n",
    "    X_test_outer_tensor = torch.tensor(\n",
    "        X_test_outer.values, dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    time_bins_outer = make_time_bins(y_time_train_outer, event=y_event_train_outer)\n",
    "\n",
    "    # Surv structures for outer C-index\n",
    "    y_train_struct_outer = Surv.from_arrays(\n",
    "        event=y_event_train_outer.numpy().astype(bool),\n",
    "        time=y_time_train_outer.numpy(),\n",
    "    )\n",
    "    y_test_struct_outer = Surv.from_arrays(\n",
    "        event=y_event_test_outer.numpy().astype(bool),\n",
    "        time=y_time_test_outer.numpy(),\n",
    "    )\n",
    "\n",
    "    # --- Ensemble: train 5 independent models on this outer fold ---\n",
    "    risk_models_test = []\n",
    "    single_cindex_this_fold = None  # C-index of model 0 (baseline single)\n",
    "\n",
    "    for m in range(N_MODELS_ENSEMBLE):\n",
    "        print(f\"\\n  >> Ensemble model {m+1}/{N_MODELS_ENSEMBLE} (outer fold {fold})\")\n",
    "\n",
    "        # Base seed for this model & fold\n",
    "        base_seed = 1000 + 100 * fold + m\n",
    "        set_global_seeds(base_seed)\n",
    "\n",
    "        # === INNER CV: Hyperparameter optimization on X_train_outer ===\n",
    "        inner_kf = KFold(\n",
    "            n_splits=N_SPLITS_INNER, shuffle=True, random_state=base_seed\n",
    "        )\n",
    "\n",
    "        def inner_objective(trial):\n",
    "            # Hyperparameter search space\n",
    "            params = {\n",
    "                \"n_hidden1\": trial.suggest_int(\"n_hidden1\", 32, 256, step=32),\n",
    "                \"n_hidden2\": trial.suggest_int(\"n_hidden2\", 16, 128, step=16),\n",
    "                \"dropout1\": trial.suggest_float(\"dropout1\", 0.0, 0.5),\n",
    "                \"dropout2\": trial.suggest_float(\"dropout2\", 0.0, 0.5),\n",
    "                \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "                \"n_epochs\": trial.suggest_int(\"n_epochs\", 50, 200, step=25),\n",
    "                \"C1\": trial.suggest_float(\"C1\", 0.1, 5.0, log=True),\n",
    "                \"activation\": trial.suggest_categorical(\n",
    "                    \"activation\", [\"relu\", \"leaky_relu\", \"elu\"]\n",
    "                ),\n",
    "                \"optimizer\": trial.suggest_categorical(\n",
    "                    \"optimizer\", [\"adam\", \"adamw\"]\n",
    "                ),\n",
    "                \"weight_decay\": trial.suggest_float(\n",
    "                    \"weight_decay\", 1e-6, 1e-3, log=True\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            inner_scores = []\n",
    "\n",
    "            for inner_fold, (inner_train_idx, inner_val_idx) in enumerate(\n",
    "                inner_kf.split(X_train_outer), 1\n",
    "            ):\n",
    "                set_global_seeds(base_seed + trial.number * 10 + inner_fold)\n",
    "\n",
    "                X_train_inner = X_train_outer.iloc[inner_train_idx]\n",
    "                X_val_inner = X_train_outer.iloc[inner_val_idx]\n",
    "\n",
    "                y_time_train_inner = y_time_train_outer[inner_train_idx]\n",
    "                y_time_val_inner = y_time_train_outer[inner_val_idx]\n",
    "                y_event_train_inner = y_event_train_outer[inner_train_idx]\n",
    "                y_event_val_inner = y_event_train_outer[inner_val_idx]\n",
    "\n",
    "                X_train_inner_tensor = torch.tensor(\n",
    "                    X_train_inner.values, dtype=torch.float32\n",
    "                )\n",
    "                X_val_inner_tensor = torch.tensor(\n",
    "                    X_val_inner.values, dtype=torch.float32\n",
    "                )\n",
    "\n",
    "                time_bins_inner = make_time_bins(\n",
    "                    y_time_train_inner, event=y_event_train_inner\n",
    "                )\n",
    "\n",
    "                model_inner = build_mtlr_model(\n",
    "                    input_dim=X_train_inner.shape[1],\n",
    "                    time_bins=time_bins_inner,\n",
    "                    params=params,\n",
    "                )\n",
    "                optimizer_inner = get_optimizer(model_inner, params)\n",
    "\n",
    "                model_inner, _ = train_mtlr_model(\n",
    "                    model=model_inner,\n",
    "                    optimizer=optimizer_inner,\n",
    "                    X_tensor=X_train_inner_tensor,\n",
    "                    y_time_fold=y_time_train_inner,\n",
    "                    y_event_fold=y_event_train_inner,\n",
    "                    time_bins=time_bins_inner,\n",
    "                    C1=params[\"C1\"],\n",
    "                    n_epochs=params[\"n_epochs\"],\n",
    "                    clip_grad=1.0,\n",
    "                    verbose=False,\n",
    "                )\n",
    "\n",
    "                if model_inner is None:\n",
    "                    inner_scores.append(0.5)\n",
    "                    continue\n",
    "\n",
    "                y_train_struct_inner = Surv.from_arrays(\n",
    "                    event=y_event_train_inner.numpy().astype(bool),\n",
    "                    time=y_time_train_inner.numpy(),\n",
    "                )\n",
    "                y_val_struct_inner = Surv.from_arrays(\n",
    "                    event=y_event_val_inner.numpy().astype(bool),\n",
    "                    time=y_time_val_inner.numpy(),\n",
    "                )\n",
    "\n",
    "                risk_val_inner = mtlr_risk_scores(\n",
    "                    model_inner, X_val_inner_tensor\n",
    "                )\n",
    "\n",
    "                if np.isnan(risk_val_inner).any():\n",
    "                    inner_scores.append(0.5)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    cindex_inner = compute_ipcw_cindex(\n",
    "                        y_train_struct_inner,\n",
    "                        y_val_struct_inner,\n",
    "                        risk_val_inner,\n",
    "                        tau=TAU_CINDEX,\n",
    "                    )\n",
    "                except Exception:\n",
    "                    cindex_inner = 0.5\n",
    "\n",
    "                inner_scores.append(cindex_inner)\n",
    "\n",
    "            return float(np.mean(inner_scores))\n",
    "\n",
    "        # Optuna study for this model & this outer fold\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=f\"mtlr_outer_fold{fold}_model{m}\",\n",
    "        )\n",
    "        study.optimize(\n",
    "            inner_objective,\n",
    "            n_trials=N_TRIALS_INNER,\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "\n",
    "        best_params = study.best_params\n",
    "        print(f\"    - Best inner CV C-index : {study.best_value:.4f}\")\n",
    "        print(f\"    - Best params           : {best_params}\")\n",
    "\n",
    "        # === Final training of this model on full outer-train set ===\n",
    "        model_m = build_mtlr_model(\n",
    "            input_dim=X_train_outer.shape[1],\n",
    "            time_bins=time_bins_outer,\n",
    "            params=best_params,\n",
    "        )\n",
    "        optimizer_m = get_optimizer(model_m, best_params)\n",
    "\n",
    "        model_m, _ = train_mtlr_model(\n",
    "            model=model_m,\n",
    "            optimizer=optimizer_m,\n",
    "            X_tensor=X_train_outer_tensor,\n",
    "            y_time_fold=y_time_train_outer,\n",
    "            y_event_fold=y_event_train_outer,\n",
    "            time_bins=time_bins_outer,\n",
    "            C1=best_params[\"C1\"],\n",
    "            n_epochs=best_params[\"n_epochs\"],\n",
    "            clip_grad=1.0,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        if model_m is None:\n",
    "            print(\"    ⚠ Training failed on outer train set. Model ignored.\")\n",
    "            continue\n",
    "\n",
    "        # Risk scores on outer test\n",
    "        risk_test_m = mtlr_risk_scores(model_m, X_test_outer_tensor)\n",
    "        risk_models_test.append(risk_test_m)\n",
    "\n",
    "        # Define \"single model\" as the first model in the ensemble\n",
    "        if m == 0:\n",
    "            if np.isnan(risk_test_m).any():\n",
    "                cindex_single = 0.5\n",
    "            else:\n",
    "                cindex_single = compute_ipcw_cindex(\n",
    "                    y_train_struct_outer,\n",
    "                    y_test_struct_outer,\n",
    "                    risk_test_m,\n",
    "                    tau=TAU_CINDEX,\n",
    "                )\n",
    "            single_cindex_this_fold = cindex_single\n",
    "            print(f\"    -> Single model C-index on outer test : {cindex_single:.4f}\")\n",
    "\n",
    "    # End of the loop over ensemble models for this outer fold\n",
    "\n",
    "    # If no valid models:\n",
    "    if len(risk_models_test) == 0:\n",
    "        print(\"  ⚠ No valid models for this fold. Ensemble C-index = 0.5.\")\n",
    "        cindex_ensemble = 0.5\n",
    "    else:\n",
    "        risk_matrix_test = np.vstack(risk_models_test)\n",
    "        risk_ensemble = rank_based_ensemble(risk_matrix_test)\n",
    "        if np.isnan(risk_ensemble).any():\n",
    "            cindex_ensemble = 0.5\n",
    "        else:\n",
    "            cindex_ensemble = compute_ipcw_cindex(\n",
    "                y_train_struct_outer,\n",
    "                y_test_struct_outer,\n",
    "                risk_ensemble,\n",
    "                tau=TAU_CINDEX,\n",
    "            )\n",
    "\n",
    "    # If single model did not compute (e.g. model 0 failed)\n",
    "    if single_cindex_this_fold is None:\n",
    "        single_cindex_this_fold = 0.5\n",
    "\n",
    "    cv_single_scores.append(single_cindex_this_fold)\n",
    "    cv_ensemble_scores.append(cindex_ensemble)\n",
    "\n",
    "    print(f\"\\n  Fold {fold} summary:\")\n",
    "    print(f\"    Single model C-index : {single_cindex_this_fold:.4f}\")\n",
    "    print(f\"    Ensemble C-index     : {cindex_ensemble:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5df896",
   "metadata": {},
   "source": [
    "# 4. Statistical Analysis: Means, 95% CI, Paired t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd53104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_scores(scores, name: str):\n",
    "    scores = np.array(scores)\n",
    "    mean = scores.mean()\n",
    "    std = scores.std(ddof=1)\n",
    "    n = len(scores)\n",
    "\n",
    "    t_crit = stats.t.ppf(0.975, df=n - 1)\n",
    "    ci_low = mean - t_crit * std / np.sqrt(n)\n",
    "    ci_high = mean + t_crit * std / np.sqrt(n)\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Scores by fold : {np.round(scores, 4)}\")\n",
    "    print(f\"  Mean           : {mean:.4f}\")\n",
    "    print(f\"  Std dev        : {std:.4f}\")\n",
    "    print(f\"  95% CI         : [{ci_low:.4f} ; {ci_high:.4f}]\")\n",
    "    return scores, mean, std, (ci_low, ci_high)\n",
    "\n",
    "\n",
    "single_scores, single_mean, single_std, single_ci = summarize_scores(\n",
    "    cv_single_scores, \"Single model (first model of ensemble)\"\n",
    ")\n",
    "ensemble_scores, ensemble_mean, ensemble_std, ensemble_ci = summarize_scores(\n",
    "    cv_ensemble_scores, \"Ensemble (5 models, rank-based)\"\n",
    ")\n",
    "\n",
    "# Paired t-test: ensemble vs single\n",
    "diff = ensemble_scores - single_scores\n",
    "t_stat, p_val = stats.ttest_rel(ensemble_scores, single_scores)\n",
    "\n",
    "print(\"\\nPaired t-test: Ensemble vs Single model\")\n",
    "print(f\"  Fold-wise differences (ensemble - single) : {np.round(diff, 4)}\")\n",
    "print(f\"  Mean difference : {diff.mean():.4f}\")\n",
    "print(f\"  t-statistic     : {t_stat:.4f}\")\n",
    "print(f\"  p-value         : {p_val:.4f}\")\n",
    "if p_val < 0.05:\n",
    "    print(\"  => Statistically significant improvement at 5% level\")\n",
    "else:\n",
    "    print(\"  => NOT statistically significant at 5% level with these folds\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "folds = np.arange(1, N_SPLITS_OUTER + 1)\n",
    "plt.plot(folds, single_scores, \"o-\", label=\"Single model\", linewidth=2, markersize=8)\n",
    "plt.plot(folds, ensemble_scores, \"s-\", label=\"Ensemble (rank)\", linewidth=2, markersize=8)\n",
    "plt.xlabel(\"Outer fold\")\n",
    "plt.ylabel(\"IPCW C-index\")\n",
    "plt.title(\"C-index per outer fold\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar([\"Single\", \"Ensemble\"], [single_mean, ensemble_mean],\n",
    "        yerr=[single_std, ensemble_std], capsize=5, alpha=0.7)\n",
    "plt.ylabel(\"IPCW C-index (mean)\")\n",
    "plt.title(\"Average performance (±1 SD)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172ffd4",
   "metadata": {},
   "source": [
    "# 5. Final Training on Full Train Set + 5-Model Ensemble for Evaluation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_and_train_full_model(X_data, y_time_data, y_event_data, base_seed: int):\n",
    "    \"\"\"\n",
    "    On the full dataset:\n",
    "      - Inner CV to optimize hyperparameters (with Optuna)\n",
    "      - Final training on the full data with best hyperparameters\n",
    "    Returns the trained model, time bins, best params, and loss history.\n",
    "    \"\"\"\n",
    "    set_global_seeds(base_seed)\n",
    "\n",
    "    X_data = X_data.reset_index(drop=True)\n",
    "    X_tensor = torch.tensor(X_data.values, dtype=torch.float32)\n",
    "    time_bins_data = make_time_bins(y_time_data, event=y_event_data)\n",
    "\n",
    "    inner_kf = KFold(\n",
    "        n_splits=N_SPLITS_INNER, shuffle=True, random_state=base_seed\n",
    "    )\n",
    "\n",
    "    def inner_objective_full(trial):\n",
    "        params = {\n",
    "            \"n_hidden1\": trial.suggest_int(\"n_hidden1\", 32, 256, step=32),\n",
    "            \"n_hidden2\": trial.suggest_int(\"n_hidden2\", 16, 128, step=16),\n",
    "            \"dropout1\": trial.suggest_float(\"dropout1\", 0.0, 0.5),\n",
    "            \"dropout2\": trial.suggest_float(\"dropout2\", 0.0, 0.5),\n",
    "            \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "            \"n_epochs\": trial.suggest_int(\"n_epochs\", 50, 200, step=25),\n",
    "            \"C1\": trial.suggest_float(\"C1\", 0.1, 5.0, log=True),\n",
    "            \"activation\": trial.suggest_categorical(\n",
    "                \"activation\", [\"relu\", \"leaky_relu\", \"elu\"]\n",
    "            ),\n",
    "            \"optimizer\": trial.suggest_categorical(\n",
    "                \"optimizer\", [\"adam\", \"adamw\"]\n",
    "            ),\n",
    "            \"weight_decay\": trial.suggest_float(\n",
    "                \"weight_decay\", 1e-6, 1e-3, log=True\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        inner_scores = []\n",
    "\n",
    "        for inner_fold, (inner_train_idx, inner_val_idx) in enumerate(\n",
    "            inner_kf.split(X_data), 1\n",
    "        ):\n",
    "            set_global_seeds(base_seed + trial.number * 10 + inner_fold)\n",
    "\n",
    "            X_train_inner = X_data.iloc[inner_train_idx]\n",
    "            X_val_inner = X_data.iloc[inner_val_idx]\n",
    "\n",
    "            y_time_train_inner = y_time_data[inner_train_idx]\n",
    "            y_time_val_inner = y_time_data[inner_val_idx]\n",
    "            y_event_train_inner = y_event_data[inner_train_idx]\n",
    "            y_event_val_inner = y_event_data[inner_val_idx]\n",
    "\n",
    "            X_train_inner_tensor = torch.tensor(\n",
    "                X_train_inner.values, dtype=torch.float32\n",
    "            )\n",
    "            X_val_inner_tensor = torch.tensor(\n",
    "                X_val_inner.values, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            time_bins_inner = make_time_bins(\n",
    "                y_time_train_inner, event=y_event_train_inner\n",
    "            )\n",
    "\n",
    "            model_inner = build_mtlr_model(\n",
    "                input_dim=X_train_inner.shape[1],\n",
    "                time_bins=time_bins_inner,\n",
    "                params=params,\n",
    "            )\n",
    "            optimizer_inner = get_optimizer(model_inner, params)\n",
    "\n",
    "            model_inner, _ = train_mtlr_model(\n",
    "                model=model_inner,\n",
    "                optimizer=optimizer_inner,\n",
    "                X_tensor=X_train_inner_tensor,\n",
    "                y_time_fold=y_time_train_inner,\n",
    "                y_event_fold=y_event_train_inner,\n",
    "                time_bins=time_bins_inner,\n",
    "                C1=params[\"C1\"],\n",
    "                n_epochs=params[\"n_epochs\"],\n",
    "                clip_grad=1.0,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            if model_inner is None:\n",
    "                inner_scores.append(0.5)\n",
    "                continue\n",
    "\n",
    "            y_train_struct_inner = Surv.from_arrays(\n",
    "                event=y_event_train_inner.numpy().astype(bool),\n",
    "                time=y_time_train_inner.numpy(),\n",
    "            )\n",
    "            y_val_struct_inner = Surv.from_arrays(\n",
    "                event=y_event_val_inner.numpy().astype(bool),\n",
    "                time=y_time_val_inner.numpy(),\n",
    "            )\n",
    "\n",
    "            risk_val_inner = mtlr_risk_scores(model_inner, X_val_inner_tensor)\n",
    "\n",
    "            if np.isnan(risk_val_inner).any():\n",
    "                inner_scores.append(0.5)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                cindex_inner = compute_ipcw_cindex(\n",
    "                    y_train_struct_inner,\n",
    "                    y_val_struct_inner,\n",
    "                    risk_val_inner,\n",
    "                    tau=TAU_CINDEX,\n",
    "                )\n",
    "            except Exception:\n",
    "                cindex_inner = 0.5\n",
    "\n",
    "            inner_scores.append(cindex_inner)\n",
    "\n",
    "        return float(np.mean(inner_scores))\n",
    "\n",
    "    # Optuna on full dataset (inner CV)\n",
    "    study_full = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=f\"mtlr_full_seed{base_seed}\",\n",
    "    )\n",
    "    study_full.optimize(\n",
    "        inner_objective_full,\n",
    "        n_trials=N_TRIALS_INNER,\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    best_params_full = study_full.best_params\n",
    "    print(f\"Seed {base_seed} - best inner CV C-index: {study_full.best_value:.4f}\")\n",
    "    print(f\"Best params: {best_params_full}\")\n",
    "\n",
    "    # Final training on full data\n",
    "    model_full = build_mtlr_model(\n",
    "        input_dim=X_data.shape[1],\n",
    "        time_bins=time_bins_data,\n",
    "        params=best_params_full,\n",
    "    )\n",
    "    optimizer_full = get_optimizer(model_full, best_params_full)\n",
    "\n",
    "    model_full, losses_full = train_mtlr_model(\n",
    "        model=model_full,\n",
    "        optimizer=optimizer_full,\n",
    "        X_tensor=X_tensor,\n",
    "        y_time_fold=y_time_data,\n",
    "        y_event_fold=y_event_data,\n",
    "        time_bins=time_bins_data,\n",
    "        C1=best_params_full[\"C1\"],\n",
    "        n_epochs=best_params_full[\"n_epochs\"],\n",
    "        clip_grad=1.0,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return model_full, time_bins_data, best_params_full, losses_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1807f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 5 final models on the full train set\n",
    "X_tensor_full = torch.tensor(X.values, dtype=torch.float32)\n",
    "X_eval_tensor = torch.tensor(X_eval.values, dtype=torch.float32)\n",
    "\n",
    "models_final = []\n",
    "risk_matrix_eval = []\n",
    "\n",
    "print(\"\\n=== Final ensemble training (5 models) on the full train set ===\")\n",
    "\n",
    "for m in range(N_MODELS_ENSEMBLE):\n",
    "    base_seed = 5000 + m\n",
    "    print(f\"\\n##### Final model {m+1}/{N_MODELS_ENSEMBLE} (seed={base_seed}) #####\")\n",
    "    model_m, time_bins_full, best_params_m, losses_m = optimize_and_train_full_model(\n",
    "        X_data=X, y_time_data=y_time, y_event_data=y_event, base_seed=base_seed\n",
    "    )\n",
    "\n",
    "    if model_m is None:\n",
    "        print(\"⚠ Final model is invalid, ignored.\")\n",
    "        continue\n",
    "\n",
    "    models_final.append(model_m)\n",
    "    risk_eval_m = mtlr_risk_scores(model_m, X_eval_tensor)\n",
    "    risk_matrix_eval.append(risk_eval_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_matrix_eval = np.array(risk_matrix_eval)\n",
    "print(f\"\\nNumber of valid final models in ensemble: {risk_matrix_eval.shape[0]}\")\n",
    "\n",
    "if risk_matrix_eval.shape[0] == 0:\n",
    "    raise RuntimeError(\"No valid final model for the ensemble.\")\n",
    "\n",
    "# Final ensemble: rank-based aggregation\n",
    "risk_ensemble_eval = rank_based_ensemble(risk_matrix_eval)\n",
    "\n",
    "# Optional normalization to [0, 1] for interpretability\n",
    "risk_min = risk_ensemble_eval.min()\n",
    "risk_max = risk_ensemble_eval.max()\n",
    "risk_ensemble_eval_norm = (risk_ensemble_eval - risk_min) / (risk_max - risk_min + 1e-8)\n",
    "\n",
    "print(f\"Min risk (raw ensemble) : {risk_min:.4f}\")\n",
    "print(f\"Max risk (raw ensemble) : {risk_max:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prediction dataframe\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": df_eval[\"ID\"],\n",
    "    \"risk_score\": risk_ensemble_eval_norm,  # or risk_ensemble_eval if you prefer raw scores\n",
    "})\n",
    "\n",
    "print(submission.head())\n",
    "\n",
    "submission.to_csv(\"submission_mtlr_ensemble_nestedcv.csv\", index=False)\n",
    "print(\"\\nPrediction file saved as submission_mtlr_ensemble_nestedcv.csv\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
