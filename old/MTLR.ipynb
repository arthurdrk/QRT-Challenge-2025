{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "```python"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importation des librairies"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:25:40.462922Z",
     "start_time": "2025-11-14T20:24:45.642508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmtlr import MTLR, mtlr_neg_log_likelihood, mtlr_survival\n",
    "from torchmtlr.utils import encode_survival, make_time_bins\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Modèle MTLR"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_train = pd.read_csv(\"../data/train_enhanced.csv\")\n",
    "df_eval = pd.read_csv(\"../data/eval_enhanced.csv\")\n",
    "\n",
    "print(f\"Train data: {df_train.shape}\")\n",
    "print(f\"Eval data: {df_eval.shape}\")\n",
    "print(\"\\nColonnes train:\")\n",
    "print(df_train.columns.tolist())\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"'ID' in df_train columns:\", \"ID\" in df_train.columns)\n",
    "print(\"\\nFirst few columns of df_train:\")\n",
    "print(df_train.columns[:10].tolist())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "target = [\"OS_STATUS\", \"OS_YEARS\"]\n",
    "X = df_train.drop(columns=target + [\"ID\"])\n",
    "X = pd.get_dummies(X, drop_first=True).astype(float)\n",
    "\n",
    "# Préparation des données d'évaluation\n",
    "X_eval = df_eval.drop(columns=[\"ID\"])\n",
    "X_eval = pd.get_dummies(X_eval, drop_first=True)\n",
    "X_eval = X_eval.reindex(columns=X.columns, fill_value=0).astype(float)\n",
    "\n",
    "# Conversion en tenseurs\n",
    "y_time = torch.tensor(df_train['OS_YEARS'].values, dtype=torch.float32)\n",
    "y_event = torch.tensor(df_train['OS_STATUS'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"X_eval shape: {X_eval.shape}\")\n",
    "print(f\"Nombre de features: {X.shape[1]\")\n",
    "\n",
    "# Définir le splitter KFold avant d'utiliser dans objective\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Insertion d objective et optimisation Optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparamètres à optimiser\n",
    "    params = {\n",
    "        'n_hidden1': trial.suggest_int('n_hidden1', 16, 240, step=32),\n",
    "        'n_hidden2': trial.suggest_int('n_hidden2', 8, 120, step=16),\n",
    "        'dropout1': trial.suggest_float('dropout1', 0.0, 0.5),\n",
    "        'dropout2': trial.suggest_float('dropout2', 0.0, 0.5),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "        'n_epochs': trial.suggest_int('n_epochs', 50, 200, step=25),\n",
    "        'C1': trial.suggest_float('C1', 0.1, 5.0, log=True),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'elu']),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['adam', 'adamw']),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    }\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "        # Préparation des données du fold\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx]\n",
    "        y_time_train_fold = y_time[train_idx]\n",
    "        y_time_test_fold = y_time[test_idx]\n",
    "        y_event_train_fold = y_event[train_idx]\n",
    "        y_event_test_fold = y_event[test_idx]\n",
    "\n",
    "        # Le reste du pipeline (encodage, time_bins, entraînement, évaluation)\n",
    "        # Conversion en tenseurs\n",
    "        X_train_fold_tensor = torch.tensor(X_train_fold.values, dtype=torch.float32)\n",
    "        X_test_fold_tensor = torch.tensor(X_test_fold.values, dtype=torch.float32)\n",
    "\n",
    "        # Time bins et encodage\n",
    "        time_bins_fold = make_time_bins(y_time_train_fold, event=y_event_train_fold)\n",
    "        target_fold = encode_survival(y_time_train_fold, y_event_train_fold, time_bins_fold)\n",
    "\n",
    "        # Structure Surv pour l'entraînement\n",
    "        y_train_struct_fold = Surv.from_arrays(\n",
    "            event=y_event_train_fold.numpy().astype(bool),\n",
    "            time=y_time_train_fold.numpy()\n",
    "        )\n",
    "\n",
    "        # Construction du modèle\n",
    "        if params['activation'] == 'relu':\n",
    "            activation = nn.ReLU()\n",
    "        elif params['activation'] == 'leaky_relu':\n",
    "            activation = nn.LeakyReLU()\n",
    "        else:  # elu\n",
    "            activation = nn.ELU()\n",
    "\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(X_train_fold.shape[1], params['n_hidden1']),\n",
    "            nn.BatchNorm1d(params['n_hidden1']),\n",
    "            activation,\n",
    "            nn.Dropout(params['dropout1']),\n",
    "            nn.Linear(params['n_hidden1'], params['n_hidden2']),\n",
    "            nn.BatchNorm1d(params['n_hidden2']),\n",
    "            activation,\n",
    "            nn.Dropout(params['dropout2']),\n",
    "            MTLR(params['n_hidden2'], len(time_bins_fold))\n",
    "        )\n",
    "\n",
    "        # Optimizer\n",
    "        if params['optimizer'] == 'adamw':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'],\n",
    "                                          weight_decay=params['weight_decay'])\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'],\n",
    "                                         weight_decay=params['weight_decay'])\n",
    "\n",
    "        # Entraînement\n",
    "        model.train()\n",
    "        for epoch in range(params['n_epochs']):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_train_fold_tensor)\n",
    "            loss = mtlr_neg_log_likelihood(logits, target_fold, model[-1],\n",
    "                                           C1=params['C1'], average=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Évaluation avec la méthode optimale (logsumexp)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_test = model(X_test_fold_tensor)\n",
    "            test_risk = torch.logsumexp(logits_test, dim=1).numpy()\n",
    "\n",
    "        # Structure Surv pour le test\n",
    "        y_test_struct_fold = Surv.from_arrays(\n",
    "            event=y_event_test_fold.numpy().astype(bool),\n",
    "            time=y_time_test_fold.numpy()\n",
    "        )\n",
    "\n",
    "        # Calcul du C-index\n",
    "        try:\n",
    "            test_score = concordance_index_ipcw(\n",
    "                y_train_struct_fold,\n",
    "                y_test_struct_fold,\n",
    "                test_risk,\n",
    "                tau=7.0\n",
    "            )[0]\n",
    "            cv_scores.append(test_score)\n",
    "        except Exception as e:\n",
    "            # En cas d'erreur, on utilise un score par défaut\n",
    "            cv_scores.append(0.5)\n",
    "            continue\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "\n",
    "# ... démarrage de l’optimisation Optuna comme précédemment ...\n",
    "print(\"Début de l'optimisation Optuna...\")\n",
    "study = optuna.create_study(direction='maximize', study_name='mtlr_optimization')\n",
    "study.optimize(objective, n_trials=1000, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n OPTIMISATION TERMINÉE\")\n",
    "print(f\"Best C-Index IPCW: {study.best_value:.4f}\")\n",
    "print(\"\\nMeilleurs hyperparamètres:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Sauvegarde des meilleurs paramètres\n",
    "best_params = study.best_params\n",
    "with open(\"best_params_mtlr.json\", \"w\") as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "print(\"Paramètres sauvegardés dans best_params_mtlr.json\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig1 = plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "fig2 = plot_param_importances(study)\n",
    "fig2.show()\n",
    "\n",
    "# Sauvegarde des paramètres\n",
    "best_params = study.best_params\n",
    "with open(\"best_params_mtlr.json\", \"w\") as f:\n",
    "    json.dump(best_params, f, indent=4)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "cv_scores_train_tuned = []\n",
    "cv_scores_test_tuned = []\n",
    "\n",
    "print(\"\\n=== VALIDATION CROISÉE AVEC MODÈLE OPTIMISÉ ===\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "    print(f\"Fold {fold}: train = {len(train_idx)}, test = {len(test_idx)}\")\n",
    "    # Préparation des données du fold\n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    X_test_fold = X.iloc[test_idx]\n",
    "    y_time_train_fold = y_time[train_idx]\n",
    "    y_time_test_fold = y_time[test_idx]\n",
    "    y_event_train_fold = y_event[train_idx]\n",
    "    y_event_test_fold = y_event[test_idx]\n",
    "    \n",
    "    # Conversion en tenseurs\n",
    "    X_train_fold_tensor = torch.tensor(X_train_fold.values, dtype=torch.float32)\n",
    "    X_test_fold_tensor = torch.tensor(X_test_fold.values, dtype=torch.float32)\n",
    "    \n",
    "    # Création des time bins pour ce fold\n",
    "    time_bins_fold = make_time_bins(y_time_train_fold, event=y_event_train_fold)\n",
    "    target_fold = encode_survival(y_time_train_fold, y_event_train_fold, time_bins_fold)\n",
    "    \n",
    "    # Structures Surv pour l'évaluation\n",
    "    y_train_struct_fold = Surv.from_arrays(\n",
    "        event=y_event_train_fold.numpy().astype(bool), \n",
    "        time=y_time_train_fold.numpy()\n",
    "    )\n",
    "    y_test_struct_fold = Surv.from_arrays(\n",
    "        event=y_event_test_fold.numpy().astype(bool), \n",
    "        time=y_time_test_fold.numpy()\n",
    "    )\n",
    "    \n",
    "    # Construction du modèle avec les meilleurs paramètres\n",
    "    if best_params['activation'] == 'relu':\n",
    "        activation = nn.ReLU()\n",
    "    elif best_params['activation'] == 'leaky_relu':\n",
    "        activation = nn.LeakyReLU()\n",
    "    else:\n",
    "        activation = nn.ELU()\n",
    "        \n",
    "    model_tuned = nn.Sequential(\n",
    "        nn.Linear(X_train_fold.shape[1], best_params['n_hidden1']),\n",
    "        nn.BatchNorm1d(best_params['n_hidden1']),\n",
    "        activation,\n",
    "        nn.Dropout(best_params['dropout1']),\n",
    "        nn.Linear(best_params['n_hidden1'], best_params['n_hidden2']),\n",
    "        nn.BatchNorm1d(best_params['n_hidden2']),\n",
    "        activation,\n",
    "        nn.Dropout(best_params['dropout2']),\n",
    "        MTLR(best_params['n_hidden2'], len(time_bins_fold))\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    if best_params['optimizer'] == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(model_tuned.parameters(), lr=best_params['lr'], \n",
    "                                    weight_decay=best_params['weight_decay'])\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model_tuned.parameters(), lr=best_params['lr'], \n",
    "                                   weight_decay=best_params['weight_decay'])\n",
    "    \n",
    "    # Entraînement du modèle optimisé\n",
    "    model_tuned.train()\n",
    "    fold_losses = []\n",
    "    for epoch in range(best_params['n_epochs']):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_tuned(X_train_fold_tensor)\n",
    "        loss = mtlr_neg_log_likelihood(logits, target_fold, model_tuned[-1], \n",
    "                                     C1=best_params['C1'], average=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fold_losses.append(loss.item())\n",
    "    \n",
    "    # Évaluation avec la méthode optimale (logsumexp)\n",
    "    model_tuned.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_train = model_tuned(X_train_fold_tensor)\n",
    "        logits_test = model_tuned(X_test_fold_tensor)\n",
    "        \n",
    "        train_risk = torch.logsumexp(logits_train, dim=1).numpy()\n",
    "        test_risk = torch.logsumexp(logits_test, dim=1).numpy()\n",
    "    \n",
    "    # Calcul des C-index\n",
    "    train_score = concordance_index_ipcw(y_train_struct_fold, y_train_struct_fold, train_risk, tau=7)[0]\n",
    "    test_score = concordance_index_ipcw(y_train_struct_fold, y_test_struct_fold, test_risk, tau=7)[0]\n",
    "    \n",
    "    cv_scores_train_tuned.append(train_score)\n",
    "    cv_scores_test_tuned.append(test_score)\n",
    "    \n",
    "    print(f\"Fold {fold} - Train C-Index IPCW: {train_score:.4f}, Test C-Index IPCW: {test_score:.4f}\")\n",
    "    print(f\"          Loss finale: {fold_losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nRÉSULTATS MODÈLE OPTIMISÉ\")\n",
    "print(f\"Average Train C-Index IPCW: {np.mean(cv_scores_train_tuned):.4f} (+/- {np.std(cv_scores_train_tuned):.4f})\")\n",
    "print(f\"Average Test C-Index IPCW: {np.mean(cv_scores_test_tuned):.4f} (+/- {np.std(cv_scores_test_tuned):.4f})\")\n",
    "\n",
    "# Ensure score containers exist before using them\n",
    "if 'cv_scores_test' not in locals():\n",
    "    cv_scores_test = []\n",
    "if 'cv_scores_test_tuned' not in locals():\n",
    "    cv_scores_test_tuned = []\n",
    "\n",
    "# If they are still empty, inform the user and avoid raising an exception\n",
    "if len(cv_scores_test) == 0 or len(cv_scores_test_tuned) == 0:\n",
    "    print(\"Warning: One or both cross-validation score lists are empty. \"\n",
    "          \"Baseline or Tuned scores may not have been computed due to earlier errors.\")\n",
    "else:\n",
    "    # Comparaison avec le baseline\n",
    "    print(f\"\\nCOMPARAISON AVEC BASELINE\")\n",
    "    print(f\"Baseline Model - Average Test C-Index IPCW: {np.mean(cv_scores_test):.4f} (+/- {np.std(cv_scores_test):.4f})\")\n",
    "    print(f\"Tuned Model    - Average Test C-Index IPCW: {np.mean(cv_scores_test_tuned):.4f} (+/- {np.std(cv_scores_test_tuned):.4f})\")\n",
    "\n",
    "    improvement = np.mean(cv_scores_test_tuned) - np.mean(cv_scores_test)\n",
    "    print(f\"\\nImprovement (Tuned - Baseline): {improvement:.4f}\")\n",
    "\n",
    "# Visualisation des scores par fold\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "folds = range(1, 6)\n",
    "plt.plot(folds, cv_scores_test, 'o-', label='Baseline', linewidth=2, markersize=8)\n",
    "plt.plot(folds, cv_scores_test_tuned, 's-', label='Optimisé', linewidth=2, markersize=8)\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('C-Index IPCW')\n",
    "plt.title('Comparaison des performances par fold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "models = ['Baseline', 'Optimisé']\n",
    "means = [np.mean(cv_scores_test), np.mean(cv_scores_test_tuned)]\n",
    "stds = [np.std(cv_scores_test), np.std(cv_scores_test_tuned)]\n",
    "plt.bar(models, means, yerr=stds, capsize=5, alpha=0.7, color=['skyblue', 'lightcoral'])\n",
    "plt.ylabel('C-Index IPCW (moyenne)')\n",
    "plt.title('Performance moyenne avec écart-type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Time bins sur toutes les données\n",
    "time_bins_final = make_time_bins(y_time, event=y_event)\n",
    "target_final = encode_survival(y_time, y_event, time_bins_final)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "X_eval_tensor = torch.tensor(X_eval.values, dtype=torch.float32)\n",
    "\n",
    "# Construction du modèle final avec les meilleurs paramètres\n",
    "if best_params['activation'] == 'relu':\n",
    "    activation_final = nn.ReLU()\n",
    "elif best_params['activation'] == 'leaky_relu':\n",
    "    activation_final = nn.LeakyReLU()\n",
    "else:\n",
    "    activation_final = nn.ELU()\n",
    "\n",
    "model_final = nn.Sequential(\n",
    "    nn.Linear(X.shape[1], best_params['n_hidden1']),\n",
    "    nn.BatchNorm1d(best_params['n_hidden1']),\n",
    "    activation_final,\n",
    "    nn.Dropout(best_params['dropout1']),\n",
    "    nn.Linear(best_params['n_hidden1'], best_params['n_hidden2']),\n",
    "    nn.BatchNorm1d(best_params['n_hidden2']),\n",
    "    activation_final,\n",
    "    nn.Dropout(best_params['dropout2']),\n",
    "    MTLR(best_params['n_hidden2'], len(time_bins_final))\n",
    ")\n",
    "\n",
    "if best_params['optimizer'] == 'adamw':\n",
    "    optimizer = torch.optim.AdamW(model_final.parameters(), lr=best_params['lr'], \n",
    "                                weight_decay=best_params['weight_decay'])\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model_final.parameters(), lr=best_params['lr'], \n",
    "                               weight_decay=best_params['weight_decay'])\n",
    "\n",
    "# Entraînement final avec affichage de la progression\n",
    "model_final.train()\n",
    "final_losses = []\n",
    "for epoch in range(best_params['n_epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model_final(X_tensor)\n",
    "    loss = mtlr_neg_log_likelihood(logits, target_final, model_final[-1], \n",
    "                                 C1=best_params['C1'], average=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    final_losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{best_params['n_epochs']} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot de la loss finale\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(final_losses)\n",
    "plt.title('Loss pendant l\\'entraînement final')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "eval_risk = mtlr_predict_risk(model_final, X_eval_tensor, time_bins_final)\n",
    "\n",
    "# Création du fichier de soumission\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": df_eval[\"ID\"],\n",
    "    \"risk_score\": eval_risk\n",
    "})\n",
    "\n",
    "# Sauvegarde\n",
    "submission.to_csv('../submissions/mtlr_tuned.csv', index=False)\n",
    "\n",
    "# Aperçu des prédictions\n",
    "submission.head(10)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Importance"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_mtlr_feature_importance(model, feature_names, X_tensor, y_event_sub, y_time_sub, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Calcule l'importance des features par permutation\n",
    "    \"\"\"\n",
    "    # Score de base\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        baseline_risk = torch.logsumexp(logits, dim=1).numpy()\n",
    "    \n",
    "    # Structure Surv pour calcul du C-index\n",
    "    y_struct = Surv.from_arrays(\n",
    "        event=y_event_sub.numpy().astype(bool), \n",
    "        time=y_time_sub.numpy()\n",
    "    )\n",
    "    \n",
    "    baseline_score = concordance_index_ipcw(y_struct, y_struct, baseline_risk, tau=7)[0]\n",
    "    \n",
    "    # Importance par permutation\n",
    "    importances = {}\n",
    "    X_np = X_tensor.numpy().copy()\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        feature_scores = []\n",
    "        \n",
    "        for _ in range(n_iterations):\n",
    "            # Permutation de la feature\n",
    "            X_permuted = X_np.copy()\n",
    "            np.random.shuffle(X_permuted[:, i])\n",
    "            \n",
    "            # Prédiction avec feature permutée\n",
    "            with torch.no_grad():\n",
    "                logits_perm = model(torch.tensor(X_permuted, dtype=torch.float32))\n",
    "                risk_perm = torch.logsumexp(logits_perm, dim=1).numpy()\n",
    "            \n",
    "            score_perm = concordance_index_ipcw(y_struct, y_struct, risk_perm, tau=7)[0]\n",
    "            feature_scores.append(score_perm)\n",
    "        \n",
    "        # Importance = baisse de performance\n",
    "        importance = baseline_score - np.mean(feature_scores)\n",
    "        importances[feature] = importance\n",
    "    \n",
    "    return importances, baseline_score\n",
    "\n",
    "# Utilisation sur un sous-ensemble\n",
    "idx = np.arange(1000)  # ou np.random.choice(len(X), 1000, replace=False) pour un échantillon aléatoire\n",
    "feature_names = X.columns.tolist()\n",
    "mtlr_importances, baseline_score = compute_mtlr_feature_importance(\n",
    "    model_final, \n",
    "    feature_names[:20],\n",
    "    X_tensor[idx],\n",
    "    y_event[idx],\n",
    "    y_time[idx],\n",
    "    n_iterations=50\n",
    ")\n",
    "\n",
    "# Tri par importance\n",
    "mtlr_importances = dict(sorted(mtlr_importances.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "\n",
    "# Top 10 features\n",
    "top_features = list(mtlr_importances.keys())[:10]\n",
    "top_importances = list(mtlr_importances.values())[:10]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red' if imp > 0 else 'blue' for imp in top_importances]\n",
    "\n",
    "plt.barh(top_features, top_importances, color=colors, alpha=0.7)\n",
    "plt.xlabel('Importance (drop in C-index)')\n",
    "plt.title('Top 10 Features les plus importantes (MTLR)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "for i, (feature, imp) in enumerate(zip(top_features, top_importances)):\n",
    "    plt.text(imp, i, f'{imp:.4f}', va='center', ha='left' if imp > 0 else 'right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarde\n",
    "feature_importance_results = {\n",
    "    'baseline_score': baseline_score,\n",
    "    'feature_importances': mtlr_importances\n",
    "}\n",
    "\n",
    "with open(\"mtlr_feature_importance.json\", \"w\") as f:\n",
    "    json.dump(feature_importance_results, f, indent=4)\n",
    "    event=y_event_train.numpy().astype(bool), \n",
    "    time=y_time_train.numpy()\n",
    ")\n",
    "y_test_struct = Surv.from_arrays(\n",
    "    event=y_event_test.numpy().astype(bool), \n",
    "    time=y_time_test.numpy()  \n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} échantillons\")\n",
    "print(f\"Test: {len(X_test)} échantillons\")\n",
    "print(f\"Time bins: {len(time_bins)} points\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_baseline = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    MTLR(32, len(time_bins))\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model_baseline.parameters(), lr=0.001)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "losses = []\n",
    "model_baseline.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model_baseline(X_train_tensor)\n",
    "    loss = mtlr_neg_log_likelihood(logits, target_train, model_baseline[-1], C1=1.0, average=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/100 - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot de la loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.title('Loss pendant l\\'entraînement baseline')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "best_params= {'n_hidden1': 16, 'n_hidden2': 88, 'n_layers': 2, 'dropout1': 0.4880527166139476, 'dropout2': 0.3406450265001462, 'lr': 0.006867173180115493, 'n_epochs': 75, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adamw', 'use_weight_decay': False, 'C1': 1.5790286737537849}"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def mtlr_predict_risk(model, X_tensor, time_bins):\n",
    "    \"\"\"Méthode optimale : log-sum-exp des logits\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        risk_scores = torch.logsumexp(logits, dim=1).numpy()\n",
    "    return risk_scores\n",
    "\n",
    "# Prédictions\n",
    "mtlr_risk_train = mtlr_predict_risk(model_baseline, X_train_tensor, time_bins)\n",
    "mtlr_risk_test = mtlr_predict_risk(model_baseline, X_test_tensor, time_bins)\n",
    "\n",
    "# Évaluation\n",
    "mtlr_cindex_train = concordance_index_ipcw(y_train_struct, y_train_struct, mtlr_risk_train, tau=7)[0]\n",
    "mtlr_cindex_test = concordance_index_ipcw(y_train_struct, y_test_struct, mtlr_risk_test, tau=7)[0]\n",
    "\n",
    "\n",
    "print(f\"MTLR Model Concordance Index IPCW on train: {mtlr_cindex_train:.4f}\")\n",
    "print(f\"MTLR Model Concordance Index IPCW on test: {mtlr_cindex_test:.4f}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def objective(trial):\n",
    "    # Hyperparamètres à optimiser\n",
    "    params = {\n",
    "        'n_hidden1': trial.suggest_int('n_hidden1', 16, 240, step=32),  # 16 + 7*32 = 240\n",
    "        'n_hidden2': trial.suggest_int('n_hidden2', 8, 120, step=16),   # 8 + 7*16 = 120\n",
    "        'dropout1': trial.suggest_float('dropout1', 0.0, 0.5),\n",
    "        'dropout2': trial.suggest_float('dropout2', 0.0, 0.5),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "        'n_epochs': trial.suggest_int('n_epochs', 50, 200, step=25),\n",
    "        'C1': trial.suggest_float('C1', 0.1, 5.0, log=True),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'elu']),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['adam', 'adamw']),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        # Préparation des données du fold\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx]\n",
    "        y_time_train_fold = y_time[train_idx]\n",
    "        y_time_test_fold = y_time[test_idx]\n",
    "        y_event_train_fold = y_event[train_idx]\n",
    "        y_event_test_fold = y_event[test_idx]\n",
    "        \n",
    "        # Conversion en tenseurs\n",
    "        X_train_fold_tensor = torch.tensor(X_train_fold.values, dtype=torch.float32)\n",
    "        X_test_fold_tensor = torch.tensor(X_test_fold.values, dtype=torch.float32)\n",
    "        \n",
    "        # Time bins et encodage\n",
    "        time_bins_fold = make_time_bins(y_time_train_fold, event=y_event_train_fold)\n",
    "        target_fold = encode_survival(y_time_train_fold, y_event_train_fold, time_bins_fold)\n",
    "        \n",
    "        # Structure Surv pour l'entraînement\n",
    "        y_train_struct_fold = Surv.from_arrays(\n",
    "            event=y_event_train_fold.numpy().astype(bool), \n",
    "            time=y_time_train_fold.numpy()\n",
    "        )\n",
    "        \n",
    "        # Construction du modèle\n",
    "        if params['activation'] == 'relu':\n",
    "            activation = nn.ReLU()\n",
    "        elif params['activation'] == 'leaky_relu':\n",
    "            activation = nn.LeakyReLU()\n",
    "        else:  # elu\n",
    "            activation = nn.ELU()\n",
    "            \n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(X_train_fold.shape[1], params['n_hidden1']),\n",
    "            nn.BatchNorm1d(params['n_hidden1']),\n",
    "            activation,\n",
    "            nn.Dropout(params['dropout1']),\n",
    "            nn.Linear(params['n_hidden1'], params['n_hidden2']),\n",
    "            nn.BatchNorm1d(params['n_hidden2']),\n",
    "            activation,\n",
    "            nn.Dropout(params['dropout2']),\n",
    "            MTLR(params['n_hidden2'], len(time_bins_fold))\n",
    "        )\n",
    "        \n",
    "        # Optimizer\n",
    "        if params['optimizer'] == 'adamw':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], \n",
    "                                        weight_decay=params['weight_decay'])\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], \n",
    "                                       weight_decay=params['weight_decay'])\n",
    "        \n",
    "        # Entraînement\n",
    "        model.train()\n",
    "        for epoch in range(params['n_epochs']):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_train_fold_tensor)\n",
    "            loss = mtlr_neg_log_likelihood(logits, target_fold, model[-1], \n",
    "                                         C1=params['C1'], average=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Évaluation avec la méthode optimale (logsumexp)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_test = model(X_test_fold_tensor)\n",
    "            test_risk = torch.logsumexp(logits_test, dim=1).numpy()\n",
    "        \n",
    "        # Structure Surv pour le test\n",
    "        y_test_struct_fold = Surv.from_arrays(\n",
    "            event=y_event_test_fold.numpy().astype(bool), \n",
    "            time=y_time_test_fold.numpy()\n",
    "        )\n",
    "        \n",
    "        # Calcul du C-index\n",
    "        try:\n",
    "            test_score = concordance_index_ipcw(\n",
    "                y_train_struct_fold, \n",
    "                y_test_struct_fold, \n",
    "                test_risk, \n",
    "                tau=7.0\n",
    "            )[0]\n",
    "            cv_scores.append(test_score)\n",
    "        except Exception as e:\n",
    "            # En cas d'erreur, on utilise un score par défaut\n",
    "            cv_scores.append(0.5)\n",
    "            continue\n",
    "    \n",
    "    return np.mean(cv\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
