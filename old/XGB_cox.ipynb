{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_enhanced = pd.read_csv('../data/train_pivot4.csv', sep=',')\n",
    "eval_enhanced  = pd.read_csv('../data/eval_pivot4.csv',  sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import xgboost as xgb  # <--- important\n",
    "\n",
    "time_col  = \"OS_YEARS\"\n",
    "event_col = \"OS_STATUS\"\n",
    "exclude_cols = {time_col, event_col, \"ID\"}\n",
    "feature_cols = [c for c in train_enhanced.columns if c not in exclude_cols]\n",
    "\n",
    "X_df = train_enhanced[feature_cols].astype(float)\n",
    "X_df = X_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X = X_df.to_numpy(dtype=float)\n",
    "\n",
    "time_vals  = train_enhanced[time_col].to_numpy(dtype=float)   # temps bruts\n",
    "event_vals = train_enhanced[event_col].to_numpy(dtype=int)    # 1 = event, 0 = censuré (int pour Cox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bce344",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ## Tuning des hyperparamètres XGBoost COX avec Optuna\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"survival:cox\",\n",
    "        \"eval_metric\": \"cox-nloglik\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 10.0),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "\n",
    "    num_boost_round = trial.suggest_int(\"num_boost_round\", 50, 500)\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    c_indices = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        t_train_fold, t_val_fold = time_vals[train_idx], time_vals[val_idx]\n",
    "        e_train_fold, e_val_fold = event_vals[train_idx], event_vals[val_idx]\n",
    "\n",
    "        # XGBoost Cox: label = time, et on fournit la censure via weight=event (0=censuré, 1=event)\n",
    "        dtrain_fold = xgb.DMatrix(X_train_fold, label=t_train_fold, weight=e_train_fold.astype(float))\n",
    "        dval_fold   = xgb.DMatrix(X_val_fold,   label=t_val_fold,   weight=e_val_fold.astype(float))\n",
    "\n",
    "        model_fold = xgb.train(\n",
    "            params,\n",
    "            dtrain_fold,\n",
    "            num_boost_round=num_boost_round,\n",
    "            evals=[(dval_fold, \"valid\")],\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        # Pour Cox, predict = log-risk score (plus grand = plus de risque)\n",
    "        pred_val = model_fold.predict(dval_fold)\n",
    "\n",
    "        c_index = concordance_index_censored(\n",
    "            e_val_fold.astype(bool),\n",
    "            t_val_fold,\n",
    "            pred_val\n",
    "        )[0]\n",
    "\n",
    "        c_indices.append(c_index)\n",
    "\n",
    "    return float(np.mean(c_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b500d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", study_name=\"xgb_cox_tuning\")\n",
    "study.optimize(objective, n_trials=1000, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleurs hyperparamètres:\")\n",
    "print(study.best_params)\n",
    "print(f\"\\nMeilleur C-index moyen: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': 0.01426075160919655, 'max_depth': 3, 'subsample': 0.5784653179987965, 'colsample_bytree': 0.5780335399700407, 'min_child_weight': 8, 'gamma': 3.613478865761625, 'reg_alpha': 0.10045067420915038, 'reg_lambda': 3.2166983232864625, 'num_boost_round': 446}\n",
    "\n",
    "best_params[\"objective\"] = \"survival:cox\"\n",
    "best_params[\"eval_metric\"] = \"cox-nloglik\"\n",
    "best_params[\"tree_method\"] = \"hist\"\n",
    "best_params[\"seed\"] = 42\n",
    "\n",
    "best_num_boost_round = best_params.pop(\"num_boost_round\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final sur tout le train, puis prédictions sur eval, puis sauvegarde submission\n",
    "X_full = train_enhanced[feature_cols].fillna(0.0).to_numpy(dtype=float)\n",
    "t_full = time_vals\n",
    "e_full = event_vals.astype(float)\n",
    "\n",
    "dfull_final = xgb.DMatrix(X_full, label=t_full, weight=e_full)\n",
    "\n",
    "model_tuned = xgb.train(\n",
    "    best_params,\n",
    "    dfull_final,\n",
    "    num_boost_round=best_num_boost_round,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "X_eval_final = eval_enhanced[feature_cols].fillna(0.0).to_numpy(dtype=float)\n",
    "deval_final = xgb.DMatrix(X_eval_final)\n",
    "\n",
    "# Cox -> predict = log-risk score\n",
    "risk_score_tuned = model_tuned.predict(deval_final)\n",
    "\n",
    "submission_tuned = pd.DataFrame({\n",
    "    \"ID\": eval_enhanced[\"ID\"],\n",
    "    \"risk_score\": risk_score_tuned\n",
    "})\n",
    "\n",
    "submission_tuned.to_csv('../submissions/submission_xgb_cox_tuned.csv', index=False)\n",
    "print(\"Prédictions avec hyperparamètres optimisés sauvegardées dans '../submissions/submission_xgb_cox_tuned.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f84a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
